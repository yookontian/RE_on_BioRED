{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing#scrollTo=T-gy-LxM0yAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from labels import get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_special_tokens': ['[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]']} \n",
      " {'additional_special_tokens': ['[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]']}\n"
     ]
    }
   ],
   "source": [
    "# load labels for bert_w_ner\n",
    "additional_tokens, _, _, _ = get_labels(mode='GPT_w_ner')\n",
    "print(additional_tokens, \"\\n\", additional_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model in 8-bit quantization configuration\n",
    "# the max length of the input is 1024\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, \n",
    "    # load_in_8bit=True, \n",
    "    device_map={'':torch.cuda.current_device()},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 346763264 || all params: 346763264 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 6 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('GPT_without_ner/GPT_w_ner_tokenizer/tokenizer_config.json',\n",
       " 'GPT_without_ner/GPT_w_ner_tokenizer/special_tokens_map.json',\n",
       " 'GPT_without_ner/GPT_w_ner_tokenizer/vocab.json',\n",
       " 'GPT_without_ner/GPT_w_ner_tokenizer/merges.txt',\n",
       " 'GPT_without_ner/GPT_w_ner_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding new tokens to the tokenizer\n",
    "# since I haven't load the model so I will resize the embedding of the model later]\n",
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(\"GPT_without_ner/GPT_w_ner_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(42390, 1024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in float32 for stability.\n",
    "\n",
    "We also cast the output of the last layer and embedding layer in float32 for the same reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.biogpt.embed_tokens = CastOutputToFloat(model.biogpt.embed_tokens)\n",
    "model.output_projection = CastOutputToFloat(model.output_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/tian/mambaforge/envs/BioRED did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/lib/mesa-diverted/x86_64-linux-gnu'), PosixPath('/usr/lib/x86_64-linux-gnu/mesa'), PosixPath('/usr/lib/x86_64-linux-gnu/gallium-pipe')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib/mesa-diverted/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/mesa:/usr/lib/x86_64-linux-gnu/dri:/usr/lib/x86_64-linux-gnu/gallium-pipe did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('0')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/tian-desktop'), PosixPath('@/tmp/.ICE-unix/4742,unix/tian-desktop')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/tian/mambaforge/etc/xml/catalog file'), PosixPath('file')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/peft/tuners/lora.py:230: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 348342272 || trainable%: 0.4515283175278825\n"
     ]
    }
   ],
   "source": [
    "# more with LoRAconfig: https://huggingface.co/docs/peft/conceptual_guides/lora\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    r=16,\n",
    "    # alpha: LoRA scaling factor.\n",
    "    lora_alpha=32, \n",
    "    # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    fan_in_fan_out=True,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 44980224 || all params: 348342272 || trainable%: 12.912651611803232\n"
     ]
    }
   ],
   "source": [
    "# make model's embed_tokens layer also trainable\n",
    "\n",
    "model.biogpt.embed_tokens[0].weight.requires_grad = True\n",
    "model.output_projection[0].weight.requires_grad = True\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BioGptForCausalLM(\n",
       "      (biogpt): BioGptModel(\n",
       "        (embed_tokens): CastOutputToFloat(\n",
       "          (0): Embedding(42390, 1024)\n",
       "        )\n",
       "        (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x BioGptDecoderLayer(\n",
       "            (self_attn): BioGptAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output_projection): CastOutputToFloat(\n",
       "        (0): Linear(in_features=1024, out_features=42390, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.biogpt.embed_tokens.0.weight torch.Size([42390, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# for model, print the layer's name if the layer is trainable, and print the precision of the layer\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape, param.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_preprocessing import make_GPT_re_data, GPT_w_ner_preprocess_function\n",
    "\n",
    "from data_preprocessing import all_line_of_pmid, get_original_text, get_identifier_and_entity, reorder_list, get_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and valid file paths\n",
    "train_file_path = 'data/BioRED/processed/train.tsv'\n",
    "valid_file_path = 'data/BioRED/processed/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=train_file_path\n",
    "lower=True\n",
    "from relations import relations\n",
    "import random\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 87\u001b[0m\n\u001b[1;32m     83\u001b[0m     reordered_e2[j] \u001b[39m=\u001b[39m new_string\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m dataset\u001b[39m.\u001b[39miloc[i, \u001b[39m3\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m(entities\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]:\n\u001b[1;32m     86\u001b[0m     \u001b[39m# reordered_e1 is the source\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     output_line \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe source is \u001b[39m\u001b[39m{\u001b[39;00mreordered_e1[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m and the target is \u001b[39m\u001b[39m{\u001b[39;00mreordered_e2[\u001b[39m0\u001b[39;49m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     included_relations[dataset\u001b[39m.\u001b[39miloc[i, \u001b[39m8\u001b[39m]\u001b[39m.\u001b[39mlower()][\u001b[39m'\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(output_line)\n\u001b[1;32m     89\u001b[0m     included_relations[dataset\u001b[39m.\u001b[39miloc[i, \u001b[39m8\u001b[39m]\u001b[39m.\u001b[39mlower()][\u001b[39m'\u001b[39m\u001b[39mentities\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend((reordered_e1, reordered_e2))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# def make_GPT_re_data_no_ner(file_path, lower=True):\n",
    "\"\"\"make a dictionary for the dataset for GPT_re\n",
    "input is the .tsv file path\n",
    "return the hugging face dataset\n",
    "data_dict = {\n",
    "    \"pmids\": [],\n",
    "    \"text\": [],\n",
    "    \"entities\": [],\n",
    "    \"outputs\": []\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"make a dictionary for the dataset for GPT_re\n",
    "input is the .tsv file path\n",
    "return the hugging face dataset\n",
    "data_dict = {\n",
    "    \"pmids\": [],\n",
    "    \"text\": [],\n",
    "    \"entities\": [],\n",
    "    \"outputs\": [],\n",
    "    \"relations\": [] (lower)\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "random.seed(10)\n",
    "data_dict = {\n",
    "    \"pmids\": [],\n",
    "    \"text\": [],\n",
    "    \"entities\": [],\n",
    "    \"outputs\": [],\n",
    "    \"relation\": []\n",
    "}\n",
    "dropped = []\n",
    "dataset = pd.read_csv(file_path, delimiter=\"\\t\", header=None)\n",
    "\n",
    "# relations_dict = {relations[i]: f\"[RELATION{i}]\" for i in range(len(relations))}\n",
    "\n",
    "# tag_dict = {}\n",
    "start = 0\n",
    "while start < (len(dataset) - 1):\n",
    "# while start == 0:\n",
    "    pmid, start, end = all_line_of_pmid(dataset, start)\n",
    "    # the text\n",
    "    text = get_original_text(dataset, start, end, lower)\n",
    "\n",
    "    pmid = dataset.iloc[start, 0]\n",
    "    included_relations = {}\n",
    "    for i in range(start, end):\n",
    "        if dataset.iloc[i, 8] == \"None\":\n",
    "            continue\n",
    "        else:\n",
    "            included_relations[dataset.iloc[i, 8].lower()] = {\n",
    "                'entities':[],\n",
    "                'outputs':[],\n",
    "                'output_lines':[]\n",
    "            }\n",
    "    \n",
    "    # entities_list: [(source, target), (source, target), ...)] \n",
    "\n",
    "    # for i in range(start, start + 1):\n",
    "    for i in range(start, end):\n",
    "        if dataset.iloc[i, 8] == \"None\":\n",
    "            continue\n",
    "        # get entities and their identifiers of this line. the 1st item of entities is the src and the 2nd is the tgt\n",
    "        entities, entity_to_identifier = get_identifier_and_entity(dataset, i, i + 1, lower)\n",
    "\n",
    "        # 1) reorder the entities, find the first occurred entity as the entity1\n",
    "        reordered_e1 = reorder_list(text, {list(entities.items())[0][0]:list(entities.items())[0][1]}, lower, mode='length')\n",
    "        reordered_e2 = reorder_list(text, {list(entities.items())[1][0]:list(entities.items())[1][1]}, lower, mode='length')\n",
    "        if len(reordered_e1) == 0 or len(reordered_e2) == 0:\n",
    "            dropped.append(i)\n",
    "            continue\n",
    "        # for strings in reordered_e1 and reordered_e2, if there is a space before or after a dot, delete the space\n",
    "        for j in range(len(reordered_e1)):\n",
    "            new_string = \".\".join(reordered_e1[j].split(\" . \"))\n",
    "            new_string = \".\".join(new_string.split(\" .\"))\n",
    "            new_string = \".\". join (new_string.split(\". \"))\n",
    "            reordered_e1[j] = new_string\n",
    "\n",
    "        for j in range(len(reordered_e2)):\n",
    "            new_string = \".\".join(reordered_e2[j].split(\" . \"))\n",
    "            new_string = \".\".join(new_string.split(\" .\"))\n",
    "            new_string = \".\". join (new_string.split(\". \"))\n",
    "            reordered_e2[j] = new_string\n",
    "\n",
    "        if dataset.iloc[i, 3] == list(entities.keys())[0]:\n",
    "            # reordered_e1 is the source\n",
    "            output_line = f\"the source is {reordered_e1[0]} and the target is {reordered_e2[0]}\"\n",
    "            included_relations[dataset.iloc[i, 8].lower()]['outputs'].append(output_line)\n",
    "            included_relations[dataset.iloc[i, 8].lower()]['entities'].append((reordered_e1, reordered_e2))\n",
    "        elif dataset.iloc[i, 3] == list(entities.keys())[1]:\n",
    "            # reordered_e2 is the source\n",
    "            output_line = f\"the source is {reordered_e2[0]} and the target is {reordered_e1[0]}\"\n",
    "            included_relations[dataset.iloc[i, 8].lower()]['outputs'].append(output_line)\n",
    "            included_relations[dataset.iloc[i, 8].lower()]['entities'].append((reordered_e2, reordered_e1))\n",
    "        else:\n",
    "            dropped.append(i)\n",
    "            print(\"error in line: \", i)\n",
    "            continue\n",
    "\n",
    "    for r, v in included_relations.items():\n",
    "        data_dict[\"pmids\"].append(str(pmid))\n",
    "        data_dict[\"text\"].append(text.strip())\n",
    "        data_dict[\"relation\"].append(r.lower().strip())\n",
    "        out_line = \"\"\n",
    "        for line in v['outputs']:\n",
    "            out_line += line.lower().strip() + \" ; \"\n",
    "        out_line = out_line[:-3] + \" . \"\n",
    "        data_dict[\"outputs\"].append(out_line)\n",
    "        data_dict['entities'].append(v['entities'])\n",
    "    # randomly choosing a itwm that is in the relations and not in the included_relations.keys()\n",
    "    # have a random index in len(relations)\n",
    "    # if the index is in the included_relations.keys(), continue\n",
    "    # else, add the relation to the included_relations.keys()\n",
    "    random_index = random.randint(0, len(relations) - 1)\n",
    "    while relations[random_index].lower() in included_relations.keys() or relations[random_index].lower() == \"none\":\n",
    "        random_index = random.randint(0, len(relations) - 1)\n",
    "    \n",
    "    # add the relation to the included_relations.keys()\n",
    "    data_dict[\"pmids\"].append(str(pmid))\n",
    "    data_dict[\"text\"].append(text.strip())\n",
    "    data_dict[\"relation\"].append(relations[random_index].lower().strip())\n",
    "    data_dict[\"outputs\"].append(\"the source is none . \")\n",
    "    data_dict['entities'].append([(['none'], ['none'])])\n",
    "    start = end\n",
    "\n",
    "if dropped:\n",
    "    print(f\"Dropped {len(dropped)} line:\\n {dropped}\")\n",
    "\n",
    "# for k,v in data_dict.items():\n",
    "    # print(k, v)\n",
    "# return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reordered_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['type ii diabetes',\n",
       "  'type ii diabetic',\n",
       "  'maturity-onset diabetes',\n",
       "  'type ii diabetes mellitus',\n",
       "  'type ii ( non-insulin-dependent ) diabetes mellitus'],\n",
       " ['hnf-6', 'hepatocyte nuclear factor-6', 'hepatocyte nuclear factor (hnf)-6'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['entities'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3175'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entities.keys())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'association': {'entities': [], 'outputs': []},\n",
       " 'positive_correlation': {'entities': [], 'outputs': []}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "included_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'none'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m included_relations[dataset\u001b[39m.\u001b[39;49miloc[i, \u001b[39m8\u001b[39;49m]\u001b[39m.\u001b[39;49mlower()][\u001b[39m'\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'none'"
     ]
    }
   ],
   "source": [
    "included_relations[dataset.iloc[i, 8].lower()]['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hnf-6'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_e2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hnf-6', 'hepatocyte nuclear factor-6', 'hepatocyte nuclear factor (hnf)-6']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type ii diabetes',\n",
       " 'type ii diabetic',\n",
       " 'maturity-onset diabetes',\n",
       " 'type ii diabetes mellitus',\n",
       " 'type ii ( non-insulin-dependent ) diabetes mellitus']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hnf-6': {'entity_type': 'GeneOrGeneProduct', 'identifier': '3175'},\n",
       " 'hepatocyte nuclear factor-6': {'entity_type': 'GeneOrGeneProduct',\n",
       "  'identifier': '3175'},\n",
       " 'hepatocyte nuclear factor (hnf)-6': {'entity_type': 'GeneOrGeneProduct',\n",
       "  'identifier': '3175'},\n",
       " 'mody4': {'entity_type': 'GeneOrGeneProduct', 'identifier': '3651'},\n",
       " 'mody': {'entity_type': 'GeneOrGeneProduct', 'identifier': '3651'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_to_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hepatocyte nuclear factor-6', 'hepatocyte nuclear factor (hnf)-6', 'hnf-6']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordered_e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_GPT_re_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# make bert_re data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data_raw \u001b[39m=\u001b[39m make_GPT_re_data(file_path\u001b[39m=\u001b[39mtrain_file_path, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m valid_data_raw \u001b[39m=\u001b[39m make_GPT_re_data(file_path\u001b[39m=\u001b[39mvalid_file_path, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_GPT_re_data' is not defined"
     ]
    }
   ],
   "source": [
    "# make bert_re data\n",
    "train_data_raw = make_GPT_re_data(file_path=train_file_path, lower=True)\n",
    "valid_data_raw = make_GPT_re_data(file_path=valid_file_path, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the raw data\n",
    "# import json\n",
    "\n",
    "# with open('GPT_w_ner/data/train_data_dict.json', 'w') as f:\n",
    "#     json.dump(train_data_raw, f)\n",
    "\n",
    "# with open('GPT_w_ner/data/valid_data_dict.json', 'w') as f:\n",
    "#     json.dump(valid_data_raw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('GPT_w_ner/data/train_data_dict.json', 'r') as f:\n",
    "#     train_data_raw = json.load(f)\n",
    "\n",
    "# with open('GPT_w_ner/data/valid_data_dict.json', 'r') as f:\n",
    "#     valid_data_raw = json.load(f)\n",
    "\n",
    "# print(train_data_raw.keys())\n",
    "# for k, v in train_data_raw.items():\n",
    "#     print(k, len(v))\n",
    "\n",
    "# # make into Dataset type\n",
    "# train_data_raw = Dataset.from_dict(train_data_raw)\n",
    "# valid_data_raw = Dataset.from_dict(valid_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\"\"\"\n",
    "for the train_dataset:\n",
    "{'[None]': 18720,\n",
    " '[Association]': 2183,\n",
    " '[Bind]': 60,\n",
    " '[Comparison]': 28,\n",
    " '[Conversion]': 3,\n",
    " '[Cotreatment]': 31,\n",
    " '[Drug_Interaction]': 11,\n",
    " '[Negative_Correlation]': 763,\n",
    " '[Positive_Correlation]': 1088}\n",
    "\n",
    "so it is neccessary to balance the dataset, we randomly choose 3000 samples from the [None] class with the seed 42\n",
    "\"\"\"\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# get the index of the [None] class of the datasets type of train_data_raw\n",
    "none_index = [i for i, example in enumerate(train_data_raw) if example['relation'] == 'none']\n",
    "\n",
    "# randomly choose 18720-3000 samples from the [None] class\n",
    "none_index = random.sample(none_index, 18720-3000)\n",
    "keep_indices = [i for i in range(len(train_data_raw)) if i not in none_index]\n",
    "\n",
    "# delete the [None] class samples from the train_data_raw\n",
    "train_data_raw_balanced = train_data_raw.select(keep_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_data_raw_balanced'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_data_raw_balanced\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_data_raw_balanced[0]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_data_raw_balanced[0]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset = DatasetDict({\\n    \"train\": train_data_raw_balanced,\\n    \"valid\": valid_data_raw\\n})'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"dataset = DatasetDict({\n",
    "    \"train\": train_data_raw_balanced,\n",
    "    \"valid\": valid_data_raw\n",
    "})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenized_datasets = dataset.map(lambda example: GPT_w_ner_preprocess_function(example, tokenizer, mode=\"gpt_w_ner\"), batched=True, remove_columns=[\\'pmids\\', \\'text\\', \\'entities\\', \\'outputs\\', \\'relation\\'])'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tokenized_datasets = dataset.map(lambda example: GPT_w_ner_preprocess_function(example, tokenizer, mode=\"gpt_w_ner\"), batched=True, remove_columns=['pmids', 'text', 'entities', 'outputs', 'relation'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets.save_to_disk('GPT_w_ner/data/tokenized_dataset_w_ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 7167\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 6650\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_datasets = load_from_disk('GPT_w_ner/data/tokenized_dataset_w_ner')\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hepatocyte nuclear factor-6: associations between genetic variability and type ii diabetes and between genetic variability and estimates of insulin secretion. the transcription factor hepatocyte nuclear factor (hnf) -6 is an upstream regulator of several genes involved in the pathogenesis of maturity-onset diabetes of the young. we therefore tested the hypothesis that variability in the hnf-6 gene is associated with subsets of type ii (non-insulin-dependent) diabetes mellitus and estimates of insulin secretion in glucose tolerant subjects. we cloned the coding region as well as the intron-exon boundaries of the hnf-6 gene. w e then examined them on genomic dna in six mody probands without mutations in the mody1, mody3 and mody4 genes and in 54 patients with late-onset type ii diabetes by combined single strand conformational polymorphism-heteroduplex analysis followed by direct sequencing of identified variants. an identified missense variant was examined in association studies and genotype-phenotype studies. we identified two silent and one missense (pro75 ala) variant. i n an association study the allelic frequency of the pro75ala polymorphism was 3.2% (95% confidence interval, 1.9 - 4.5) in 330 patients with type ii diabetes mellitus compared with 4.2% (2.4 - 6.0) in 238 age-matched glucose tolerant control subjects. moreover, in studies of 238 middle-aged glucose tolerant subjects, of 226 glucose tolerant offspring of type ii diabetic patients and of 367 young healthy subjects, the carriers of the polymorphism did not differ from non-carriers in glucose induced serum insulin or c-peptide responses. mutations in the coding region of the hnf-6 gene are not associated with type ii diabetes or with changes in insulin responses to glucose among the caucasians examined. entity 1: hepatocyte nuclear factor-6; hepatocyte nuclear factor (hnf) -6; hnf-6. entity 2: mody; mody1. [learn1] [learn2] [learn3] [learn4] [learn5] [learn6] the relation between the source entity 1 and the target entity 2 is None. </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/BioRED/wandb/run-20230711_190351-1odgi893</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/GPT2/runs/1odgi893' target=\"_blank\">BioGPT_w_ner_epoch_5_balanced_train_data_no_[]</a></strong> to <a href='https://wandb.ai/tian1995/GPT2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/GPT2' target=\"_blank\">https://wandb.ai/tian1995/GPT2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/GPT2/runs/1odgi893' target=\"_blank\">https://wandb.ai/tian1995/GPT2/runs/1odgi893</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/GPT2/runs/1odgi893?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9cb87e4cd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"GPT2\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"BioGPT_w_ner_epoch_5_balanced_train_data_no_[]\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e515f8c3a968422b9617eff2f594f9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2103, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.01}\n",
      "{'loss': 3.2309, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.02}\n",
      "{'loss': 3.2165, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.03}\n",
      "{'loss': 3.2313, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.04}\n",
      "{'loss': 3.1868, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 3.1758, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.05}\n",
      "{'loss': 3.1678, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 3.2831, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 3.3207, 'learning_rate': 1.8e-06, 'epoch': 0.08}\n",
      "{'loss': 3.1115, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2277, 'learning_rate': 2.2e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2029, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.11}\n",
      "{'loss': 3.2163, 'learning_rate': 2.6e-06, 'epoch': 0.12}\n",
      "{'loss': 3.3188, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.12}\n",
      "{'loss': 3.2118, 'learning_rate': 3e-06, 'epoch': 0.13}\n",
      "{'loss': 3.1733, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.14}\n",
      "{'loss': 3.2972, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.15}\n",
      "{'loss': 3.2289, 'learning_rate': 3.6e-06, 'epoch': 0.16}\n",
      "{'loss': 3.2383, 'learning_rate': 3.8e-06, 'epoch': 0.17}\n",
      "{'loss': 3.2181, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.18}\n",
      "{'loss': 3.1647, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.19}\n",
      "{'loss': 3.1948, 'learning_rate': 4.4e-06, 'epoch': 0.2}\n",
      "{'loss': 3.1486, 'learning_rate': 4.6e-06, 'epoch': 0.21}\n",
      "{'loss': 3.3072, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.21}\n",
      "{'loss': 3.2325, 'learning_rate': 5e-06, 'epoch': 0.22}\n",
      "{'loss': 3.1667, 'learning_rate': 5.2e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2477, 'learning_rate': 5.4e-06, 'epoch': 0.24}\n",
      "{'loss': 3.198, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2287, 'learning_rate': 5.8e-06, 'epoch': 0.26}\n",
      "{'loss': 3.1986, 'learning_rate': 6e-06, 'epoch': 0.27}\n",
      "{'loss': 3.1897, 'learning_rate': 6.2e-06, 'epoch': 0.28}\n",
      "{'loss': 3.1475, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.29}\n",
      "{'loss': 3.1888, 'learning_rate': 6.6e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2349, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.3}\n",
      "{'loss': 3.1838, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.1928, 'learning_rate': 7.2e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2263, 'learning_rate': 7.4e-06, 'epoch': 0.33}\n",
      "{'loss': 3.1724, 'learning_rate': 7.6e-06, 'epoch': 0.34}\n",
      "{'loss': 3.0649, 'learning_rate': 7.8e-06, 'epoch': 0.35}\n",
      "{'loss': 3.1969, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.36}\n",
      "{'loss': 3.1541, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.37}\n",
      "{'loss': 3.1249, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.38}\n",
      "{'loss': 3.1859, 'learning_rate': 8.599999999999999e-06, 'epoch': 0.38}\n",
      "{'loss': 3.0919, 'learning_rate': 8.8e-06, 'epoch': 0.39}\n",
      "{'loss': 3.1276, 'learning_rate': 9e-06, 'epoch': 0.4}\n",
      "{'loss': 3.1868, 'learning_rate': 9.2e-06, 'epoch': 0.41}\n",
      "{'loss': 3.0742, 'learning_rate': 9.4e-06, 'epoch': 0.42}\n",
      "{'loss': 3.1883, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.43}\n",
      "{'loss': 3.2007, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.44}\n",
      "{'loss': 3.1329, 'learning_rate': 1e-05, 'epoch': 0.45}\n",
      "{'loss': 3.1655, 'learning_rate': 1.02e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1443, 'learning_rate': 1.04e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1108, 'learning_rate': 1.06e-05, 'epoch': 0.47}\n",
      "{'loss': 3.1737, 'learning_rate': 1.08e-05, 'epoch': 0.48}\n",
      "{'loss': 3.0833, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1317, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.5}\n",
      "{'loss': 3.157, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.51}\n",
      "{'loss': 3.1136, 'learning_rate': 1.16e-05, 'epoch': 0.52}\n",
      "{'loss': 3.0959, 'learning_rate': 1.18e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1026, 'learning_rate': 1.2e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1919, 'learning_rate': 1.22e-05, 'epoch': 0.54}\n",
      "{'loss': 3.0749, 'learning_rate': 1.24e-05, 'epoch': 0.55}\n",
      "{'loss': 3.0605, 'learning_rate': 1.2600000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 3.0175, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.57}\n",
      "{'loss': 2.9823, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.58}\n",
      "{'loss': 3.0408, 'learning_rate': 1.32e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1251, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1563, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.61}\n",
      "{'loss': 3.0242, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1176, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.62}\n",
      "{'loss': 3.0686, 'learning_rate': 1.42e-05, 'epoch': 0.63}\n",
      "{'loss': 3.0109, 'learning_rate': 1.44e-05, 'epoch': 0.64}\n",
      "{'loss': 3.0052, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.65}\n",
      "{'loss': 3.0068, 'learning_rate': 1.48e-05, 'epoch': 0.66}\n",
      "{'loss': 3.0239, 'learning_rate': 1.5e-05, 'epoch': 0.67}\n",
      "{'loss': 2.9646, 'learning_rate': 1.52e-05, 'epoch': 0.68}\n",
      "{'loss': 3.0211, 'learning_rate': 1.54e-05, 'epoch': 0.69}\n",
      "{'loss': 3.0976, 'learning_rate': 1.56e-05, 'epoch': 0.7}\n",
      "{'loss': 2.9751, 'learning_rate': 1.58e-05, 'epoch': 0.71}\n",
      "{'loss': 2.9535, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.71}\n",
      "{'loss': 2.954, 'learning_rate': 1.62e-05, 'epoch': 0.72}\n",
      "{'loss': 2.9995, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.73}\n",
      "{'loss': 3.0555, 'learning_rate': 1.66e-05, 'epoch': 0.74}\n",
      "{'loss': 2.9629, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.75}\n",
      "{'loss': 3.0018, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.76}\n",
      "{'loss': 2.9353, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.77}\n",
      "{'loss': 2.9665, 'learning_rate': 1.74e-05, 'epoch': 0.78}\n",
      "{'loss': 2.9759, 'learning_rate': 1.76e-05, 'epoch': 0.79}\n",
      "{'loss': 2.9171, 'learning_rate': 1.78e-05, 'epoch': 0.79}\n",
      "{'loss': 2.9365, 'learning_rate': 1.8e-05, 'epoch': 0.8}\n",
      "{'loss': 2.9602, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.81}\n",
      "{'loss': 2.9775, 'learning_rate': 1.84e-05, 'epoch': 0.82}\n",
      "{'loss': 2.8563, 'learning_rate': 1.86e-05, 'epoch': 0.83}\n",
      "{'loss': 2.9344, 'learning_rate': 1.88e-05, 'epoch': 0.84}\n",
      "{'loss': 2.8763, 'learning_rate': 1.9e-05, 'epoch': 0.85}\n",
      "{'loss': 2.9746, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.86}\n",
      "{'loss': 2.886, 'learning_rate': 1.94e-05, 'epoch': 0.87}\n",
      "{'loss': 2.8819, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.88}\n",
      "{'loss': 2.9209, 'learning_rate': 1.9800000000000004e-05, 'epoch': 0.88}\n",
      "{'loss': 2.9434, 'learning_rate': 2e-05, 'epoch': 0.89}\n",
      "{'loss': 2.8332, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.9}\n",
      "{'loss': 2.8553, 'learning_rate': 2.04e-05, 'epoch': 0.91}\n",
      "{'loss': 2.8049, 'learning_rate': 2.06e-05, 'epoch': 0.92}\n",
      "{'loss': 2.7741, 'learning_rate': 2.08e-05, 'epoch': 0.93}\n",
      "{'loss': 2.8889, 'learning_rate': 2.1e-05, 'epoch': 0.94}\n",
      "{'loss': 2.903, 'learning_rate': 2.12e-05, 'epoch': 0.95}\n",
      "{'loss': 2.8557, 'learning_rate': 2.1400000000000002e-05, 'epoch': 0.96}\n",
      "{'loss': 2.7775, 'learning_rate': 2.16e-05, 'epoch': 0.96}\n",
      "{'loss': 2.7804, 'learning_rate': 2.18e-05, 'epoch': 0.97}\n",
      "{'loss': 2.8439, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.98}\n",
      "{'loss': 2.7414, 'learning_rate': 2.22e-05, 'epoch': 0.99}\n",
      "{'loss': 2.8149, 'learning_rate': 2.2400000000000002e-05, 'epoch': 1.0}\n",
      "{'loss': 2.792, 'learning_rate': 2.26e-05, 'epoch': 1.01}\n",
      "{'loss': 2.6642, 'learning_rate': 2.2800000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 2.7861, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.03}\n",
      "{'loss': 2.773, 'learning_rate': 2.32e-05, 'epoch': 1.04}\n",
      "{'loss': 2.6615, 'learning_rate': 2.3400000000000003e-05, 'epoch': 1.04}\n",
      "{'loss': 2.6914, 'learning_rate': 2.36e-05, 'epoch': 1.05}\n",
      "{'loss': 2.7251, 'learning_rate': 2.38e-05, 'epoch': 1.06}\n",
      "{'loss': 2.7277, 'learning_rate': 2.4e-05, 'epoch': 1.07}\n",
      "{'loss': 2.7374, 'learning_rate': 2.4200000000000002e-05, 'epoch': 1.08}\n",
      "{'loss': 2.6746, 'learning_rate': 2.44e-05, 'epoch': 1.09}\n",
      "{'loss': 2.6447, 'learning_rate': 2.46e-05, 'epoch': 1.1}\n",
      "{'loss': 2.6203, 'learning_rate': 2.48e-05, 'epoch': 1.11}\n",
      "{'loss': 2.6097, 'learning_rate': 2.5e-05, 'epoch': 1.12}\n",
      "{'loss': 2.6431, 'learning_rate': 2.5200000000000003e-05, 'epoch': 1.12}\n",
      "{'loss': 2.5962, 'learning_rate': 2.54e-05, 'epoch': 1.13}\n",
      "{'loss': 2.6795, 'learning_rate': 2.5600000000000002e-05, 'epoch': 1.14}\n",
      "{'loss': 2.6384, 'learning_rate': 2.58e-05, 'epoch': 1.15}\n",
      "{'loss': 2.5321, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.16}\n",
      "{'loss': 2.5819, 'learning_rate': 2.6200000000000003e-05, 'epoch': 1.17}\n",
      "{'loss': 2.615, 'learning_rate': 2.64e-05, 'epoch': 1.18}\n",
      "{'loss': 2.5213, 'learning_rate': 2.6600000000000003e-05, 'epoch': 1.19}\n",
      "{'loss': 2.5298, 'learning_rate': 2.6800000000000004e-05, 'epoch': 1.2}\n",
      "{'loss': 2.4712, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.21}\n",
      "{'loss': 2.467, 'learning_rate': 2.7200000000000004e-05, 'epoch': 1.21}\n",
      "{'loss': 2.5688, 'learning_rate': 2.7400000000000002e-05, 'epoch': 1.22}\n",
      "{'loss': 2.4935, 'learning_rate': 2.7600000000000003e-05, 'epoch': 1.23}\n",
      "{'loss': 2.4774, 'learning_rate': 2.7800000000000005e-05, 'epoch': 1.24}\n",
      "{'loss': 2.4038, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.25}\n",
      "{'loss': 2.454, 'learning_rate': 2.8199999999999998e-05, 'epoch': 1.26}\n",
      "{'loss': 2.3988, 'learning_rate': 2.84e-05, 'epoch': 1.27}\n",
      "{'loss': 2.3273, 'learning_rate': 2.86e-05, 'epoch': 1.28}\n",
      "{'loss': 2.3805, 'learning_rate': 2.88e-05, 'epoch': 1.29}\n",
      "{'loss': 2.4666, 'learning_rate': 2.9e-05, 'epoch': 1.29}\n",
      "{'loss': 2.4114, 'learning_rate': 2.9199999999999998e-05, 'epoch': 1.3}\n",
      "{'loss': 2.3776, 'learning_rate': 2.94e-05, 'epoch': 1.31}\n",
      "{'loss': 2.2935, 'learning_rate': 2.96e-05, 'epoch': 1.32}\n",
      "{'loss': 2.4346, 'learning_rate': 2.98e-05, 'epoch': 1.33}\n",
      "{'loss': 2.2917, 'learning_rate': 3e-05, 'epoch': 1.34}\n",
      "{'loss': 2.2938, 'learning_rate': 3.02e-05, 'epoch': 1.35}\n",
      "{'loss': 2.3725, 'learning_rate': 3.04e-05, 'epoch': 1.36}\n",
      "{'loss': 2.277, 'learning_rate': 3.06e-05, 'epoch': 1.37}\n",
      "{'loss': 2.305, 'learning_rate': 3.08e-05, 'epoch': 1.38}\n",
      "{'loss': 2.2394, 'learning_rate': 3.1e-05, 'epoch': 1.38}\n",
      "{'loss': 2.1767, 'learning_rate': 3.12e-05, 'epoch': 1.39}\n",
      "{'loss': 2.2873, 'learning_rate': 3.1400000000000004e-05, 'epoch': 1.4}\n",
      "{'loss': 2.3177, 'learning_rate': 3.16e-05, 'epoch': 1.41}\n",
      "{'loss': 2.3391, 'learning_rate': 3.18e-05, 'epoch': 1.42}\n",
      "{'loss': 2.3155, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.43}\n",
      "{'loss': 2.1493, 'learning_rate': 3.2200000000000003e-05, 'epoch': 1.44}\n",
      "{'loss': 2.2272, 'learning_rate': 3.24e-05, 'epoch': 1.45}\n",
      "{'loss': 2.2423, 'learning_rate': 3.26e-05, 'epoch': 1.46}\n",
      "{'loss': 2.2362, 'learning_rate': 3.26e-05, 'epoch': 1.46}\n",
      "{'loss': 2.2364, 'learning_rate': 3.2800000000000004e-05, 'epoch': 1.47}\n",
      "{'loss': 2.1813, 'learning_rate': 3.3e-05, 'epoch': 1.48}\n",
      "{'loss': 2.1986, 'learning_rate': 3.32e-05, 'epoch': 1.49}\n",
      "{'loss': 2.2383, 'learning_rate': 3.3400000000000005e-05, 'epoch': 1.5}\n",
      "{'loss': 2.2035, 'learning_rate': 3.3600000000000004e-05, 'epoch': 1.51}\n",
      "{'loss': 2.2074, 'learning_rate': 3.38e-05, 'epoch': 1.52}\n",
      "{'loss': 2.2287, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.53}\n",
      "{'loss': 2.128, 'learning_rate': 3.4200000000000005e-05, 'epoch': 1.54}\n",
      "{'loss': 2.1706, 'learning_rate': 3.4399999999999996e-05, 'epoch': 1.54}\n",
      "{'loss': 2.2306, 'learning_rate': 3.46e-05, 'epoch': 1.55}\n",
      "{'loss': 2.113, 'learning_rate': 3.48e-05, 'epoch': 1.56}\n",
      "{'loss': 2.1253, 'learning_rate': 3.5e-05, 'epoch': 1.57}\n",
      "{'loss': 2.1406, 'learning_rate': 3.52e-05, 'epoch': 1.58}\n",
      "{'loss': 2.0997, 'learning_rate': 3.54e-05, 'epoch': 1.59}\n",
      "{'loss': 2.11, 'learning_rate': 3.56e-05, 'epoch': 1.6}\n",
      "{'loss': 2.0455, 'learning_rate': 3.58e-05, 'epoch': 1.61}\n",
      "{'loss': 2.1158, 'learning_rate': 3.6e-05, 'epoch': 1.62}\n",
      "{'loss': 2.1176, 'learning_rate': 3.62e-05, 'epoch': 1.62}\n",
      "{'loss': 2.0691, 'learning_rate': 3.6400000000000004e-05, 'epoch': 1.63}\n",
      "{'loss': 2.0336, 'learning_rate': 3.66e-05, 'epoch': 1.64}\n",
      "{'loss': 2.074, 'learning_rate': 3.68e-05, 'epoch': 1.65}\n",
      "{'loss': 2.0316, 'learning_rate': 3.7e-05, 'epoch': 1.66}\n",
      "{'loss': 2.0596, 'learning_rate': 3.72e-05, 'epoch': 1.67}\n",
      "{'loss': 2.0717, 'learning_rate': 3.74e-05, 'epoch': 1.68}\n",
      "{'loss': 2.0998, 'learning_rate': 3.76e-05, 'epoch': 1.69}\n",
      "{'loss': 2.1228, 'learning_rate': 3.7800000000000004e-05, 'epoch': 1.7}\n",
      "{'loss': 1.9805, 'learning_rate': 3.8e-05, 'epoch': 1.71}\n",
      "{'loss': 2.1127, 'learning_rate': 3.82e-05, 'epoch': 1.71}\n",
      "{'loss': 2.0387, 'learning_rate': 3.8400000000000005e-05, 'epoch': 1.72}\n",
      "{'loss': 2.0311, 'learning_rate': 3.86e-05, 'epoch': 1.73}\n",
      "{'loss': 2.0274, 'learning_rate': 3.88e-05, 'epoch': 1.74}\n",
      "{'loss': 2.0225, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.75}\n",
      "{'loss': 2.0017, 'learning_rate': 3.9200000000000004e-05, 'epoch': 1.76}\n",
      "{'loss': 1.918, 'learning_rate': 3.94e-05, 'epoch': 1.77}\n",
      "{'loss': 1.99, 'learning_rate': 3.960000000000001e-05, 'epoch': 1.78}\n",
      "{'loss': 2.0472, 'learning_rate': 3.9800000000000005e-05, 'epoch': 1.79}\n",
      "{'loss': 2.0421, 'learning_rate': 4e-05, 'epoch': 1.79}\n",
      "{'loss': 1.9462, 'learning_rate': 4.02e-05, 'epoch': 1.8}\n",
      "{'loss': 1.9887, 'learning_rate': 4.0400000000000006e-05, 'epoch': 1.81}\n",
      "{'loss': 1.968, 'learning_rate': 4.0600000000000004e-05, 'epoch': 1.82}\n",
      "{'loss': 1.9043, 'learning_rate': 4.08e-05, 'epoch': 1.83}\n",
      "{'loss': 1.8984, 'learning_rate': 4.1e-05, 'epoch': 1.84}\n",
      "{'loss': 1.8592, 'learning_rate': 4.12e-05, 'epoch': 1.85}\n",
      "{'loss': 1.8768, 'learning_rate': 4.14e-05, 'epoch': 1.86}\n",
      "{'loss': 1.9349, 'learning_rate': 4.16e-05, 'epoch': 1.87}\n",
      "{'loss': 1.8728, 'learning_rate': 4.18e-05, 'epoch': 1.88}\n",
      "{'loss': 1.8724, 'learning_rate': 4.2e-05, 'epoch': 1.88}\n",
      "{'loss': 1.8388, 'learning_rate': 4.22e-05, 'epoch': 1.89}\n",
      "{'loss': 1.9443, 'learning_rate': 4.24e-05, 'epoch': 1.9}\n",
      "{'loss': 1.8698, 'learning_rate': 4.26e-05, 'epoch': 1.91}\n",
      "{'loss': 1.8919, 'learning_rate': 4.2800000000000004e-05, 'epoch': 1.92}\n",
      "{'loss': 1.8924, 'learning_rate': 4.3e-05, 'epoch': 1.93}\n",
      "{'loss': 1.8225, 'learning_rate': 4.32e-05, 'epoch': 1.94}\n",
      "{'loss': 1.7904, 'learning_rate': 4.3400000000000005e-05, 'epoch': 1.95}\n",
      "{'loss': 1.9419, 'learning_rate': 4.36e-05, 'epoch': 1.96}\n",
      "{'loss': 1.7886, 'learning_rate': 4.38e-05, 'epoch': 1.96}\n",
      "{'loss': 1.8721, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.97}\n",
      "{'loss': 1.831, 'learning_rate': 4.4200000000000004e-05, 'epoch': 1.98}\n",
      "{'loss': 1.8434, 'learning_rate': 4.44e-05, 'epoch': 1.99}\n",
      "{'loss': 1.7397, 'learning_rate': 4.46e-05, 'epoch': 2.0}\n",
      "{'loss': 1.7563, 'learning_rate': 4.4800000000000005e-05, 'epoch': 2.01}\n",
      "{'loss': 1.77, 'learning_rate': 4.5e-05, 'epoch': 2.02}\n",
      "{'loss': 1.8295, 'learning_rate': 4.52e-05, 'epoch': 2.03}\n",
      "{'loss': 1.842, 'learning_rate': 4.5400000000000006e-05, 'epoch': 2.04}\n",
      "{'loss': 1.655, 'learning_rate': 4.5600000000000004e-05, 'epoch': 2.04}\n",
      "{'loss': 1.7047, 'learning_rate': 4.58e-05, 'epoch': 2.05}\n",
      "{'loss': 1.7761, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.06}\n",
      "{'loss': 1.779, 'learning_rate': 4.6200000000000005e-05, 'epoch': 2.07}\n",
      "{'loss': 1.7914, 'learning_rate': 4.64e-05, 'epoch': 2.08}\n",
      "{'loss': 1.7042, 'learning_rate': 4.660000000000001e-05, 'epoch': 2.09}\n",
      "{'loss': 1.7565, 'learning_rate': 4.6800000000000006e-05, 'epoch': 2.1}\n",
      "{'loss': 1.7718, 'learning_rate': 4.7e-05, 'epoch': 2.11}\n",
      "{'loss': 1.6939, 'learning_rate': 4.72e-05, 'epoch': 2.12}\n",
      "{'loss': 1.6898, 'learning_rate': 4.74e-05, 'epoch': 2.12}\n",
      "{'loss': 1.5983, 'learning_rate': 4.76e-05, 'epoch': 2.13}\n",
      "{'loss': 1.7443, 'learning_rate': 4.78e-05, 'epoch': 2.14}\n",
      "{'loss': 1.7138, 'learning_rate': 4.8e-05, 'epoch': 2.15}\n",
      "{'loss': 1.6264, 'learning_rate': 4.82e-05, 'epoch': 2.16}\n",
      "{'loss': 1.7284, 'learning_rate': 4.8400000000000004e-05, 'epoch': 2.17}\n",
      "{'loss': 1.6127, 'learning_rate': 4.86e-05, 'epoch': 2.18}\n",
      "{'loss': 1.6049, 'learning_rate': 4.88e-05, 'epoch': 2.19}\n",
      "{'loss': 1.6556, 'learning_rate': 4.9e-05, 'epoch': 2.2}\n",
      "{'loss': 1.6258, 'learning_rate': 4.92e-05, 'epoch': 2.21}\n",
      "{'loss': 1.6744, 'learning_rate': 4.94e-05, 'epoch': 2.21}\n",
      "{'loss': 1.6639, 'learning_rate': 4.96e-05, 'epoch': 2.22}\n",
      "{'loss': 1.7654, 'learning_rate': 4.9800000000000004e-05, 'epoch': 2.23}\n",
      "{'loss': 1.6003, 'learning_rate': 5e-05, 'epoch': 2.24}\n",
      "{'loss': 1.5915, 'learning_rate': 5.02e-05, 'epoch': 2.25}\n",
      "{'loss': 1.6095, 'learning_rate': 5.0400000000000005e-05, 'epoch': 2.26}\n",
      "{'loss': 1.654, 'learning_rate': 5.0600000000000003e-05, 'epoch': 2.27}\n",
      "{'loss': 1.6338, 'learning_rate': 5.08e-05, 'epoch': 2.28}\n",
      "{'loss': 1.6921, 'learning_rate': 5.1000000000000006e-05, 'epoch': 2.29}\n",
      "{'loss': 1.5581, 'learning_rate': 5.1200000000000004e-05, 'epoch': 2.29}\n",
      "{'loss': 1.6588, 'learning_rate': 5.14e-05, 'epoch': 2.3}\n",
      "{'loss': 1.7377, 'learning_rate': 5.16e-05, 'epoch': 2.31}\n",
      "{'loss': 1.6203, 'learning_rate': 5.1800000000000005e-05, 'epoch': 2.32}\n",
      "{'loss': 1.6129, 'learning_rate': 5.2000000000000004e-05, 'epoch': 2.33}\n",
      "{'loss': 1.6303, 'learning_rate': 5.22e-05, 'epoch': 2.34}\n",
      "{'loss': 1.6736, 'learning_rate': 5.2400000000000007e-05, 'epoch': 2.35}\n",
      "{'loss': 1.5353, 'learning_rate': 5.2600000000000005e-05, 'epoch': 2.36}\n",
      "{'loss': 1.5021, 'learning_rate': 5.28e-05, 'epoch': 2.37}\n",
      "{'loss': 1.5788, 'learning_rate': 5.300000000000001e-05, 'epoch': 2.38}\n",
      "{'loss': 1.5275, 'learning_rate': 5.3200000000000006e-05, 'epoch': 2.38}\n",
      "{'loss': 1.6163, 'learning_rate': 5.3400000000000004e-05, 'epoch': 2.39}\n",
      "{'loss': 1.5666, 'learning_rate': 5.360000000000001e-05, 'epoch': 2.4}\n",
      "{'loss': 1.6474, 'learning_rate': 5.380000000000001e-05, 'epoch': 2.41}\n",
      "{'loss': 1.4618, 'learning_rate': 5.4000000000000005e-05, 'epoch': 2.42}\n",
      "{'loss': 1.568, 'learning_rate': 5.420000000000001e-05, 'epoch': 2.43}\n",
      "{'loss': 1.5103, 'learning_rate': 5.440000000000001e-05, 'epoch': 2.44}\n",
      "{'loss': 1.5427, 'learning_rate': 5.4600000000000006e-05, 'epoch': 2.45}\n",
      "{'loss': 1.5128, 'learning_rate': 5.4800000000000004e-05, 'epoch': 2.46}\n",
      "{'loss': 1.5002, 'learning_rate': 5.500000000000001e-05, 'epoch': 2.46}\n",
      "{'loss': 1.5234, 'learning_rate': 5.520000000000001e-05, 'epoch': 2.47}\n",
      "{'loss': 1.5304, 'learning_rate': 5.5400000000000005e-05, 'epoch': 2.48}\n",
      "{'loss': 1.5281, 'learning_rate': 5.560000000000001e-05, 'epoch': 2.49}\n",
      "{'loss': 1.5071, 'learning_rate': 5.580000000000001e-05, 'epoch': 2.5}\n",
      "{'loss': 1.5006, 'learning_rate': 5.6000000000000006e-05, 'epoch': 2.51}\n",
      "{'loss': 1.6167, 'learning_rate': 5.620000000000001e-05, 'epoch': 2.52}\n",
      "{'loss': 1.537, 'learning_rate': 5.6399999999999995e-05, 'epoch': 2.53}\n",
      "{'loss': 1.4688, 'learning_rate': 5.66e-05, 'epoch': 2.54}\n",
      "{'loss': 1.3911, 'learning_rate': 5.68e-05, 'epoch': 2.54}\n",
      "{'loss': 1.5015, 'learning_rate': 5.6999999999999996e-05, 'epoch': 2.55}\n",
      "{'loss': 1.4788, 'learning_rate': 5.72e-05, 'epoch': 2.56}\n",
      "{'loss': 1.471, 'learning_rate': 5.74e-05, 'epoch': 2.57}\n",
      "{'loss': 1.5011, 'learning_rate': 5.76e-05, 'epoch': 2.58}\n",
      "{'loss': 1.4501, 'learning_rate': 5.7799999999999995e-05, 'epoch': 2.59}\n",
      "{'loss': 1.5719, 'learning_rate': 5.8e-05, 'epoch': 2.6}\n",
      "{'loss': 1.4836, 'learning_rate': 5.82e-05, 'epoch': 2.61}\n",
      "{'loss': 1.5134, 'learning_rate': 5.8399999999999997e-05, 'epoch': 2.62}\n",
      "{'loss': 1.416, 'learning_rate': 5.86e-05, 'epoch': 2.62}\n",
      "{'loss': 1.4044, 'learning_rate': 5.88e-05, 'epoch': 2.63}\n",
      "{'loss': 1.4855, 'learning_rate': 5.9e-05, 'epoch': 2.64}\n",
      "{'loss': 1.3818, 'learning_rate': 5.92e-05, 'epoch': 2.65}\n",
      "{'loss': 1.3955, 'learning_rate': 5.94e-05, 'epoch': 2.66}\n",
      "{'loss': 1.4573, 'learning_rate': 5.96e-05, 'epoch': 2.67}\n",
      "{'loss': 1.4136, 'learning_rate': 5.9800000000000003e-05, 'epoch': 2.68}\n",
      "{'loss': 1.415, 'learning_rate': 6e-05, 'epoch': 2.69}\n",
      "{'loss': 1.3911, 'learning_rate': 6.02e-05, 'epoch': 2.7}\n",
      "{'loss': 1.3989, 'learning_rate': 6.04e-05, 'epoch': 2.71}\n",
      "{'loss': 1.4121, 'learning_rate': 6.06e-05, 'epoch': 2.71}\n",
      "{'loss': 1.4372, 'learning_rate': 6.08e-05, 'epoch': 2.72}\n",
      "{'loss': 1.454, 'learning_rate': 6.1e-05, 'epoch': 2.73}\n",
      "{'loss': 1.5571, 'learning_rate': 6.12e-05, 'epoch': 2.74}\n",
      "{'loss': 1.4985, 'learning_rate': 6.14e-05, 'epoch': 2.75}\n",
      "{'loss': 1.4111, 'learning_rate': 6.16e-05, 'epoch': 2.76}\n",
      "{'loss': 1.3157, 'learning_rate': 6.18e-05, 'epoch': 2.77}\n",
      "{'loss': 1.4636, 'learning_rate': 6.2e-05, 'epoch': 2.78}\n",
      "{'loss': 1.446, 'learning_rate': 6.220000000000001e-05, 'epoch': 2.79}\n",
      "{'loss': 1.3254, 'learning_rate': 6.24e-05, 'epoch': 2.79}\n",
      "{'loss': 1.3904, 'learning_rate': 6.26e-05, 'epoch': 2.8}\n",
      "{'loss': 1.3804, 'learning_rate': 6.280000000000001e-05, 'epoch': 2.81}\n",
      "{'loss': 1.3716, 'learning_rate': 6.3e-05, 'epoch': 2.82}\n",
      "{'loss': 1.3696, 'learning_rate': 6.32e-05, 'epoch': 2.83}\n",
      "{'loss': 1.3525, 'learning_rate': 6.340000000000001e-05, 'epoch': 2.84}\n",
      "{'loss': 1.348, 'learning_rate': 6.36e-05, 'epoch': 2.85}\n",
      "{'loss': 1.3321, 'learning_rate': 6.38e-05, 'epoch': 2.86}\n",
      "{'loss': 1.3062, 'learning_rate': 6.400000000000001e-05, 'epoch': 2.87}\n",
      "{'loss': 1.3372, 'learning_rate': 6.42e-05, 'epoch': 2.88}\n",
      "{'loss': 1.402, 'learning_rate': 6.440000000000001e-05, 'epoch': 2.88}\n",
      "{'loss': 1.3325, 'learning_rate': 6.460000000000001e-05, 'epoch': 2.89}\n",
      "{'loss': 1.2804, 'learning_rate': 6.48e-05, 'epoch': 2.9}\n",
      "{'loss': 1.2711, 'learning_rate': 6.500000000000001e-05, 'epoch': 2.91}\n",
      "{'loss': 1.3464, 'learning_rate': 6.52e-05, 'epoch': 2.92}\n",
      "{'loss': 1.2236, 'learning_rate': 6.54e-05, 'epoch': 2.93}\n",
      "{'loss': 1.3286, 'learning_rate': 6.560000000000001e-05, 'epoch': 2.94}\n",
      "{'loss': 1.2893, 'learning_rate': 6.58e-05, 'epoch': 2.95}\n",
      "{'loss': 1.3511, 'learning_rate': 6.6e-05, 'epoch': 2.96}\n",
      "{'loss': 1.2988, 'learning_rate': 6.620000000000001e-05, 'epoch': 2.96}\n",
      "{'loss': 1.4216, 'learning_rate': 6.64e-05, 'epoch': 2.97}\n",
      "{'loss': 1.2439, 'learning_rate': 6.66e-05, 'epoch': 2.98}\n",
      "{'loss': 1.3379, 'learning_rate': 6.680000000000001e-05, 'epoch': 2.99}\n",
      "{'loss': 1.2929, 'learning_rate': 6.7e-05, 'epoch': 3.0}\n",
      "{'loss': 1.2418, 'learning_rate': 6.720000000000001e-05, 'epoch': 3.01}\n",
      "{'loss': 1.2844, 'learning_rate': 6.740000000000001e-05, 'epoch': 3.02}\n",
      "{'loss': 1.3576, 'learning_rate': 6.76e-05, 'epoch': 3.03}\n",
      "{'loss': 1.32, 'learning_rate': 6.780000000000001e-05, 'epoch': 3.04}\n",
      "{'loss': 1.2394, 'learning_rate': 6.800000000000001e-05, 'epoch': 3.04}\n",
      "{'loss': 1.2394, 'learning_rate': 6.82e-05, 'epoch': 3.05}\n",
      "{'loss': 1.2204, 'learning_rate': 6.840000000000001e-05, 'epoch': 3.06}\n",
      "{'loss': 1.2697, 'learning_rate': 6.860000000000001e-05, 'epoch': 3.07}\n",
      "{'loss': 1.3003, 'learning_rate': 6.879999999999999e-05, 'epoch': 3.08}\n",
      "{'loss': 1.2243, 'learning_rate': 6.9e-05, 'epoch': 3.09}\n",
      "{'loss': 1.2046, 'learning_rate': 6.92e-05, 'epoch': 3.1}\n",
      "{'loss': 1.2482, 'learning_rate': 6.939999999999999e-05, 'epoch': 3.11}\n",
      "{'loss': 1.2604, 'learning_rate': 6.96e-05, 'epoch': 3.12}\n",
      "{'loss': 1.2981, 'learning_rate': 6.98e-05, 'epoch': 3.12}\n",
      "{'loss': 1.3391, 'learning_rate': 7e-05, 'epoch': 3.13}\n",
      "{'loss': 1.255, 'learning_rate': 7.02e-05, 'epoch': 3.14}\n",
      "{'loss': 1.2137, 'learning_rate': 7.04e-05, 'epoch': 3.15}\n",
      "{'loss': 1.1847, 'learning_rate': 7.06e-05, 'epoch': 3.16}\n",
      "{'loss': 1.2065, 'learning_rate': 7.08e-05, 'epoch': 3.17}\n",
      "{'loss': 1.1488, 'learning_rate': 7.1e-05, 'epoch': 3.18}\n",
      "{'loss': 1.1865, 'learning_rate': 7.12e-05, 'epoch': 3.19}\n",
      "{'loss': 1.2029, 'learning_rate': 7.14e-05, 'epoch': 3.2}\n",
      "{'loss': 1.0828, 'learning_rate': 7.16e-05, 'epoch': 3.21}\n",
      "{'loss': 1.2419, 'learning_rate': 7.18e-05, 'epoch': 3.21}\n",
      "{'loss': 1.1985, 'learning_rate': 7.2e-05, 'epoch': 3.22}\n",
      "{'loss': 1.2218, 'learning_rate': 7.22e-05, 'epoch': 3.23}\n",
      "{'loss': 1.2091, 'learning_rate': 7.24e-05, 'epoch': 3.24}\n",
      "{'loss': 1.3417, 'learning_rate': 7.26e-05, 'epoch': 3.25}\n",
      "{'loss': 1.1619, 'learning_rate': 7.280000000000001e-05, 'epoch': 3.26}\n",
      "{'loss': 1.2125, 'learning_rate': 7.3e-05, 'epoch': 3.27}\n",
      "{'loss': 1.1545, 'learning_rate': 7.32e-05, 'epoch': 3.28}\n",
      "{'loss': 1.1213, 'learning_rate': 7.340000000000001e-05, 'epoch': 3.29}\n",
      "{'loss': 1.2074, 'learning_rate': 7.36e-05, 'epoch': 3.29}\n",
      "{'loss': 1.1942, 'learning_rate': 7.38e-05, 'epoch': 3.3}\n",
      "{'loss': 1.1113, 'learning_rate': 7.4e-05, 'epoch': 3.31}\n",
      "{'loss': 1.1745, 'learning_rate': 7.42e-05, 'epoch': 3.32}\n",
      "{'loss': 1.1361, 'learning_rate': 7.44e-05, 'epoch': 3.33}\n",
      "{'loss': 1.1365, 'learning_rate': 7.46e-05, 'epoch': 3.34}\n",
      "{'loss': 1.1614, 'learning_rate': 7.48e-05, 'epoch': 3.35}\n",
      "{'loss': 1.2853, 'learning_rate': 7.500000000000001e-05, 'epoch': 3.36}\n",
      "{'loss': 1.184, 'learning_rate': 7.52e-05, 'epoch': 3.37}\n",
      "{'loss': 1.1497, 'learning_rate': 7.54e-05, 'epoch': 3.38}\n",
      "{'loss': 1.1014, 'learning_rate': 7.560000000000001e-05, 'epoch': 3.38}\n",
      "{'loss': 1.2013, 'learning_rate': 7.58e-05, 'epoch': 3.39}\n",
      "{'loss': 1.0835, 'learning_rate': 7.6e-05, 'epoch': 3.4}\n",
      "{'loss': 1.2253, 'learning_rate': 7.620000000000001e-05, 'epoch': 3.41}\n",
      "{'loss': 1.156, 'learning_rate': 7.64e-05, 'epoch': 3.42}\n",
      "{'loss': 1.032, 'learning_rate': 7.66e-05, 'epoch': 3.43}\n",
      "{'loss': 1.1446, 'learning_rate': 7.680000000000001e-05, 'epoch': 3.44}\n",
      "{'loss': 1.1174, 'learning_rate': 7.7e-05, 'epoch': 3.45}\n",
      "{'loss': 1.0695, 'learning_rate': 7.72e-05, 'epoch': 3.46}\n",
      "{'loss': 1.1241, 'learning_rate': 7.740000000000001e-05, 'epoch': 3.46}\n",
      "{'loss': 1.061, 'learning_rate': 7.76e-05, 'epoch': 3.47}\n",
      "{'loss': 1.0953, 'learning_rate': 7.780000000000001e-05, 'epoch': 3.48}\n",
      "{'loss': 1.1149, 'learning_rate': 7.800000000000001e-05, 'epoch': 3.49}\n",
      "{'loss': 1.0861, 'learning_rate': 7.82e-05, 'epoch': 3.5}\n",
      "{'loss': 1.0401, 'learning_rate': 7.840000000000001e-05, 'epoch': 3.51}\n",
      "{'loss': 1.0137, 'learning_rate': 7.860000000000001e-05, 'epoch': 3.52}\n",
      "{'loss': 1.048, 'learning_rate': 7.88e-05, 'epoch': 3.53}\n",
      "{'loss': 1.1883, 'learning_rate': 7.900000000000001e-05, 'epoch': 3.54}\n",
      "{'loss': 1.099, 'learning_rate': 7.920000000000001e-05, 'epoch': 3.54}\n",
      "{'loss': 1.0653, 'learning_rate': 7.94e-05, 'epoch': 3.55}\n",
      "{'loss': 1.1197, 'learning_rate': 7.960000000000001e-05, 'epoch': 3.56}\n",
      "{'loss': 1.0933, 'learning_rate': 7.98e-05, 'epoch': 3.57}\n",
      "{'loss': 0.9957, 'learning_rate': 8e-05, 'epoch': 3.58}\n",
      "{'loss': 1.068, 'learning_rate': 8.020000000000001e-05, 'epoch': 3.59}\n",
      "{'loss': 1.0183, 'learning_rate': 8.04e-05, 'epoch': 3.6}\n",
      "{'loss': 1.0679, 'learning_rate': 8.060000000000001e-05, 'epoch': 3.61}\n",
      "{'loss': 1.0668, 'learning_rate': 8.080000000000001e-05, 'epoch': 3.62}\n",
      "{'loss': 1.1168, 'learning_rate': 8.1e-05, 'epoch': 3.62}\n",
      "{'loss': 0.9479, 'learning_rate': 8.120000000000001e-05, 'epoch': 3.63}\n",
      "{'loss': 1.1816, 'learning_rate': 8.14e-05, 'epoch': 3.64}\n",
      "{'loss': 1.0443, 'learning_rate': 8.16e-05, 'epoch': 3.65}\n",
      "{'loss': 1.0945, 'learning_rate': 8.18e-05, 'epoch': 3.66}\n",
      "{'loss': 1.0744, 'learning_rate': 8.2e-05, 'epoch': 3.67}\n",
      "{'loss': 1.0722, 'learning_rate': 8.22e-05, 'epoch': 3.68}\n",
      "{'loss': 1.0787, 'learning_rate': 8.24e-05, 'epoch': 3.69}\n",
      "{'loss': 1.1159, 'learning_rate': 8.26e-05, 'epoch': 3.7}\n",
      "{'loss': 1.0996, 'learning_rate': 8.28e-05, 'epoch': 3.71}\n",
      "{'loss': 1.0048, 'learning_rate': 8.3e-05, 'epoch': 3.71}\n",
      "{'loss': 1.0002, 'learning_rate': 8.32e-05, 'epoch': 3.72}\n",
      "{'loss': 0.9711, 'learning_rate': 8.34e-05, 'epoch': 3.73}\n",
      "{'loss': 1.054, 'learning_rate': 8.36e-05, 'epoch': 3.74}\n",
      "{'loss': 1.0565, 'learning_rate': 8.38e-05, 'epoch': 3.75}\n",
      "{'loss': 0.9416, 'learning_rate': 8.4e-05, 'epoch': 3.76}\n",
      "{'loss': 1.0316, 'learning_rate': 8.42e-05, 'epoch': 3.77}\n",
      "{'loss': 1.0755, 'learning_rate': 8.44e-05, 'epoch': 3.78}\n",
      "{'loss': 1.0242, 'learning_rate': 8.46e-05, 'epoch': 3.79}\n",
      "{'loss': 1.0993, 'learning_rate': 8.48e-05, 'epoch': 3.79}\n",
      "{'loss': 1.0979, 'learning_rate': 8.5e-05, 'epoch': 3.8}\n",
      "{'loss': 1.0913, 'learning_rate': 8.52e-05, 'epoch': 3.81}\n",
      "{'loss': 1.0118, 'learning_rate': 8.54e-05, 'epoch': 3.82}\n",
      "{'loss': 0.968, 'learning_rate': 8.560000000000001e-05, 'epoch': 3.83}\n",
      "{'loss': 1.0692, 'learning_rate': 8.58e-05, 'epoch': 3.84}\n",
      "{'loss': 1.0133, 'learning_rate': 8.6e-05, 'epoch': 3.85}\n",
      "{'loss': 0.9857, 'learning_rate': 8.620000000000001e-05, 'epoch': 3.86}\n",
      "{'loss': 0.967, 'learning_rate': 8.64e-05, 'epoch': 3.87}\n",
      "{'loss': 1.0474, 'learning_rate': 8.66e-05, 'epoch': 3.88}\n",
      "{'loss': 0.9731, 'learning_rate': 8.680000000000001e-05, 'epoch': 3.88}\n",
      "{'loss': 0.9072, 'learning_rate': 8.7e-05, 'epoch': 3.89}\n",
      "{'loss': 0.9757, 'learning_rate': 8.72e-05, 'epoch': 3.9}\n",
      "{'loss': 0.9302, 'learning_rate': 8.740000000000001e-05, 'epoch': 3.91}\n",
      "{'loss': 0.9917, 'learning_rate': 8.76e-05, 'epoch': 3.92}\n",
      "{'loss': 0.9388, 'learning_rate': 8.78e-05, 'epoch': 3.93}\n",
      "{'loss': 0.9605, 'learning_rate': 8.800000000000001e-05, 'epoch': 3.94}\n",
      "{'loss': 0.9265, 'learning_rate': 8.82e-05, 'epoch': 3.95}\n",
      "{'loss': 0.8722, 'learning_rate': 8.840000000000001e-05, 'epoch': 3.96}\n",
      "{'loss': 0.8891, 'learning_rate': 8.86e-05, 'epoch': 3.96}\n",
      "{'loss': 0.985, 'learning_rate': 8.88e-05, 'epoch': 3.97}\n",
      "{'loss': 1.0902, 'learning_rate': 8.900000000000001e-05, 'epoch': 3.98}\n",
      "{'loss': 0.8833, 'learning_rate': 8.92e-05, 'epoch': 3.99}\n",
      "{'loss': 0.9832, 'learning_rate': 8.94e-05, 'epoch': 4.0}\n",
      "{'loss': 0.9352, 'learning_rate': 8.960000000000001e-05, 'epoch': 4.01}\n",
      "{'loss': 0.8288, 'learning_rate': 8.98e-05, 'epoch': 4.02}\n",
      "{'loss': 0.8843, 'learning_rate': 9e-05, 'epoch': 4.03}\n",
      "{'loss': 0.9359, 'learning_rate': 9.020000000000001e-05, 'epoch': 4.04}\n",
      "{'loss': 0.9857, 'learning_rate': 9.04e-05, 'epoch': 4.04}\n",
      "{'loss': 0.9752, 'learning_rate': 9.06e-05, 'epoch': 4.05}\n",
      "{'loss': 0.9461, 'learning_rate': 9.080000000000001e-05, 'epoch': 4.06}\n",
      "{'loss': 0.9089, 'learning_rate': 9.1e-05, 'epoch': 4.07}\n",
      "{'loss': 0.9311, 'learning_rate': 9.120000000000001e-05, 'epoch': 4.08}\n",
      "{'loss': 0.9525, 'learning_rate': 9.140000000000001e-05, 'epoch': 4.09}\n",
      "{'loss': 1.0277, 'learning_rate': 9.16e-05, 'epoch': 4.1}\n",
      "{'loss': 0.8811, 'learning_rate': 9.180000000000001e-05, 'epoch': 4.11}\n",
      "{'loss': 0.8095, 'learning_rate': 9.200000000000001e-05, 'epoch': 4.12}\n",
      "{'loss': 0.9304, 'learning_rate': 9.22e-05, 'epoch': 4.12}\n",
      "{'loss': 0.9089, 'learning_rate': 9.240000000000001e-05, 'epoch': 4.13}\n",
      "{'loss': 0.9459, 'learning_rate': 9.260000000000001e-05, 'epoch': 4.14}\n",
      "{'loss': 0.8729, 'learning_rate': 9.28e-05, 'epoch': 4.15}\n",
      "{'loss': 0.8842, 'learning_rate': 9.300000000000001e-05, 'epoch': 4.16}\n",
      "{'loss': 0.922, 'learning_rate': 9.320000000000002e-05, 'epoch': 4.17}\n",
      "{'loss': 0.9086, 'learning_rate': 9.340000000000001e-05, 'epoch': 4.18}\n",
      "{'loss': 0.8999, 'learning_rate': 9.360000000000001e-05, 'epoch': 4.19}\n",
      "{'loss': 0.8715, 'learning_rate': 9.38e-05, 'epoch': 4.2}\n",
      "{'loss': 0.8722, 'learning_rate': 9.4e-05, 'epoch': 4.21}\n",
      "{'loss': 0.9157, 'learning_rate': 9.42e-05, 'epoch': 4.21}\n",
      "{'loss': 0.9347, 'learning_rate': 9.44e-05, 'epoch': 4.22}\n",
      "{'loss': 0.9113, 'learning_rate': 9.46e-05, 'epoch': 4.23}\n",
      "{'loss': 0.8787, 'learning_rate': 9.48e-05, 'epoch': 4.24}\n",
      "{'loss': 0.956, 'learning_rate': 9.5e-05, 'epoch': 4.25}\n",
      "{'loss': 0.8817, 'learning_rate': 9.52e-05, 'epoch': 4.26}\n",
      "{'loss': 0.7921, 'learning_rate': 9.54e-05, 'epoch': 4.27}\n",
      "{'loss': 0.9486, 'learning_rate': 9.56e-05, 'epoch': 4.28}\n",
      "{'loss': 0.9342, 'learning_rate': 9.58e-05, 'epoch': 4.29}\n",
      "{'loss': 0.733, 'learning_rate': 9.6e-05, 'epoch': 4.29}\n",
      "{'loss': 0.84, 'learning_rate': 9.620000000000001e-05, 'epoch': 4.3}\n",
      "{'loss': 0.8576, 'learning_rate': 9.64e-05, 'epoch': 4.31}\n",
      "{'loss': 0.8941, 'learning_rate': 9.66e-05, 'epoch': 4.32}\n",
      "{'loss': 0.8969, 'learning_rate': 9.680000000000001e-05, 'epoch': 4.33}\n",
      "{'loss': 0.8498, 'learning_rate': 9.7e-05, 'epoch': 4.34}\n",
      "{'loss': 0.929, 'learning_rate': 9.72e-05, 'epoch': 4.35}\n",
      "{'loss': 0.8773, 'learning_rate': 9.74e-05, 'epoch': 4.36}\n",
      "{'loss': 0.7885, 'learning_rate': 9.76e-05, 'epoch': 4.37}\n",
      "{'loss': 0.9426, 'learning_rate': 9.78e-05, 'epoch': 4.38}\n",
      "{'loss': 0.8204, 'learning_rate': 9.8e-05, 'epoch': 4.38}\n",
      "{'loss': 0.8109, 'learning_rate': 9.82e-05, 'epoch': 4.39}\n",
      "{'loss': 0.8988, 'learning_rate': 9.84e-05, 'epoch': 4.4}\n",
      "{'loss': 0.8317, 'learning_rate': 9.86e-05, 'epoch': 4.41}\n",
      "{'loss': 0.8791, 'learning_rate': 9.88e-05, 'epoch': 4.42}\n",
      "{'loss': 0.9383, 'learning_rate': 9.900000000000001e-05, 'epoch': 4.43}\n",
      "{'loss': 0.7744, 'learning_rate': 9.92e-05, 'epoch': 4.44}\n",
      "{'loss': 0.7824, 'learning_rate': 9.94e-05, 'epoch': 4.45}\n",
      "{'loss': 0.9555, 'learning_rate': 9.960000000000001e-05, 'epoch': 4.46}\n",
      "{'loss': 0.7838, 'learning_rate': 9.98e-05, 'epoch': 4.46}\n",
      "{'loss': 0.8231, 'learning_rate': 0.0001, 'epoch': 4.47}\n",
      "{'loss': 0.9426, 'learning_rate': 0.00010020000000000001, 'epoch': 4.48}\n",
      "{'loss': 0.7795, 'learning_rate': 0.0001004, 'epoch': 4.49}\n",
      "{'loss': 0.7936, 'learning_rate': 0.0001006, 'epoch': 4.5}\n",
      "{'loss': 0.8806, 'learning_rate': 0.00010080000000000001, 'epoch': 4.51}\n",
      "{'loss': 0.7737, 'learning_rate': 0.000101, 'epoch': 4.52}\n",
      "{'loss': 0.8129, 'learning_rate': 0.00010120000000000001, 'epoch': 4.53}\n",
      "{'loss': 0.7902, 'learning_rate': 0.00010140000000000001, 'epoch': 4.54}\n",
      "{'loss': 0.8041, 'learning_rate': 0.0001016, 'epoch': 4.54}\n",
      "{'loss': 0.8979, 'learning_rate': 0.00010180000000000001, 'epoch': 4.55}\n",
      "{'loss': 0.8221, 'learning_rate': 0.00010200000000000001, 'epoch': 4.56}\n",
      "{'loss': 0.9304, 'learning_rate': 0.0001022, 'epoch': 4.57}\n",
      "{'loss': 0.7843, 'learning_rate': 0.00010240000000000001, 'epoch': 4.58}\n",
      "{'loss': 0.719, 'learning_rate': 0.00010260000000000001, 'epoch': 4.59}\n",
      "{'loss': 0.8196, 'learning_rate': 0.0001028, 'epoch': 4.6}\n",
      "{'loss': 0.7935, 'learning_rate': 0.00010300000000000001, 'epoch': 4.61}\n",
      "{'loss': 0.7348, 'learning_rate': 0.0001032, 'epoch': 4.62}\n",
      "{'loss': 0.8201, 'learning_rate': 0.0001034, 'epoch': 4.62}\n",
      "{'loss': 0.803, 'learning_rate': 0.00010360000000000001, 'epoch': 4.63}\n",
      "{'loss': 0.7139, 'learning_rate': 0.0001038, 'epoch': 4.64}\n",
      "{'loss': 0.7698, 'learning_rate': 0.00010400000000000001, 'epoch': 4.65}\n",
      "{'loss': 0.8694, 'learning_rate': 0.00010420000000000001, 'epoch': 4.66}\n",
      "{'loss': 0.7819, 'learning_rate': 0.0001044, 'epoch': 4.67}\n",
      "{'loss': 0.7178, 'learning_rate': 0.00010460000000000001, 'epoch': 4.68}\n",
      "{'loss': 0.7508, 'learning_rate': 0.00010480000000000001, 'epoch': 4.69}\n",
      "{'loss': 0.8207, 'learning_rate': 0.000105, 'epoch': 4.7}\n",
      "{'loss': 0.7487, 'learning_rate': 0.00010520000000000001, 'epoch': 4.71}\n",
      "{'loss': 0.7338, 'learning_rate': 0.00010540000000000001, 'epoch': 4.71}\n",
      "{'loss': 0.706, 'learning_rate': 0.0001056, 'epoch': 4.72}\n",
      "{'loss': 0.7828, 'learning_rate': 0.00010580000000000001, 'epoch': 4.73}\n",
      "{'loss': 0.8464, 'learning_rate': 0.00010600000000000002, 'epoch': 4.74}\n",
      "{'loss': 0.812, 'learning_rate': 0.0001062, 'epoch': 4.75}\n",
      "{'loss': 0.7824, 'learning_rate': 0.00010640000000000001, 'epoch': 4.76}\n",
      "{'loss': 0.7417, 'learning_rate': 0.00010660000000000002, 'epoch': 4.77}\n",
      "{'loss': 0.7248, 'learning_rate': 0.00010680000000000001, 'epoch': 4.78}\n",
      "{'loss': 0.7085, 'learning_rate': 0.00010700000000000001, 'epoch': 4.79}\n",
      "{'loss': 0.7803, 'learning_rate': 0.00010720000000000002, 'epoch': 4.79}\n",
      "{'loss': 0.9411, 'learning_rate': 0.00010740000000000001, 'epoch': 4.8}\n",
      "{'loss': 0.7792, 'learning_rate': 0.00010760000000000001, 'epoch': 4.81}\n",
      "{'loss': 0.6941, 'learning_rate': 0.00010780000000000002, 'epoch': 4.82}\n",
      "{'loss': 0.7348, 'learning_rate': 0.00010800000000000001, 'epoch': 4.83}\n",
      "{'loss': 0.6277, 'learning_rate': 0.00010820000000000001, 'epoch': 4.84}\n",
      "{'loss': 0.7046, 'learning_rate': 0.00010840000000000002, 'epoch': 4.85}\n",
      "{'loss': 0.7001, 'learning_rate': 0.00010860000000000001, 'epoch': 4.86}\n",
      "{'loss': 0.8123, 'learning_rate': 0.00010880000000000002, 'epoch': 4.87}\n",
      "{'loss': 0.7362, 'learning_rate': 0.000109, 'epoch': 4.88}\n",
      "{'loss': 0.7323, 'learning_rate': 0.00010920000000000001, 'epoch': 4.88}\n",
      "{'loss': 0.7086, 'learning_rate': 0.00010940000000000002, 'epoch': 4.89}\n",
      "{'loss': 0.646, 'learning_rate': 0.00010960000000000001, 'epoch': 4.9}\n",
      "{'loss': 0.8413, 'learning_rate': 0.00010980000000000001, 'epoch': 4.91}\n",
      "{'loss': 0.747, 'learning_rate': 0.00011000000000000002, 'epoch': 4.92}\n",
      "{'loss': 0.6618, 'learning_rate': 0.00011020000000000001, 'epoch': 4.93}\n",
      "{'loss': 0.8486, 'learning_rate': 0.00011040000000000001, 'epoch': 4.94}\n",
      "{'loss': 0.7027, 'learning_rate': 0.00011060000000000002, 'epoch': 4.95}\n",
      "{'loss': 0.7708, 'learning_rate': 0.00011080000000000001, 'epoch': 4.96}\n",
      "{'loss': 0.7547, 'learning_rate': 0.00011100000000000001, 'epoch': 4.96}\n",
      "{'loss': 0.8056, 'learning_rate': 0.00011120000000000002, 'epoch': 4.97}\n",
      "{'loss': 0.6812, 'learning_rate': 0.00011140000000000001, 'epoch': 4.98}\n",
      "{'loss': 0.8041, 'learning_rate': 0.00011160000000000002, 'epoch': 4.99}\n",
      "{'loss': 0.8915, 'learning_rate': 0.00011180000000000002, 'epoch': 5.0}\n",
      "{'loss': 0.7297, 'learning_rate': 0.00011200000000000001, 'epoch': 5.01}\n",
      "{'loss': 0.7665, 'learning_rate': 0.00011220000000000002, 'epoch': 5.02}\n",
      "{'loss': 0.6785, 'learning_rate': 0.00011240000000000002, 'epoch': 5.03}\n",
      "{'loss': 0.6884, 'learning_rate': 0.0001126, 'epoch': 5.04}\n",
      "{'loss': 0.6976, 'learning_rate': 0.00011279999999999999, 'epoch': 5.04}\n",
      "{'loss': 0.7051, 'learning_rate': 0.000113, 'epoch': 5.05}\n",
      "{'loss': 0.6664, 'learning_rate': 0.0001132, 'epoch': 5.06}\n",
      "{'loss': 0.7399, 'learning_rate': 0.00011339999999999999, 'epoch': 5.07}\n",
      "{'loss': 0.6272, 'learning_rate': 0.0001136, 'epoch': 5.08}\n",
      "{'loss': 0.6267, 'learning_rate': 0.0001138, 'epoch': 5.09}\n",
      "{'loss': 0.6442, 'learning_rate': 0.00011399999999999999, 'epoch': 5.1}\n",
      "{'loss': 0.6972, 'learning_rate': 0.0001142, 'epoch': 5.11}\n",
      "{'loss': 0.6155, 'learning_rate': 0.0001144, 'epoch': 5.12}\n",
      "{'loss': 0.8099, 'learning_rate': 0.0001146, 'epoch': 5.12}\n",
      "{'loss': 0.6859, 'learning_rate': 0.0001148, 'epoch': 5.13}\n",
      "{'loss': 0.6279, 'learning_rate': 0.00011499999999999999, 'epoch': 5.14}\n",
      "{'loss': 0.6193, 'learning_rate': 0.0001152, 'epoch': 5.15}\n",
      "{'loss': 0.626, 'learning_rate': 0.0001154, 'epoch': 5.16}\n",
      "{'loss': 0.6666, 'learning_rate': 0.00011559999999999999, 'epoch': 5.17}\n",
      "{'loss': 0.6452, 'learning_rate': 0.0001158, 'epoch': 5.18}\n",
      "{'loss': 0.8549, 'learning_rate': 0.000116, 'epoch': 5.19}\n",
      "{'loss': 0.6799, 'learning_rate': 0.00011619999999999999, 'epoch': 5.2}\n",
      "{'loss': 0.6142, 'learning_rate': 0.0001164, 'epoch': 5.21}\n",
      "{'loss': 0.6011, 'learning_rate': 0.0001166, 'epoch': 5.21}\n",
      "{'loss': 0.6658, 'learning_rate': 0.00011679999999999999, 'epoch': 5.22}\n",
      "{'loss': 0.5839, 'learning_rate': 0.000117, 'epoch': 5.23}\n",
      "{'loss': 0.7025, 'learning_rate': 0.0001172, 'epoch': 5.24}\n",
      "{'loss': 0.6361, 'learning_rate': 0.0001174, 'epoch': 5.25}\n",
      "{'loss': 0.7537, 'learning_rate': 0.0001176, 'epoch': 5.26}\n",
      "{'loss': 0.5809, 'learning_rate': 0.0001178, 'epoch': 5.27}\n",
      "{'loss': 0.6947, 'learning_rate': 0.000118, 'epoch': 5.28}\n",
      "{'loss': 0.5828, 'learning_rate': 0.0001182, 'epoch': 5.29}\n",
      "{'loss': 0.6836, 'learning_rate': 0.0001184, 'epoch': 5.29}\n",
      "{'loss': 0.6653, 'learning_rate': 0.0001186, 'epoch': 5.3}\n",
      "{'loss': 0.5678, 'learning_rate': 0.0001188, 'epoch': 5.31}\n",
      "{'loss': 0.6858, 'learning_rate': 0.000119, 'epoch': 5.32}\n",
      "{'loss': 0.6234, 'learning_rate': 0.0001192, 'epoch': 5.33}\n",
      "{'loss': 0.6308, 'learning_rate': 0.0001194, 'epoch': 5.34}\n",
      "{'loss': 0.6914, 'learning_rate': 0.00011960000000000001, 'epoch': 5.35}\n",
      "{'loss': 0.7312, 'learning_rate': 0.0001198, 'epoch': 5.36}\n",
      "{'loss': 0.6581, 'learning_rate': 0.00012, 'epoch': 5.37}\n",
      "{'loss': 0.6356, 'learning_rate': 0.00012020000000000001, 'epoch': 5.38}\n",
      "{'loss': 0.6385, 'learning_rate': 0.0001204, 'epoch': 5.38}\n",
      "{'loss': 0.7327, 'learning_rate': 0.0001206, 'epoch': 5.39}\n",
      "{'loss': 0.624, 'learning_rate': 0.0001208, 'epoch': 5.4}\n",
      "{'loss': 0.6163, 'learning_rate': 0.000121, 'epoch': 5.41}\n",
      "{'loss': 0.6601, 'learning_rate': 0.0001212, 'epoch': 5.42}\n",
      "{'loss': 0.763, 'learning_rate': 0.0001214, 'epoch': 5.43}\n",
      "{'loss': 0.691, 'learning_rate': 0.0001216, 'epoch': 5.44}\n",
      "{'loss': 0.674, 'learning_rate': 0.0001218, 'epoch': 5.45}\n",
      "{'loss': 0.6366, 'learning_rate': 0.000122, 'epoch': 5.46}\n",
      "{'loss': 0.6579, 'learning_rate': 0.00012220000000000002, 'epoch': 5.46}\n",
      "{'loss': 0.7268, 'learning_rate': 0.0001224, 'epoch': 5.47}\n",
      "{'loss': 0.6673, 'learning_rate': 0.0001226, 'epoch': 5.48}\n",
      "{'loss': 0.7011, 'learning_rate': 0.0001228, 'epoch': 5.49}\n",
      "{'loss': 0.6464, 'learning_rate': 0.000123, 'epoch': 5.5}\n",
      "{'loss': 0.5679, 'learning_rate': 0.0001232, 'epoch': 5.51}\n",
      "{'loss': 0.7506, 'learning_rate': 0.00012340000000000002, 'epoch': 5.52}\n",
      "{'loss': 0.6837, 'learning_rate': 0.0001236, 'epoch': 5.53}\n",
      "{'loss': 0.6837, 'learning_rate': 0.0001238, 'epoch': 5.54}\n",
      "{'loss': 0.5903, 'learning_rate': 0.000124, 'epoch': 5.54}\n",
      "{'loss': 0.715, 'learning_rate': 0.0001242, 'epoch': 5.55}\n",
      "{'loss': 0.646, 'learning_rate': 0.00012440000000000002, 'epoch': 5.56}\n",
      "{'loss': 0.653, 'learning_rate': 0.0001246, 'epoch': 5.57}\n",
      "{'loss': 0.7064, 'learning_rate': 0.0001248, 'epoch': 5.58}\n",
      "{'loss': 0.5342, 'learning_rate': 0.000125, 'epoch': 5.59}\n",
      "{'loss': 0.719, 'learning_rate': 0.0001252, 'epoch': 5.6}\n",
      "{'loss': 0.5772, 'learning_rate': 0.0001254, 'epoch': 5.61}\n",
      "{'loss': 0.642, 'learning_rate': 0.00012560000000000002, 'epoch': 5.62}\n",
      "{'loss': 0.6039, 'learning_rate': 0.0001258, 'epoch': 5.62}\n",
      "{'loss': 0.5714, 'learning_rate': 0.000126, 'epoch': 5.63}\n",
      "{'loss': 0.6397, 'learning_rate': 0.0001262, 'epoch': 5.64}\n",
      "{'loss': 0.6147, 'learning_rate': 0.0001264, 'epoch': 5.65}\n",
      "{'loss': 0.7275, 'learning_rate': 0.00012660000000000001, 'epoch': 5.66}\n",
      "{'loss': 0.6303, 'learning_rate': 0.00012680000000000002, 'epoch': 5.67}\n",
      "{'loss': 0.6056, 'learning_rate': 0.000127, 'epoch': 5.68}\n",
      "{'loss': 0.5744, 'learning_rate': 0.0001272, 'epoch': 5.69}\n",
      "{'loss': 0.6268, 'learning_rate': 0.0001274, 'epoch': 5.7}\n",
      "{'loss': 0.5538, 'learning_rate': 0.0001276, 'epoch': 5.71}\n",
      "{'loss': 0.577, 'learning_rate': 0.00012780000000000002, 'epoch': 5.71}\n",
      "{'loss': 0.5265, 'learning_rate': 0.00012800000000000002, 'epoch': 5.72}\n",
      "{'loss': 0.6531, 'learning_rate': 0.0001282, 'epoch': 5.73}\n",
      "{'loss': 0.5364, 'learning_rate': 0.0001284, 'epoch': 5.74}\n",
      "{'loss': 0.5845, 'learning_rate': 0.0001286, 'epoch': 5.75}\n",
      "{'loss': 0.6767, 'learning_rate': 0.00012880000000000001, 'epoch': 5.76}\n",
      "{'loss': 0.6025, 'learning_rate': 0.00012900000000000002, 'epoch': 5.77}\n",
      "{'loss': 0.6361, 'learning_rate': 0.00012920000000000002, 'epoch': 5.78}\n",
      "{'loss': 0.6149, 'learning_rate': 0.0001294, 'epoch': 5.79}\n",
      "{'loss': 0.539, 'learning_rate': 0.0001296, 'epoch': 5.79}\n",
      "{'loss': 0.6484, 'learning_rate': 0.0001298, 'epoch': 5.8}\n",
      "{'loss': 0.528, 'learning_rate': 0.00013000000000000002, 'epoch': 5.81}\n",
      "{'loss': 0.654, 'learning_rate': 0.00013020000000000002, 'epoch': 5.82}\n",
      "{'loss': 0.6047, 'learning_rate': 0.0001304, 'epoch': 5.83}\n",
      "{'loss': 0.5676, 'learning_rate': 0.0001306, 'epoch': 5.84}\n",
      "{'loss': 0.6829, 'learning_rate': 0.0001308, 'epoch': 5.85}\n",
      "{'loss': 0.5806, 'learning_rate': 0.000131, 'epoch': 5.86}\n",
      "{'loss': 0.5525, 'learning_rate': 0.00013120000000000002, 'epoch': 5.87}\n",
      "{'loss': 0.5328, 'learning_rate': 0.00013140000000000002, 'epoch': 5.88}\n",
      "{'loss': 0.536, 'learning_rate': 0.0001316, 'epoch': 5.88}\n",
      "{'loss': 0.5397, 'learning_rate': 0.0001318, 'epoch': 5.89}\n",
      "{'loss': 0.56, 'learning_rate': 0.000132, 'epoch': 5.9}\n",
      "{'loss': 0.6133, 'learning_rate': 0.00013220000000000001, 'epoch': 5.91}\n",
      "{'loss': 0.5584, 'learning_rate': 0.00013240000000000002, 'epoch': 5.92}\n",
      "{'loss': 0.5526, 'learning_rate': 0.00013260000000000002, 'epoch': 5.93}\n",
      "{'loss': 0.5336, 'learning_rate': 0.0001328, 'epoch': 5.94}\n",
      "{'loss': 0.5439, 'learning_rate': 0.000133, 'epoch': 5.95}\n",
      "{'loss': 0.6096, 'learning_rate': 0.0001332, 'epoch': 5.96}\n",
      "{'loss': 0.5295, 'learning_rate': 0.00013340000000000002, 'epoch': 5.96}\n",
      "{'loss': 0.4953, 'learning_rate': 0.00013360000000000002, 'epoch': 5.97}\n",
      "{'loss': 0.5906, 'learning_rate': 0.00013380000000000003, 'epoch': 5.98}\n",
      "{'loss': 0.5452, 'learning_rate': 0.000134, 'epoch': 5.99}\n",
      "{'loss': 0.539, 'learning_rate': 0.0001342, 'epoch': 6.0}\n",
      "{'loss': 0.4701, 'learning_rate': 0.00013440000000000001, 'epoch': 6.01}\n",
      "{'loss': 0.614, 'learning_rate': 0.00013460000000000002, 'epoch': 6.02}\n",
      "{'loss': 0.4922, 'learning_rate': 0.00013480000000000002, 'epoch': 6.03}\n",
      "{'loss': 0.495, 'learning_rate': 0.00013500000000000003, 'epoch': 6.04}\n",
      "{'loss': 0.5235, 'learning_rate': 0.0001352, 'epoch': 6.04}\n",
      "{'loss': 0.5533, 'learning_rate': 0.0001354, 'epoch': 6.05}\n",
      "{'loss': 0.6166, 'learning_rate': 0.00013560000000000002, 'epoch': 6.06}\n",
      "{'loss': 0.6104, 'learning_rate': 0.00013580000000000002, 'epoch': 6.07}\n",
      "{'loss': 0.4443, 'learning_rate': 0.00013600000000000003, 'epoch': 6.08}\n",
      "{'loss': 0.6017, 'learning_rate': 0.0001362, 'epoch': 6.09}\n",
      "{'loss': 0.5907, 'learning_rate': 0.0001364, 'epoch': 6.1}\n",
      "{'loss': 0.5739, 'learning_rate': 0.0001366, 'epoch': 6.11}\n",
      "{'loss': 0.5431, 'learning_rate': 0.00013680000000000002, 'epoch': 6.12}\n",
      "{'loss': 0.5402, 'learning_rate': 0.00013700000000000002, 'epoch': 6.12}\n",
      "{'loss': 0.4696, 'learning_rate': 0.00013720000000000003, 'epoch': 6.13}\n",
      "{'loss': 0.5574, 'learning_rate': 0.0001374, 'epoch': 6.14}\n",
      "{'loss': 0.4689, 'learning_rate': 0.00013759999999999998, 'epoch': 6.15}\n",
      "{'loss': 0.5787, 'learning_rate': 0.0001378, 'epoch': 6.16}\n",
      "{'loss': 0.5495, 'learning_rate': 0.000138, 'epoch': 6.17}\n",
      "{'loss': 0.6782, 'learning_rate': 0.0001382, 'epoch': 6.18}\n",
      "{'loss': 0.5471, 'learning_rate': 0.0001384, 'epoch': 6.19}\n",
      "{'loss': 0.5703, 'learning_rate': 0.0001386, 'epoch': 6.2}\n",
      "{'loss': 0.5488, 'learning_rate': 0.00013879999999999999, 'epoch': 6.21}\n",
      "{'loss': 0.5302, 'learning_rate': 0.000139, 'epoch': 6.21}\n",
      "{'loss': 0.52, 'learning_rate': 0.0001392, 'epoch': 6.22}\n",
      "{'loss': 0.4829, 'learning_rate': 0.0001394, 'epoch': 6.23}\n",
      "{'loss': 0.4818, 'learning_rate': 0.0001396, 'epoch': 6.24}\n",
      "{'loss': 0.5613, 'learning_rate': 0.0001398, 'epoch': 6.25}\n",
      "{'loss': 0.5809, 'learning_rate': 0.00014, 'epoch': 6.26}\n",
      "{'loss': 0.5965, 'learning_rate': 0.0001402, 'epoch': 6.27}\n",
      "{'loss': 0.4241, 'learning_rate': 0.0001404, 'epoch': 6.28}\n",
      "{'loss': 0.4936, 'learning_rate': 0.0001406, 'epoch': 6.29}\n",
      "{'loss': 0.5292, 'learning_rate': 0.0001408, 'epoch': 6.29}\n",
      "{'loss': 0.5202, 'learning_rate': 0.000141, 'epoch': 6.3}\n",
      "{'loss': 0.4815, 'learning_rate': 0.0001412, 'epoch': 6.31}\n",
      "{'loss': 0.5195, 'learning_rate': 0.0001414, 'epoch': 6.32}\n",
      "{'loss': 0.5126, 'learning_rate': 0.0001416, 'epoch': 6.33}\n",
      "{'loss': 0.4854, 'learning_rate': 0.0001418, 'epoch': 6.34}\n",
      "{'loss': 0.4893, 'learning_rate': 0.000142, 'epoch': 6.35}\n",
      "{'loss': 0.5145, 'learning_rate': 0.0001422, 'epoch': 6.36}\n",
      "{'loss': 0.4667, 'learning_rate': 0.0001424, 'epoch': 6.37}\n",
      "{'loss': 0.5075, 'learning_rate': 0.0001426, 'epoch': 6.38}\n",
      "{'loss': 0.4551, 'learning_rate': 0.0001428, 'epoch': 6.38}\n",
      "{'loss': 0.52, 'learning_rate': 0.000143, 'epoch': 6.39}\n",
      "{'loss': 0.4776, 'learning_rate': 0.0001432, 'epoch': 6.4}\n",
      "{'loss': 0.5039, 'learning_rate': 0.0001434, 'epoch': 6.41}\n",
      "{'loss': 0.4685, 'learning_rate': 0.0001436, 'epoch': 6.42}\n",
      "{'loss': 0.5271, 'learning_rate': 0.0001438, 'epoch': 6.43}\n",
      "{'loss': 0.384, 'learning_rate': 0.000144, 'epoch': 6.44}\n",
      "{'loss': 0.5726, 'learning_rate': 0.0001442, 'epoch': 6.45}\n",
      "{'loss': 0.5093, 'learning_rate': 0.0001444, 'epoch': 6.46}\n",
      "{'loss': 0.416, 'learning_rate': 0.0001446, 'epoch': 6.46}\n",
      "{'loss': 0.5037, 'learning_rate': 0.0001448, 'epoch': 6.47}\n",
      "{'loss': 0.5264, 'learning_rate': 0.000145, 'epoch': 6.48}\n",
      "{'loss': 0.5181, 'learning_rate': 0.0001452, 'epoch': 6.49}\n",
      "{'loss': 0.4113, 'learning_rate': 0.0001454, 'epoch': 6.5}\n",
      "{'loss': 0.5335, 'learning_rate': 0.00014560000000000002, 'epoch': 6.51}\n",
      "{'loss': 0.5084, 'learning_rate': 0.0001458, 'epoch': 6.52}\n",
      "{'loss': 0.5259, 'learning_rate': 0.000146, 'epoch': 6.53}\n",
      "{'loss': 0.5194, 'learning_rate': 0.0001462, 'epoch': 6.54}\n",
      "{'loss': 0.4155, 'learning_rate': 0.0001464, 'epoch': 6.54}\n",
      "{'loss': 0.4093, 'learning_rate': 0.0001466, 'epoch': 6.55}\n",
      "{'loss': 0.4835, 'learning_rate': 0.00014680000000000002, 'epoch': 6.56}\n",
      "{'loss': 0.4625, 'learning_rate': 0.000147, 'epoch': 6.57}\n",
      "{'loss': 0.5494, 'learning_rate': 0.0001472, 'epoch': 6.58}\n",
      "{'loss': 0.4563, 'learning_rate': 0.0001474, 'epoch': 6.59}\n",
      "{'loss': 0.5209, 'learning_rate': 0.0001476, 'epoch': 6.6}\n",
      "{'loss': 0.4906, 'learning_rate': 0.00014780000000000001, 'epoch': 6.61}\n",
      "{'loss': 0.4749, 'learning_rate': 0.000148, 'epoch': 6.62}\n",
      "{'loss': 0.4911, 'learning_rate': 0.0001482, 'epoch': 6.62}\n",
      "{'loss': 0.4165, 'learning_rate': 0.0001484, 'epoch': 6.63}\n",
      "{'loss': 0.5386, 'learning_rate': 0.0001486, 'epoch': 6.64}\n",
      "{'loss': 0.4823, 'learning_rate': 0.0001488, 'epoch': 6.65}\n",
      "{'loss': 0.4952, 'learning_rate': 0.00014900000000000002, 'epoch': 6.66}\n",
      "{'loss': 0.4888, 'learning_rate': 0.0001492, 'epoch': 6.67}\n",
      "{'loss': 0.5802, 'learning_rate': 0.0001494, 'epoch': 6.68}\n",
      "{'loss': 0.4385, 'learning_rate': 0.0001496, 'epoch': 6.69}\n",
      "{'loss': 0.573, 'learning_rate': 0.0001498, 'epoch': 6.7}\n",
      "{'loss': 0.492, 'learning_rate': 0.00015000000000000001, 'epoch': 6.71}\n",
      "{'loss': 0.3404, 'learning_rate': 0.00015020000000000002, 'epoch': 6.71}\n",
      "{'loss': 0.5026, 'learning_rate': 0.0001504, 'epoch': 6.72}\n",
      "{'loss': 0.4636, 'learning_rate': 0.0001506, 'epoch': 6.73}\n",
      "{'loss': 0.5238, 'learning_rate': 0.0001508, 'epoch': 6.74}\n",
      "{'loss': 0.5023, 'learning_rate': 0.000151, 'epoch': 6.75}\n",
      "{'loss': 0.4318, 'learning_rate': 0.00015120000000000002, 'epoch': 6.76}\n",
      "{'loss': 0.5225, 'learning_rate': 0.00015140000000000002, 'epoch': 6.77}\n",
      "{'loss': 0.4192, 'learning_rate': 0.0001516, 'epoch': 6.78}\n",
      "{'loss': 0.406, 'learning_rate': 0.0001518, 'epoch': 6.79}\n",
      "{'loss': 0.5261, 'learning_rate': 0.000152, 'epoch': 6.79}\n",
      "{'loss': 0.5064, 'learning_rate': 0.0001522, 'epoch': 6.8}\n",
      "{'loss': 0.4861, 'learning_rate': 0.00015240000000000002, 'epoch': 6.81}\n",
      "{'loss': 0.4335, 'learning_rate': 0.00015260000000000002, 'epoch': 6.82}\n",
      "{'loss': 0.4516, 'learning_rate': 0.0001528, 'epoch': 6.83}\n",
      "{'loss': 0.4468, 'learning_rate': 0.000153, 'epoch': 6.84}\n",
      "{'loss': 0.5667, 'learning_rate': 0.0001532, 'epoch': 6.85}\n",
      "{'loss': 0.5279, 'learning_rate': 0.00015340000000000002, 'epoch': 6.86}\n",
      "{'loss': 0.4059, 'learning_rate': 0.00015360000000000002, 'epoch': 6.87}\n",
      "{'loss': 0.5013, 'learning_rate': 0.0001538, 'epoch': 6.88}\n",
      "{'loss': 0.4811, 'learning_rate': 0.000154, 'epoch': 6.88}\n",
      "{'loss': 0.4116, 'learning_rate': 0.0001542, 'epoch': 6.89}\n",
      "{'loss': 0.4521, 'learning_rate': 0.0001544, 'epoch': 6.9}\n",
      "{'loss': 0.4419, 'learning_rate': 0.00015460000000000002, 'epoch': 6.91}\n",
      "{'loss': 0.4439, 'learning_rate': 0.00015480000000000002, 'epoch': 6.92}\n",
      "{'loss': 0.4383, 'learning_rate': 0.000155, 'epoch': 6.93}\n",
      "{'loss': 0.4959, 'learning_rate': 0.0001552, 'epoch': 6.94}\n",
      "{'loss': 0.4119, 'learning_rate': 0.0001554, 'epoch': 6.95}\n",
      "{'loss': 0.4176, 'learning_rate': 0.00015560000000000001, 'epoch': 6.96}\n",
      "{'loss': 0.3999, 'learning_rate': 0.00015580000000000002, 'epoch': 6.96}\n",
      "{'loss': 0.5591, 'learning_rate': 0.00015600000000000002, 'epoch': 6.97}\n",
      "{'loss': 0.4761, 'learning_rate': 0.0001562, 'epoch': 6.98}\n",
      "{'loss': 0.4044, 'learning_rate': 0.0001564, 'epoch': 6.99}\n",
      "{'loss': 0.3566, 'learning_rate': 0.0001566, 'epoch': 7.0}\n",
      "{'loss': 0.4757, 'learning_rate': 0.00015680000000000002, 'epoch': 7.01}\n",
      "{'loss': 0.3743, 'learning_rate': 0.00015700000000000002, 'epoch': 7.02}\n",
      "{'loss': 0.3948, 'learning_rate': 0.00015720000000000003, 'epoch': 7.03}\n",
      "{'loss': 0.4525, 'learning_rate': 0.0001574, 'epoch': 7.04}\n",
      "{'loss': 0.526, 'learning_rate': 0.0001576, 'epoch': 7.04}\n",
      "{'loss': 0.3748, 'learning_rate': 0.00015780000000000001, 'epoch': 7.05}\n",
      "{'loss': 0.4812, 'learning_rate': 0.00015800000000000002, 'epoch': 7.06}\n",
      "{'loss': 0.4611, 'learning_rate': 0.00015820000000000002, 'epoch': 7.07}\n",
      "{'loss': 0.3577, 'learning_rate': 0.00015840000000000003, 'epoch': 7.08}\n",
      "{'loss': 0.3988, 'learning_rate': 0.0001586, 'epoch': 7.09}\n",
      "{'loss': 0.4775, 'learning_rate': 0.0001588, 'epoch': 7.1}\n",
      "{'loss': 0.3776, 'learning_rate': 0.00015900000000000002, 'epoch': 7.11}\n",
      "{'loss': 0.4383, 'learning_rate': 0.00015920000000000002, 'epoch': 7.12}\n",
      "{'loss': 0.4185, 'learning_rate': 0.00015940000000000003, 'epoch': 7.12}\n",
      "{'loss': 0.4605, 'learning_rate': 0.0001596, 'epoch': 7.13}\n",
      "{'loss': 0.4408, 'learning_rate': 0.0001598, 'epoch': 7.14}\n",
      "{'loss': 0.3867, 'learning_rate': 0.00016, 'epoch': 7.15}\n",
      "{'loss': 0.4414, 'learning_rate': 0.00016020000000000002, 'epoch': 7.16}\n",
      "{'loss': 0.3831, 'learning_rate': 0.00016040000000000002, 'epoch': 7.17}\n",
      "{'loss': 0.3984, 'learning_rate': 0.00016060000000000003, 'epoch': 7.18}\n",
      "{'loss': 0.4194, 'learning_rate': 0.0001608, 'epoch': 7.19}\n",
      "{'loss': 0.4886, 'learning_rate': 0.000161, 'epoch': 7.2}\n",
      "{'loss': 0.4314, 'learning_rate': 0.00016120000000000002, 'epoch': 7.21}\n",
      "{'loss': 0.4335, 'learning_rate': 0.00016140000000000002, 'epoch': 7.21}\n",
      "{'loss': 0.359, 'learning_rate': 0.00016160000000000002, 'epoch': 7.22}\n",
      "{'loss': 0.4509, 'learning_rate': 0.00016180000000000003, 'epoch': 7.23}\n",
      "{'loss': 0.3533, 'learning_rate': 0.000162, 'epoch': 7.24}\n",
      "{'loss': 0.4333, 'learning_rate': 0.0001622, 'epoch': 7.25}\n",
      "{'loss': 0.3832, 'learning_rate': 0.00016240000000000002, 'epoch': 7.26}\n",
      "{'loss': 0.3978, 'learning_rate': 0.0001626, 'epoch': 7.27}\n",
      "{'loss': 0.4621, 'learning_rate': 0.0001628, 'epoch': 7.28}\n",
      "{'loss': 0.4992, 'learning_rate': 0.000163, 'epoch': 7.29}\n",
      "{'loss': 0.4142, 'learning_rate': 0.0001632, 'epoch': 7.29}\n",
      "{'loss': 0.4682, 'learning_rate': 0.0001634, 'epoch': 7.3}\n",
      "{'loss': 0.388, 'learning_rate': 0.0001636, 'epoch': 7.31}\n",
      "{'loss': 0.3748, 'learning_rate': 0.0001638, 'epoch': 7.32}\n",
      "{'loss': 0.3646, 'learning_rate': 0.000164, 'epoch': 7.33}\n",
      "{'loss': 0.4063, 'learning_rate': 0.0001642, 'epoch': 7.34}\n",
      "{'loss': 0.3921, 'learning_rate': 0.0001644, 'epoch': 7.35}\n",
      "{'loss': 0.4007, 'learning_rate': 0.0001646, 'epoch': 7.36}\n",
      "{'loss': 0.3842, 'learning_rate': 0.0001648, 'epoch': 7.37}\n",
      "{'loss': 0.4131, 'learning_rate': 0.000165, 'epoch': 7.38}\n",
      "{'loss': 0.3927, 'learning_rate': 0.0001652, 'epoch': 7.38}\n",
      "{'loss': 0.4367, 'learning_rate': 0.0001654, 'epoch': 7.39}\n",
      "{'loss': 0.3491, 'learning_rate': 0.0001656, 'epoch': 7.4}\n",
      "{'loss': 0.3969, 'learning_rate': 0.0001658, 'epoch': 7.41}\n",
      "{'loss': 0.3439, 'learning_rate': 0.000166, 'epoch': 7.42}\n",
      "{'loss': 0.3404, 'learning_rate': 0.0001662, 'epoch': 7.43}\n",
      "{'loss': 0.38, 'learning_rate': 0.0001664, 'epoch': 7.44}\n",
      "{'loss': 0.3752, 'learning_rate': 0.0001666, 'epoch': 7.45}\n",
      "{'loss': 0.3584, 'learning_rate': 0.0001668, 'epoch': 7.46}\n",
      "{'loss': 0.3762, 'learning_rate': 0.000167, 'epoch': 7.46}\n",
      "{'loss': 0.4456, 'learning_rate': 0.0001672, 'epoch': 7.47}\n",
      "{'loss': 0.4039, 'learning_rate': 0.0001674, 'epoch': 7.48}\n",
      "{'loss': 0.3855, 'learning_rate': 0.0001676, 'epoch': 7.49}\n",
      "{'loss': 0.3075, 'learning_rate': 0.0001678, 'epoch': 7.5}\n",
      "{'loss': 0.3745, 'learning_rate': 0.000168, 'epoch': 7.51}\n",
      "{'loss': 0.3636, 'learning_rate': 0.0001682, 'epoch': 7.52}\n",
      "{'loss': 0.4891, 'learning_rate': 0.0001684, 'epoch': 7.53}\n",
      "{'loss': 0.3924, 'learning_rate': 0.0001686, 'epoch': 7.54}\n",
      "{'loss': 0.403, 'learning_rate': 0.0001688, 'epoch': 7.54}\n",
      "{'loss': 0.3836, 'learning_rate': 0.00016900000000000002, 'epoch': 7.55}\n",
      "{'loss': 0.4056, 'learning_rate': 0.0001692, 'epoch': 7.56}\n",
      "{'loss': 0.354, 'learning_rate': 0.0001694, 'epoch': 7.57}\n",
      "{'loss': 0.4292, 'learning_rate': 0.0001696, 'epoch': 7.58}\n",
      "{'loss': 0.3591, 'learning_rate': 0.0001698, 'epoch': 7.59}\n",
      "{'loss': 0.3825, 'learning_rate': 0.00017, 'epoch': 7.6}\n",
      "{'loss': 0.3804, 'learning_rate': 0.00017020000000000002, 'epoch': 7.61}\n",
      "{'loss': 0.2996, 'learning_rate': 0.0001704, 'epoch': 7.62}\n",
      "{'loss': 0.4439, 'learning_rate': 0.0001706, 'epoch': 7.62}\n",
      "{'loss': 0.3756, 'learning_rate': 0.0001708, 'epoch': 7.63}\n",
      "{'loss': 0.4252, 'learning_rate': 0.000171, 'epoch': 7.64}\n",
      "{'loss': 0.3605, 'learning_rate': 0.00017120000000000001, 'epoch': 7.65}\n",
      "{'loss': 0.3731, 'learning_rate': 0.0001714, 'epoch': 7.66}\n",
      "{'loss': 0.3364, 'learning_rate': 0.0001716, 'epoch': 7.67}\n",
      "{'loss': 0.3701, 'learning_rate': 0.0001718, 'epoch': 7.68}\n",
      "{'loss': 0.3127, 'learning_rate': 0.000172, 'epoch': 7.69}\n",
      "{'loss': 0.3679, 'learning_rate': 0.0001722, 'epoch': 7.7}\n",
      "{'loss': 0.3518, 'learning_rate': 0.00017240000000000002, 'epoch': 7.71}\n",
      "{'loss': 0.3753, 'learning_rate': 0.0001726, 'epoch': 7.71}\n",
      "{'loss': 0.3638, 'learning_rate': 0.0001728, 'epoch': 7.72}\n",
      "{'loss': 0.3129, 'learning_rate': 0.000173, 'epoch': 7.73}\n",
      "{'loss': 0.3103, 'learning_rate': 0.0001732, 'epoch': 7.74}\n",
      "{'loss': 0.4268, 'learning_rate': 0.0001734, 'epoch': 7.75}\n",
      "{'loss': 0.423, 'learning_rate': 0.00017360000000000002, 'epoch': 7.76}\n",
      "{'loss': 0.3884, 'learning_rate': 0.0001738, 'epoch': 7.77}\n",
      "{'loss': 0.3577, 'learning_rate': 0.000174, 'epoch': 7.78}\n",
      "{'loss': 0.373, 'learning_rate': 0.0001742, 'epoch': 7.79}\n",
      "{'loss': 0.3937, 'learning_rate': 0.0001744, 'epoch': 7.79}\n",
      "{'loss': 0.3887, 'learning_rate': 0.00017460000000000002, 'epoch': 7.8}\n",
      "{'loss': 0.2854, 'learning_rate': 0.00017480000000000002, 'epoch': 7.81}\n",
      "{'loss': 0.3161, 'learning_rate': 0.000175, 'epoch': 7.82}\n",
      "{'loss': 0.3871, 'learning_rate': 0.0001752, 'epoch': 7.83}\n",
      "{'loss': 0.4049, 'learning_rate': 0.0001754, 'epoch': 7.84}\n",
      "{'loss': 0.4788, 'learning_rate': 0.0001756, 'epoch': 7.85}\n",
      "{'loss': 0.4205, 'learning_rate': 0.00017580000000000002, 'epoch': 7.86}\n",
      "{'loss': 0.4399, 'learning_rate': 0.00017600000000000002, 'epoch': 7.87}\n",
      "{'loss': 0.382, 'learning_rate': 0.0001762, 'epoch': 7.88}\n",
      "{'loss': 0.2973, 'learning_rate': 0.0001764, 'epoch': 7.88}\n",
      "{'loss': 0.3906, 'learning_rate': 0.0001766, 'epoch': 7.89}\n",
      "{'loss': 0.3685, 'learning_rate': 0.00017680000000000001, 'epoch': 7.9}\n",
      "{'loss': 0.3528, 'learning_rate': 0.00017700000000000002, 'epoch': 7.91}\n",
      "{'loss': 0.3723, 'learning_rate': 0.0001772, 'epoch': 7.92}\n",
      "{'loss': 0.3795, 'learning_rate': 0.0001774, 'epoch': 7.93}\n",
      "{'loss': 0.4069, 'learning_rate': 0.0001776, 'epoch': 7.94}\n",
      "{'loss': 0.3441, 'learning_rate': 0.0001778, 'epoch': 7.95}\n",
      "{'loss': 0.36, 'learning_rate': 0.00017800000000000002, 'epoch': 7.96}\n",
      "{'loss': 0.363, 'learning_rate': 0.00017820000000000002, 'epoch': 7.96}\n",
      "{'loss': 0.3424, 'learning_rate': 0.0001784, 'epoch': 7.97}\n",
      "{'loss': 0.3653, 'learning_rate': 0.0001786, 'epoch': 7.98}\n",
      "{'loss': 0.3789, 'learning_rate': 0.0001788, 'epoch': 7.99}\n",
      "{'loss': 0.3418, 'learning_rate': 0.00017900000000000001, 'epoch': 8.0}\n",
      "{'loss': 0.4184, 'learning_rate': 0.00017920000000000002, 'epoch': 8.01}\n",
      "{'loss': 0.3314, 'learning_rate': 0.00017940000000000002, 'epoch': 8.02}\n",
      "{'loss': 0.3097, 'learning_rate': 0.0001796, 'epoch': 8.03}\n",
      "{'loss': 0.3825, 'learning_rate': 0.0001798, 'epoch': 8.04}\n",
      "{'loss': 0.2967, 'learning_rate': 0.00018, 'epoch': 8.04}\n",
      "{'loss': 0.3956, 'learning_rate': 0.00018020000000000002, 'epoch': 8.05}\n",
      "{'loss': 0.3782, 'learning_rate': 0.00018040000000000002, 'epoch': 8.06}\n",
      "{'loss': 0.3557, 'learning_rate': 0.00018060000000000003, 'epoch': 8.07}\n",
      "{'loss': 0.3348, 'learning_rate': 0.0001808, 'epoch': 8.08}\n",
      "{'loss': 0.2619, 'learning_rate': 0.000181, 'epoch': 8.09}\n",
      "{'loss': 0.2785, 'learning_rate': 0.0001812, 'epoch': 8.1}\n",
      "{'loss': 0.3462, 'learning_rate': 0.00018140000000000002, 'epoch': 8.11}\n",
      "{'loss': 0.3066, 'learning_rate': 0.00018160000000000002, 'epoch': 8.12}\n",
      "{'loss': 0.3438, 'learning_rate': 0.00018180000000000003, 'epoch': 8.12}\n",
      "{'loss': 0.2938, 'learning_rate': 0.000182, 'epoch': 8.13}\n",
      "{'loss': 0.3198, 'learning_rate': 0.0001822, 'epoch': 8.14}\n",
      "{'loss': 0.3821, 'learning_rate': 0.00018240000000000002, 'epoch': 8.15}\n",
      "{'loss': 0.3055, 'learning_rate': 0.00018260000000000002, 'epoch': 8.16}\n",
      "{'loss': 0.312, 'learning_rate': 0.00018280000000000003, 'epoch': 8.17}\n",
      "{'loss': 0.2686, 'learning_rate': 0.000183, 'epoch': 8.18}\n",
      "{'loss': 0.2927, 'learning_rate': 0.0001832, 'epoch': 8.19}\n",
      "{'loss': 0.3463, 'learning_rate': 0.0001834, 'epoch': 8.2}\n",
      "{'loss': 0.2787, 'learning_rate': 0.00018360000000000002, 'epoch': 8.21}\n",
      "{'loss': 0.3154, 'learning_rate': 0.00018380000000000002, 'epoch': 8.21}\n",
      "{'loss': 0.3273, 'learning_rate': 0.00018400000000000003, 'epoch': 8.22}\n",
      "{'loss': 0.3578, 'learning_rate': 0.0001842, 'epoch': 8.23}\n",
      "{'loss': 0.335, 'learning_rate': 0.0001844, 'epoch': 8.24}\n",
      "{'loss': 0.295, 'learning_rate': 0.00018460000000000001, 'epoch': 8.25}\n",
      "{'loss': 0.3116, 'learning_rate': 0.00018480000000000002, 'epoch': 8.26}\n",
      "{'loss': 0.3101, 'learning_rate': 0.00018500000000000002, 'epoch': 8.27}\n",
      "{'loss': 0.2938, 'learning_rate': 0.00018520000000000003, 'epoch': 8.28}\n",
      "{'loss': 0.3437, 'learning_rate': 0.0001854, 'epoch': 8.29}\n",
      "{'loss': 0.3423, 'learning_rate': 0.0001856, 'epoch': 8.29}\n",
      "{'loss': 0.3724, 'learning_rate': 0.00018580000000000002, 'epoch': 8.3}\n",
      "{'loss': 0.3476, 'learning_rate': 0.00018600000000000002, 'epoch': 8.31}\n",
      "{'loss': 0.3164, 'learning_rate': 0.00018620000000000003, 'epoch': 8.32}\n",
      "{'loss': 0.3157, 'learning_rate': 0.00018640000000000003, 'epoch': 8.33}\n",
      "{'loss': 0.3363, 'learning_rate': 0.0001866, 'epoch': 8.34}\n",
      "{'loss': 0.27, 'learning_rate': 0.00018680000000000001, 'epoch': 8.35}\n",
      "{'loss': 0.2895, 'learning_rate': 0.00018700000000000002, 'epoch': 8.36}\n",
      "{'loss': 0.3939, 'learning_rate': 0.00018720000000000002, 'epoch': 8.37}\n",
      "{'loss': 0.2842, 'learning_rate': 0.00018740000000000003, 'epoch': 8.38}\n",
      "{'loss': 0.2877, 'learning_rate': 0.0001876, 'epoch': 8.38}\n",
      "{'loss': 0.309, 'learning_rate': 0.0001878, 'epoch': 8.39}\n",
      "{'loss': 0.3737, 'learning_rate': 0.000188, 'epoch': 8.4}\n",
      "{'loss': 0.2563, 'learning_rate': 0.0001882, 'epoch': 8.41}\n",
      "{'loss': 0.4232, 'learning_rate': 0.0001884, 'epoch': 8.42}\n",
      "{'loss': 0.3292, 'learning_rate': 0.0001886, 'epoch': 8.43}\n",
      "{'loss': 0.2915, 'learning_rate': 0.0001888, 'epoch': 8.44}\n",
      "{'loss': 0.3572, 'learning_rate': 0.00018899999999999999, 'epoch': 8.45}\n",
      "{'loss': 0.2967, 'learning_rate': 0.0001892, 'epoch': 8.46}\n",
      "{'loss': 0.3249, 'learning_rate': 0.0001894, 'epoch': 8.46}\n",
      "{'loss': 0.3277, 'learning_rate': 0.0001896, 'epoch': 8.47}\n",
      "{'loss': 0.3189, 'learning_rate': 0.0001898, 'epoch': 8.48}\n",
      "{'loss': 0.3577, 'learning_rate': 0.00019, 'epoch': 8.49}\n",
      "{'loss': 0.2744, 'learning_rate': 0.0001902, 'epoch': 8.5}\n",
      "{'loss': 0.3084, 'learning_rate': 0.0001904, 'epoch': 8.51}\n",
      "{'loss': 0.3441, 'learning_rate': 0.0001906, 'epoch': 8.52}\n",
      "{'loss': 0.3126, 'learning_rate': 0.0001908, 'epoch': 8.53}\n",
      "{'loss': 0.3273, 'learning_rate': 0.000191, 'epoch': 8.54}\n",
      "{'loss': 0.3669, 'learning_rate': 0.0001912, 'epoch': 8.54}\n",
      "{'loss': 0.2491, 'learning_rate': 0.0001914, 'epoch': 8.55}\n",
      "{'loss': 0.3338, 'learning_rate': 0.0001916, 'epoch': 8.56}\n",
      "{'loss': 0.2661, 'learning_rate': 0.0001918, 'epoch': 8.57}\n",
      "{'loss': 0.2484, 'learning_rate': 0.000192, 'epoch': 8.58}\n",
      "{'loss': 0.3065, 'learning_rate': 0.0001922, 'epoch': 8.59}\n",
      "{'loss': 0.3916, 'learning_rate': 0.00019240000000000001, 'epoch': 8.6}\n",
      "{'loss': 0.2647, 'learning_rate': 0.0001926, 'epoch': 8.61}\n",
      "{'loss': 0.3349, 'learning_rate': 0.0001928, 'epoch': 8.62}\n",
      "{'loss': 0.2601, 'learning_rate': 0.000193, 'epoch': 8.62}\n",
      "{'loss': 0.3514, 'learning_rate': 0.0001932, 'epoch': 8.63}\n",
      "{'loss': 0.3035, 'learning_rate': 0.0001934, 'epoch': 8.64}\n",
      "{'loss': 0.3178, 'learning_rate': 0.00019360000000000002, 'epoch': 8.65}\n",
      "{'loss': 0.3461, 'learning_rate': 0.0001938, 'epoch': 8.66}\n",
      "{'loss': 0.2898, 'learning_rate': 0.000194, 'epoch': 8.67}\n",
      "{'loss': 0.2605, 'learning_rate': 0.0001942, 'epoch': 8.68}\n",
      "{'loss': 0.2541, 'learning_rate': 0.0001944, 'epoch': 8.69}\n",
      "{'loss': 0.3159, 'learning_rate': 0.00019460000000000001, 'epoch': 8.7}\n",
      "{'loss': 0.3759, 'learning_rate': 0.0001948, 'epoch': 8.71}\n",
      "{'loss': 0.2835, 'learning_rate': 0.000195, 'epoch': 8.71}\n",
      "{'loss': 0.266, 'learning_rate': 0.0001952, 'epoch': 8.72}\n",
      "{'loss': 0.3683, 'learning_rate': 0.0001954, 'epoch': 8.73}\n",
      "{'loss': 0.3709, 'learning_rate': 0.0001956, 'epoch': 8.74}\n",
      "{'loss': 0.2442, 'learning_rate': 0.00019580000000000002, 'epoch': 8.75}\n",
      "{'loss': 0.2212, 'learning_rate': 0.000196, 'epoch': 8.76}\n",
      "{'loss': 0.3137, 'learning_rate': 0.0001962, 'epoch': 8.77}\n",
      "{'loss': 0.373, 'learning_rate': 0.0001964, 'epoch': 8.78}\n",
      "{'loss': 0.308, 'learning_rate': 0.0001966, 'epoch': 8.79}\n",
      "{'loss': 0.2717, 'learning_rate': 0.0001968, 'epoch': 8.79}\n",
      "{'loss': 0.3045, 'learning_rate': 0.00019700000000000002, 'epoch': 8.8}\n",
      "{'loss': 0.2646, 'learning_rate': 0.0001972, 'epoch': 8.81}\n",
      "{'loss': 0.3449, 'learning_rate': 0.0001974, 'epoch': 8.82}\n",
      "{'loss': 0.2355, 'learning_rate': 0.0001976, 'epoch': 8.83}\n",
      "{'loss': 0.2757, 'learning_rate': 0.0001978, 'epoch': 8.84}\n",
      "{'loss': 0.2844, 'learning_rate': 0.00019800000000000002, 'epoch': 8.85}\n",
      "{'loss': 0.3443, 'learning_rate': 0.00019820000000000002, 'epoch': 8.86}\n",
      "{'loss': 0.3146, 'learning_rate': 0.0001984, 'epoch': 8.87}\n",
      "{'loss': 0.2805, 'learning_rate': 0.0001986, 'epoch': 8.88}\n",
      "{'loss': 0.2761, 'learning_rate': 0.0001988, 'epoch': 8.88}\n",
      "{'loss': 0.317, 'learning_rate': 0.000199, 'epoch': 8.89}\n",
      "{'loss': 0.2899, 'learning_rate': 0.00019920000000000002, 'epoch': 8.9}\n",
      "{'loss': 0.3057, 'learning_rate': 0.00019940000000000002, 'epoch': 8.91}\n",
      "{'loss': 0.2464, 'learning_rate': 0.0001996, 'epoch': 8.92}\n",
      "{'loss': 0.2393, 'learning_rate': 0.0001998, 'epoch': 8.93}\n",
      "{'loss': 0.2873, 'learning_rate': 0.0002, 'epoch': 8.94}\n",
      "{'loss': 0.2965, 'learning_rate': 0.0001997058823529412, 'epoch': 8.95}\n",
      "{'loss': 0.3287, 'learning_rate': 0.00019941176470588236, 'epoch': 8.96}\n",
      "{'loss': 0.2837, 'learning_rate': 0.00019911764705882355, 'epoch': 8.96}\n",
      "{'loss': 0.3017, 'learning_rate': 0.00019882352941176472, 'epoch': 8.97}\n",
      "{'loss': 0.2761, 'learning_rate': 0.0001985294117647059, 'epoch': 8.98}\n",
      "{'loss': 0.2437, 'learning_rate': 0.00019823529411764707, 'epoch': 8.99}\n",
      "{'loss': 0.3044, 'learning_rate': 0.00019794117647058826, 'epoch': 9.0}\n",
      "{'loss': 0.3199, 'learning_rate': 0.00019764705882352942, 'epoch': 9.01}\n",
      "{'loss': 0.2586, 'learning_rate': 0.0001973529411764706, 'epoch': 9.02}\n",
      "{'loss': 0.2498, 'learning_rate': 0.00019705882352941177, 'epoch': 9.03}\n",
      "{'loss': 0.2293, 'learning_rate': 0.00019676470588235294, 'epoch': 9.04}\n",
      "{'loss': 0.3214, 'learning_rate': 0.00019647058823529413, 'epoch': 9.04}\n",
      "{'loss': 0.2844, 'learning_rate': 0.0001961764705882353, 'epoch': 9.05}\n",
      "{'loss': 0.2755, 'learning_rate': 0.00019588235294117648, 'epoch': 9.06}\n",
      "{'loss': 0.2609, 'learning_rate': 0.00019558823529411764, 'epoch': 9.07}\n",
      "{'loss': 0.2596, 'learning_rate': 0.00019529411764705883, 'epoch': 9.08}\n",
      "{'loss': 0.2966, 'learning_rate': 0.000195, 'epoch': 9.09}\n",
      "{'loss': 0.2614, 'learning_rate': 0.0001947058823529412, 'epoch': 9.1}\n",
      "{'loss': 0.2768, 'learning_rate': 0.00019441176470588235, 'epoch': 9.11}\n",
      "{'loss': 0.2603, 'learning_rate': 0.00019411764705882354, 'epoch': 9.12}\n",
      "{'loss': 0.302, 'learning_rate': 0.0001938235294117647, 'epoch': 9.12}\n",
      "{'loss': 0.3183, 'learning_rate': 0.0001935294117647059, 'epoch': 9.13}\n",
      "{'loss': 0.268, 'learning_rate': 0.00019323529411764708, 'epoch': 9.14}\n",
      "{'loss': 0.2844, 'learning_rate': 0.00019294117647058825, 'epoch': 9.15}\n",
      "{'loss': 0.2975, 'learning_rate': 0.00019264705882352944, 'epoch': 9.16}\n",
      "{'loss': 0.2733, 'learning_rate': 0.0001923529411764706, 'epoch': 9.17}\n",
      "{'loss': 0.2504, 'learning_rate': 0.0001920588235294118, 'epoch': 9.18}\n",
      "{'loss': 0.2893, 'learning_rate': 0.00019176470588235295, 'epoch': 9.19}\n",
      "{'loss': 0.2474, 'learning_rate': 0.00019147058823529414, 'epoch': 9.2}\n",
      "{'loss': 0.228, 'learning_rate': 0.0001911764705882353, 'epoch': 9.21}\n",
      "{'loss': 0.2569, 'learning_rate': 0.0001908823529411765, 'epoch': 9.21}\n",
      "{'loss': 0.2579, 'learning_rate': 0.00019058823529411766, 'epoch': 9.22}\n",
      "{'loss': 0.2816, 'learning_rate': 0.00019029411764705882, 'epoch': 9.23}\n",
      "{'loss': 0.212, 'learning_rate': 0.00019, 'epoch': 9.24}\n",
      "{'loss': 0.2316, 'learning_rate': 0.00018970588235294117, 'epoch': 9.25}\n",
      "{'loss': 0.2071, 'learning_rate': 0.00018941176470588236, 'epoch': 9.26}\n",
      "{'loss': 0.2588, 'learning_rate': 0.00018911764705882353, 'epoch': 9.27}\n",
      "{'loss': 0.3062, 'learning_rate': 0.00018882352941176472, 'epoch': 9.28}\n",
      "{'loss': 0.2041, 'learning_rate': 0.00018852941176470588, 'epoch': 9.29}\n",
      "{'loss': 0.2391, 'learning_rate': 0.00018823529411764707, 'epoch': 9.29}\n",
      "{'loss': 0.2159, 'learning_rate': 0.00018794117647058823, 'epoch': 9.3}\n",
      "{'loss': 0.2561, 'learning_rate': 0.00018764705882352942, 'epoch': 9.31}\n",
      "{'loss': 0.2837, 'learning_rate': 0.00018735294117647059, 'epoch': 9.32}\n",
      "{'loss': 0.2178, 'learning_rate': 0.00018705882352941178, 'epoch': 9.33}\n",
      "{'loss': 0.2388, 'learning_rate': 0.00018676470588235297, 'epoch': 9.34}\n",
      "{'loss': 0.3008, 'learning_rate': 0.00018647058823529413, 'epoch': 9.35}\n",
      "{'loss': 0.2445, 'learning_rate': 0.00018617647058823532, 'epoch': 9.36}\n",
      "{'loss': 0.2869, 'learning_rate': 0.00018588235294117648, 'epoch': 9.37}\n",
      "{'loss': 0.24, 'learning_rate': 0.00018558823529411767, 'epoch': 9.38}\n",
      "{'loss': 0.2757, 'learning_rate': 0.00018529411764705883, 'epoch': 9.38}\n",
      "{'loss': 0.206, 'learning_rate': 0.00018500000000000002, 'epoch': 9.39}\n",
      "{'loss': 0.2502, 'learning_rate': 0.0001847058823529412, 'epoch': 9.4}\n",
      "{'loss': 0.2218, 'learning_rate': 0.00018441176470588238, 'epoch': 9.41}\n",
      "{'loss': 0.2799, 'learning_rate': 0.00018411764705882354, 'epoch': 9.42}\n",
      "{'loss': 0.2648, 'learning_rate': 0.0001838235294117647, 'epoch': 9.43}\n",
      "{'loss': 0.2397, 'learning_rate': 0.0001835294117647059, 'epoch': 9.44}\n",
      "{'loss': 0.2901, 'learning_rate': 0.00018323529411764706, 'epoch': 9.45}\n",
      "{'loss': 0.272, 'learning_rate': 0.00018294117647058825, 'epoch': 9.46}\n",
      "{'loss': 0.2149, 'learning_rate': 0.0001826470588235294, 'epoch': 9.46}\n",
      "{'loss': 0.2544, 'learning_rate': 0.0001823529411764706, 'epoch': 9.47}\n",
      "{'loss': 0.2654, 'learning_rate': 0.00018205882352941176, 'epoch': 9.48}\n",
      "{'loss': 0.2342, 'learning_rate': 0.00018176470588235295, 'epoch': 9.49}\n",
      "{'loss': 0.2094, 'learning_rate': 0.00018147058823529412, 'epoch': 9.5}\n",
      "{'loss': 0.2287, 'learning_rate': 0.0001811764705882353, 'epoch': 9.51}\n",
      "{'loss': 0.263, 'learning_rate': 0.00018088235294117647, 'epoch': 9.52}\n",
      "{'loss': 0.2631, 'learning_rate': 0.00018058823529411766, 'epoch': 9.53}\n",
      "{'loss': 0.2145, 'learning_rate': 0.00018029411764705885, 'epoch': 9.54}\n",
      "{'loss': 0.2149, 'learning_rate': 0.00018, 'epoch': 9.54}\n",
      "{'loss': 0.2652, 'learning_rate': 0.0001797058823529412, 'epoch': 9.55}\n",
      "{'loss': 0.248, 'learning_rate': 0.00017941176470588236, 'epoch': 9.56}\n",
      "{'loss': 0.2373, 'learning_rate': 0.00017911764705882355, 'epoch': 9.57}\n",
      "{'loss': 0.2548, 'learning_rate': 0.00017882352941176472, 'epoch': 9.58}\n",
      "{'loss': 0.2469, 'learning_rate': 0.0001785294117647059, 'epoch': 9.59}\n",
      "{'loss': 0.232, 'learning_rate': 0.00017823529411764707, 'epoch': 9.6}\n",
      "{'loss': 0.3052, 'learning_rate': 0.00017794117647058823, 'epoch': 9.61}\n",
      "{'loss': 0.2785, 'learning_rate': 0.00017764705882352942, 'epoch': 9.62}\n",
      "{'loss': 0.2963, 'learning_rate': 0.00017735294117647059, 'epoch': 9.62}\n",
      "{'loss': 0.2436, 'learning_rate': 0.00017705882352941178, 'epoch': 9.63}\n",
      "{'loss': 0.216, 'learning_rate': 0.00017676470588235294, 'epoch': 9.64}\n",
      "{'loss': 0.2103, 'learning_rate': 0.00017647058823529413, 'epoch': 9.65}\n",
      "{'loss': 0.1946, 'learning_rate': 0.0001761764705882353, 'epoch': 9.66}\n",
      "{'loss': 0.24, 'learning_rate': 0.00017588235294117648, 'epoch': 9.67}\n",
      "{'loss': 0.2884, 'learning_rate': 0.00017558823529411765, 'epoch': 9.68}\n",
      "{'loss': 0.2289, 'learning_rate': 0.00017529411764705884, 'epoch': 9.69}\n",
      "{'loss': 0.218, 'learning_rate': 0.000175, 'epoch': 9.7}\n",
      "{'loss': 0.2299, 'learning_rate': 0.0001747058823529412, 'epoch': 9.71}\n",
      "{'loss': 0.2223, 'learning_rate': 0.00017441176470588235, 'epoch': 9.71}\n",
      "{'loss': 0.2842, 'learning_rate': 0.00017411764705882354, 'epoch': 9.72}\n",
      "{'loss': 0.2554, 'learning_rate': 0.00017382352941176473, 'epoch': 9.73}\n",
      "{'loss': 0.2545, 'learning_rate': 0.0001735294117647059, 'epoch': 9.74}\n",
      "{'loss': 0.3217, 'learning_rate': 0.00017323529411764708, 'epoch': 9.75}\n",
      "{'loss': 0.2438, 'learning_rate': 0.00017294117647058825, 'epoch': 9.76}\n",
      "{'loss': 0.2417, 'learning_rate': 0.00017264705882352944, 'epoch': 9.77}\n",
      "{'loss': 0.2098, 'learning_rate': 0.0001723529411764706, 'epoch': 9.78}\n",
      "{'loss': 0.2713, 'learning_rate': 0.0001720588235294118, 'epoch': 9.79}\n",
      "{'loss': 0.2978, 'learning_rate': 0.00017176470588235293, 'epoch': 9.79}\n",
      "{'loss': 0.2547, 'learning_rate': 0.00017147058823529412, 'epoch': 9.8}\n",
      "{'loss': 0.2929, 'learning_rate': 0.0001711764705882353, 'epoch': 9.81}\n",
      "{'loss': 0.2419, 'learning_rate': 0.00017088235294117647, 'epoch': 9.82}\n",
      "{'loss': 0.2041, 'learning_rate': 0.00017058823529411766, 'epoch': 9.83}\n",
      "{'loss': 0.1984, 'learning_rate': 0.00017029411764705882, 'epoch': 9.84}\n",
      "{'loss': 0.1991, 'learning_rate': 0.00017, 'epoch': 9.85}\n",
      "{'loss': 0.2633, 'learning_rate': 0.00016970588235294118, 'epoch': 9.86}\n",
      "{'loss': 0.2762, 'learning_rate': 0.00016941176470588237, 'epoch': 9.87}\n",
      "{'loss': 0.216, 'learning_rate': 0.00016911764705882353, 'epoch': 9.88}\n",
      "{'loss': 0.2339, 'learning_rate': 0.00016882352941176472, 'epoch': 9.88}\n",
      "{'loss': 0.2646, 'learning_rate': 0.00016852941176470588, 'epoch': 9.89}\n",
      "{'loss': 0.2077, 'learning_rate': 0.00016823529411764707, 'epoch': 9.9}\n",
      "{'loss': 0.3279, 'learning_rate': 0.00016794117647058823, 'epoch': 9.91}\n",
      "{'loss': 0.2274, 'learning_rate': 0.00016764705882352942, 'epoch': 9.92}\n",
      "{'loss': 0.1912, 'learning_rate': 0.00016735294117647061, 'epoch': 9.93}\n",
      "{'loss': 0.2112, 'learning_rate': 0.00016705882352941178, 'epoch': 9.94}\n",
      "{'loss': 0.2239, 'learning_rate': 0.00016676470588235297, 'epoch': 9.95}\n",
      "{'loss': 0.1882, 'learning_rate': 0.00016647058823529413, 'epoch': 9.96}\n",
      "{'loss': 0.2487, 'learning_rate': 0.00016617647058823532, 'epoch': 9.96}\n",
      "{'loss': 0.2312, 'learning_rate': 0.00016588235294117648, 'epoch': 9.97}\n",
      "{'loss': 0.2155, 'learning_rate': 0.00016558823529411765, 'epoch': 9.98}\n",
      "{'loss': 0.2097, 'learning_rate': 0.0001652941176470588, 'epoch': 9.99}\n",
      "{'loss': 0.2785, 'learning_rate': 0.000165, 'epoch': 10.0}\n",
      "{'loss': 0.182, 'learning_rate': 0.0001647058823529412, 'epoch': 10.01}\n",
      "{'loss': 0.1981, 'learning_rate': 0.00016441176470588235, 'epoch': 10.02}\n",
      "{'loss': 0.2386, 'learning_rate': 0.00016411764705882354, 'epoch': 10.03}\n",
      "{'loss': 0.2112, 'learning_rate': 0.0001638235294117647, 'epoch': 10.04}\n",
      "{'loss': 0.1945, 'learning_rate': 0.0001635294117647059, 'epoch': 10.04}\n",
      "{'loss': 0.1894, 'learning_rate': 0.00016323529411764706, 'epoch': 10.05}\n",
      "{'loss': 0.2282, 'learning_rate': 0.00016294117647058825, 'epoch': 10.06}\n",
      "{'loss': 0.2237, 'learning_rate': 0.0001626470588235294, 'epoch': 10.07}\n",
      "{'loss': 0.2413, 'learning_rate': 0.0001623529411764706, 'epoch': 10.08}\n",
      "{'loss': 0.2151, 'learning_rate': 0.00016205882352941176, 'epoch': 10.09}\n",
      "{'loss': 0.263, 'learning_rate': 0.00016176470588235295, 'epoch': 10.1}\n",
      "{'loss': 0.2516, 'learning_rate': 0.00016147058823529412, 'epoch': 10.11}\n",
      "{'loss': 0.2304, 'learning_rate': 0.0001611764705882353, 'epoch': 10.12}\n",
      "{'loss': 0.1866, 'learning_rate': 0.0001608823529411765, 'epoch': 10.12}\n",
      "{'loss': 0.2269, 'learning_rate': 0.00016058823529411766, 'epoch': 10.13}\n",
      "{'loss': 0.1976, 'learning_rate': 0.00016029411764705885, 'epoch': 10.14}\n",
      "{'loss': 0.1718, 'learning_rate': 0.00016, 'epoch': 10.15}\n",
      "{'loss': 0.265, 'learning_rate': 0.0001597058823529412, 'epoch': 10.16}\n",
      "{'loss': 0.2038, 'learning_rate': 0.00015941176470588237, 'epoch': 10.17}\n",
      "{'loss': 0.2218, 'learning_rate': 0.00015911764705882353, 'epoch': 10.18}\n",
      "{'loss': 0.2183, 'learning_rate': 0.0001588235294117647, 'epoch': 10.19}\n",
      "{'loss': 0.2059, 'learning_rate': 0.00015852941176470588, 'epoch': 10.2}\n",
      "{'loss': 0.1983, 'learning_rate': 0.00015823529411764707, 'epoch': 10.21}\n",
      "{'loss': 0.2058, 'learning_rate': 0.00015794117647058824, 'epoch': 10.21}\n",
      "{'loss': 0.2153, 'learning_rate': 0.00015764705882352943, 'epoch': 10.22}\n",
      "{'loss': 0.2832, 'learning_rate': 0.0001573529411764706, 'epoch': 10.23}\n",
      "{'loss': 0.2135, 'learning_rate': 0.00015705882352941178, 'epoch': 10.24}\n",
      "{'loss': 0.2091, 'learning_rate': 0.00015676470588235294, 'epoch': 10.25}\n",
      "{'loss': 0.157, 'learning_rate': 0.00015647058823529413, 'epoch': 10.26}\n",
      "{'loss': 0.2225, 'learning_rate': 0.0001561764705882353, 'epoch': 10.27}\n",
      "{'loss': 0.1665, 'learning_rate': 0.00015588235294117648, 'epoch': 10.28}\n",
      "{'loss': 0.1857, 'learning_rate': 0.00015558823529411765, 'epoch': 10.29}\n",
      "{'loss': 0.2497, 'learning_rate': 0.00015529411764705884, 'epoch': 10.29}\n",
      "{'loss': 0.1729, 'learning_rate': 0.000155, 'epoch': 10.3}\n",
      "{'loss': 0.2349, 'learning_rate': 0.0001547058823529412, 'epoch': 10.31}\n",
      "{'loss': 0.191, 'learning_rate': 0.00015441176470588238, 'epoch': 10.32}\n",
      "{'loss': 0.1952, 'learning_rate': 0.00015411764705882354, 'epoch': 10.33}\n",
      "{'loss': 0.2168, 'learning_rate': 0.00015382352941176473, 'epoch': 10.34}\n",
      "{'loss': 0.2354, 'learning_rate': 0.0001535294117647059, 'epoch': 10.35}\n",
      "{'loss': 0.2401, 'learning_rate': 0.00015323529411764709, 'epoch': 10.36}\n",
      "{'loss': 0.2559, 'learning_rate': 0.00015294117647058822, 'epoch': 10.37}\n",
      "{'loss': 0.2211, 'learning_rate': 0.0001526470588235294, 'epoch': 10.38}\n",
      "{'loss': 0.2276, 'learning_rate': 0.00015235294117647057, 'epoch': 10.38}\n",
      "{'loss': 0.2736, 'learning_rate': 0.00015205882352941176, 'epoch': 10.39}\n",
      "{'loss': 0.2462, 'learning_rate': 0.00015176470588235295, 'epoch': 10.4}\n",
      "{'loss': 0.2575, 'learning_rate': 0.00015147058823529412, 'epoch': 10.41}\n",
      "{'loss': 0.1908, 'learning_rate': 0.0001511764705882353, 'epoch': 10.42}\n",
      "{'loss': 0.2155, 'learning_rate': 0.00015088235294117647, 'epoch': 10.43}\n",
      "{'loss': 0.2237, 'learning_rate': 0.00015058823529411766, 'epoch': 10.44}\n",
      "{'loss': 0.224, 'learning_rate': 0.00015029411764705882, 'epoch': 10.45}\n",
      "{'loss': 0.2183, 'learning_rate': 0.00015000000000000001, 'epoch': 10.46}\n",
      "{'loss': 0.2691, 'learning_rate': 0.00014970588235294118, 'epoch': 10.46}\n",
      "{'loss': 0.2356, 'learning_rate': 0.00014941176470588237, 'epoch': 10.47}\n",
      "{'loss': 0.1702, 'learning_rate': 0.00014911764705882353, 'epoch': 10.48}\n",
      "{'loss': 0.2253, 'learning_rate': 0.00014882352941176472, 'epoch': 10.49}\n",
      "{'loss': 0.2095, 'learning_rate': 0.00014852941176470588, 'epoch': 10.5}\n",
      "{'loss': 0.1733, 'learning_rate': 0.00014823529411764707, 'epoch': 10.51}\n",
      "{'loss': 0.2081, 'learning_rate': 0.00014794117647058826, 'epoch': 10.52}\n",
      "{'loss': 0.1641, 'learning_rate': 0.00014764705882352943, 'epoch': 10.53}\n",
      "{'loss': 0.2049, 'learning_rate': 0.00014735294117647062, 'epoch': 10.54}\n",
      "{'loss': 0.1794, 'learning_rate': 0.00014705882352941178, 'epoch': 10.54}\n",
      "{'loss': 0.2258, 'learning_rate': 0.00014676470588235294, 'epoch': 10.55}\n",
      "{'loss': 0.1902, 'learning_rate': 0.0001464705882352941, 'epoch': 10.56}\n",
      "{'loss': 0.2111, 'learning_rate': 0.0001461764705882353, 'epoch': 10.57}\n",
      "{'loss': 0.2108, 'learning_rate': 0.00014588235294117646, 'epoch': 10.58}\n",
      "{'loss': 0.2146, 'learning_rate': 0.00014558823529411765, 'epoch': 10.59}\n",
      "{'loss': 0.209, 'learning_rate': 0.00014529411764705884, 'epoch': 10.6}\n",
      "{'loss': 0.2158, 'learning_rate': 0.000145, 'epoch': 10.61}\n",
      "{'loss': 0.2024, 'learning_rate': 0.0001447058823529412, 'epoch': 10.62}\n",
      "{'loss': 0.2099, 'learning_rate': 0.00014441176470588235, 'epoch': 10.62}\n",
      "{'loss': 0.1892, 'learning_rate': 0.00014411764705882354, 'epoch': 10.63}\n",
      "{'loss': 0.1599, 'learning_rate': 0.0001438235294117647, 'epoch': 10.64}\n",
      "{'loss': 0.2209, 'learning_rate': 0.0001435294117647059, 'epoch': 10.65}\n",
      "{'loss': 0.1653, 'learning_rate': 0.00014323529411764706, 'epoch': 10.66}\n",
      "{'loss': 0.2025, 'learning_rate': 0.00014294117647058825, 'epoch': 10.67}\n",
      "{'loss': 0.1616, 'learning_rate': 0.0001426470588235294, 'epoch': 10.68}\n",
      "{'loss': 0.2019, 'learning_rate': 0.0001423529411764706, 'epoch': 10.69}\n",
      "{'loss': 0.2101, 'learning_rate': 0.00014205882352941177, 'epoch': 10.7}\n",
      "{'loss': 0.2365, 'learning_rate': 0.00014176470588235296, 'epoch': 10.71}\n",
      "{'loss': 0.2046, 'learning_rate': 0.00014147058823529415, 'epoch': 10.71}\n",
      "{'loss': 0.1937, 'learning_rate': 0.0001411764705882353, 'epoch': 10.72}\n",
      "{'loss': 0.2222, 'learning_rate': 0.0001408823529411765, 'epoch': 10.73}\n",
      "{'loss': 0.1959, 'learning_rate': 0.00014058823529411763, 'epoch': 10.74}\n",
      "{'loss': 0.2314, 'learning_rate': 0.00014029411764705882, 'epoch': 10.75}\n",
      "{'loss': 0.2033, 'learning_rate': 0.00014, 'epoch': 10.76}\n",
      "{'loss': 0.1942, 'learning_rate': 0.00013970588235294118, 'epoch': 10.77}\n",
      "{'loss': 0.1991, 'learning_rate': 0.00013941176470588234, 'epoch': 10.78}\n",
      "{'loss': 0.2078, 'learning_rate': 0.00013911764705882353, 'epoch': 10.79}\n",
      "{'loss': 0.2137, 'learning_rate': 0.00013882352941176472, 'epoch': 10.79}\n",
      "{'loss': 0.1792, 'learning_rate': 0.00013852941176470588, 'epoch': 10.8}\n",
      "{'loss': 0.2239, 'learning_rate': 0.00013823529411764707, 'epoch': 10.81}\n",
      "{'loss': 0.183, 'learning_rate': 0.00013794117647058824, 'epoch': 10.82}\n",
      "{'loss': 0.1736, 'learning_rate': 0.00013764705882352943, 'epoch': 10.83}\n",
      "{'loss': 0.2458, 'learning_rate': 0.0001373529411764706, 'epoch': 10.84}\n",
      "{'loss': 0.1759, 'learning_rate': 0.00013705882352941178, 'epoch': 10.85}\n",
      "{'loss': 0.1383, 'learning_rate': 0.00013676470588235294, 'epoch': 10.86}\n",
      "{'loss': 0.3017, 'learning_rate': 0.00013647058823529413, 'epoch': 10.87}\n",
      "{'loss': 0.2076, 'learning_rate': 0.0001361764705882353, 'epoch': 10.88}\n",
      "{'loss': 0.1671, 'learning_rate': 0.00013588235294117649, 'epoch': 10.88}\n",
      "{'loss': 0.19, 'learning_rate': 0.00013558823529411765, 'epoch': 10.89}\n",
      "{'loss': 0.2061, 'learning_rate': 0.00013529411764705884, 'epoch': 10.9}\n",
      "{'loss': 0.1715, 'learning_rate': 0.00013500000000000003, 'epoch': 10.91}\n",
      "{'loss': 0.2039, 'learning_rate': 0.0001347058823529412, 'epoch': 10.92}\n",
      "{'loss': 0.2443, 'learning_rate': 0.00013441176470588238, 'epoch': 10.93}\n",
      "{'loss': 0.1977, 'learning_rate': 0.00013411764705882352, 'epoch': 10.94}\n",
      "{'loss': 0.2104, 'learning_rate': 0.0001338235294117647, 'epoch': 10.95}\n",
      "{'loss': 0.187, 'learning_rate': 0.00013352941176470587, 'epoch': 10.96}\n",
      "{'loss': 0.2517, 'learning_rate': 0.00013323529411764706, 'epoch': 10.96}\n",
      "{'loss': 0.2142, 'learning_rate': 0.00013294117647058822, 'epoch': 10.97}\n",
      "{'loss': 0.2218, 'learning_rate': 0.00013264705882352941, 'epoch': 10.98}\n",
      "{'loss': 0.1825, 'learning_rate': 0.0001323529411764706, 'epoch': 10.99}\n",
      "{'loss': 0.1832, 'learning_rate': 0.00013205882352941177, 'epoch': 11.0}\n",
      "{'loss': 0.143, 'learning_rate': 0.00013176470588235296, 'epoch': 11.01}\n",
      "{'loss': 0.1556, 'learning_rate': 0.00013147058823529412, 'epoch': 11.02}\n",
      "{'loss': 0.1907, 'learning_rate': 0.0001311764705882353, 'epoch': 11.03}\n",
      "{'loss': 0.2182, 'learning_rate': 0.00013088235294117647, 'epoch': 11.04}\n",
      "{'loss': 0.1896, 'learning_rate': 0.00013058823529411766, 'epoch': 11.04}\n",
      "{'loss': 0.18, 'learning_rate': 0.00013029411764705883, 'epoch': 11.05}\n",
      "{'loss': 0.205, 'learning_rate': 0.00013000000000000002, 'epoch': 11.06}\n",
      "{'loss': 0.2076, 'learning_rate': 0.00012970588235294118, 'epoch': 11.07}\n",
      "{'loss': 0.1783, 'learning_rate': 0.00012941176470588237, 'epoch': 11.08}\n",
      "{'loss': 0.1922, 'learning_rate': 0.00012911764705882353, 'epoch': 11.09}\n",
      "{'loss': 0.1925, 'learning_rate': 0.00012882352941176472, 'epoch': 11.1}\n",
      "{'loss': 0.193, 'learning_rate': 0.00012852941176470588, 'epoch': 11.11}\n",
      "{'loss': 0.2143, 'learning_rate': 0.00012823529411764707, 'epoch': 11.12}\n",
      "{'loss': 0.1905, 'learning_rate': 0.00012794117647058824, 'epoch': 11.12}\n",
      "{'loss': 0.1548, 'learning_rate': 0.0001276470588235294, 'epoch': 11.13}\n",
      "{'loss': 0.1957, 'learning_rate': 0.0001273529411764706, 'epoch': 11.14}\n",
      "{'loss': 0.2092, 'learning_rate': 0.00012705882352941175, 'epoch': 11.15}\n",
      "{'loss': 0.176, 'learning_rate': 0.00012676470588235294, 'epoch': 11.16}\n",
      "{'loss': 0.1592, 'learning_rate': 0.0001264705882352941, 'epoch': 11.17}\n",
      "{'loss': 0.1577, 'learning_rate': 0.0001261764705882353, 'epoch': 11.18}\n",
      "{'loss': 0.1822, 'learning_rate': 0.0001258823529411765, 'epoch': 11.19}\n",
      "{'loss': 0.2077, 'learning_rate': 0.00012558823529411765, 'epoch': 11.2}\n",
      "{'loss': 0.1688, 'learning_rate': 0.00012529411764705884, 'epoch': 11.21}\n",
      "{'loss': 0.1441, 'learning_rate': 0.000125, 'epoch': 11.21}\n",
      "{'loss': 0.2078, 'learning_rate': 0.0001247058823529412, 'epoch': 11.22}\n",
      "{'loss': 0.2092, 'learning_rate': 0.00012441176470588236, 'epoch': 11.23}\n",
      "{'loss': 0.1858, 'learning_rate': 0.00012411764705882355, 'epoch': 11.24}\n",
      "{'loss': 0.206, 'learning_rate': 0.0001238235294117647, 'epoch': 11.25}\n",
      "{'loss': 0.1598, 'learning_rate': 0.0001235294117647059, 'epoch': 11.26}\n",
      "{'loss': 0.1718, 'learning_rate': 0.00012323529411764706, 'epoch': 11.27}\n",
      "{'loss': 0.188, 'learning_rate': 0.00012294117647058825, 'epoch': 11.28}\n",
      "{'loss': 0.1403, 'learning_rate': 0.00012264705882352941, 'epoch': 11.29}\n",
      "{'loss': 0.1451, 'learning_rate': 0.0001223529411764706, 'epoch': 11.29}\n",
      "{'loss': 0.1648, 'learning_rate': 0.00012205882352941178, 'epoch': 11.3}\n",
      "{'loss': 0.2129, 'learning_rate': 0.00012176470588235293, 'epoch': 11.31}\n",
      "{'loss': 0.2048, 'learning_rate': 0.00012147058823529412, 'epoch': 11.32}\n",
      "{'loss': 0.2073, 'learning_rate': 0.0001211764705882353, 'epoch': 11.33}\n",
      "{'loss': 0.155, 'learning_rate': 0.00012088235294117647, 'epoch': 11.34}\n",
      "{'loss': 0.2126, 'learning_rate': 0.00012058823529411765, 'epoch': 11.35}\n",
      "{'loss': 0.1718, 'learning_rate': 0.00012029411764705883, 'epoch': 11.36}\n",
      "{'loss': 0.2308, 'learning_rate': 0.00012, 'epoch': 11.37}\n",
      "{'loss': 0.1597, 'learning_rate': 0.00011970588235294118, 'epoch': 11.38}\n",
      "{'loss': 0.2201, 'learning_rate': 0.00011941176470588236, 'epoch': 11.38}\n",
      "{'loss': 0.1867, 'learning_rate': 0.00011911764705882353, 'epoch': 11.39}\n",
      "{'loss': 0.2031, 'learning_rate': 0.00011882352941176471, 'epoch': 11.4}\n",
      "{'loss': 0.1421, 'learning_rate': 0.00011852941176470589, 'epoch': 11.41}\n",
      "{'loss': 0.1459, 'learning_rate': 0.00011823529411764706, 'epoch': 11.42}\n",
      "{'loss': 0.1835, 'learning_rate': 0.00011794117647058824, 'epoch': 11.43}\n",
      "{'loss': 0.2102, 'learning_rate': 0.00011764705882352942, 'epoch': 11.44}\n",
      "{'loss': 0.166, 'learning_rate': 0.0001173529411764706, 'epoch': 11.45}\n",
      "{'loss': 0.1905, 'learning_rate': 0.00011705882352941178, 'epoch': 11.46}\n",
      "{'loss': 0.1602, 'learning_rate': 0.00011676470588235296, 'epoch': 11.46}\n",
      "{'loss': 0.1696, 'learning_rate': 0.00011647058823529413, 'epoch': 11.47}\n",
      "{'loss': 0.1853, 'learning_rate': 0.00011617647058823531, 'epoch': 11.48}\n",
      "{'loss': 0.2173, 'learning_rate': 0.00011588235294117649, 'epoch': 11.49}\n",
      "{'loss': 0.2138, 'learning_rate': 0.00011558823529411764, 'epoch': 11.5}\n",
      "{'loss': 0.2004, 'learning_rate': 0.00011529411764705881, 'epoch': 11.51}\n",
      "{'loss': 0.1719, 'learning_rate': 0.00011499999999999999, 'epoch': 11.52}\n",
      "{'loss': 0.1587, 'learning_rate': 0.00011470588235294118, 'epoch': 11.53}\n",
      "{'loss': 0.1541, 'learning_rate': 0.00011441176470588236, 'epoch': 11.54}\n",
      "{'loss': 0.158, 'learning_rate': 0.00011411764705882353, 'epoch': 11.54}\n",
      "{'loss': 0.1714, 'learning_rate': 0.00011382352941176471, 'epoch': 11.55}\n",
      "{'loss': 0.1713, 'learning_rate': 0.00011352941176470589, 'epoch': 11.56}\n",
      "{'loss': 0.2813, 'learning_rate': 0.00011323529411764706, 'epoch': 11.57}\n",
      "{'loss': 0.1855, 'learning_rate': 0.00011294117647058824, 'epoch': 11.58}\n",
      "{'loss': 0.163, 'learning_rate': 0.00011264705882352942, 'epoch': 11.59}\n",
      "{'loss': 0.1962, 'learning_rate': 0.00011235294117647059, 'epoch': 11.6}\n",
      "{'loss': 0.1893, 'learning_rate': 0.00011205882352941177, 'epoch': 11.61}\n",
      "{'loss': 0.1593, 'learning_rate': 0.00011176470588235294, 'epoch': 11.62}\n",
      "{'loss': 0.1743, 'learning_rate': 0.00011147058823529412, 'epoch': 11.62}\n",
      "{'loss': 0.1846, 'learning_rate': 0.0001111764705882353, 'epoch': 11.63}\n",
      "{'loss': 0.2098, 'learning_rate': 0.00011088235294117649, 'epoch': 11.64}\n",
      "{'loss': 0.1907, 'learning_rate': 0.00011058823529411766, 'epoch': 11.65}\n",
      "{'loss': 0.1798, 'learning_rate': 0.00011029411764705884, 'epoch': 11.66}\n",
      "{'loss': 0.1727, 'learning_rate': 0.00011000000000000002, 'epoch': 11.67}\n",
      "{'loss': 0.1774, 'learning_rate': 0.0001097058823529412, 'epoch': 11.68}\n",
      "{'loss': 0.186, 'learning_rate': 0.00010941176470588237, 'epoch': 11.69}\n",
      "{'loss': 0.1821, 'learning_rate': 0.00010911764705882352, 'epoch': 11.7}\n",
      "{'loss': 0.1443, 'learning_rate': 0.0001088235294117647, 'epoch': 11.71}\n",
      "{'loss': 0.1808, 'learning_rate': 0.00010852941176470587, 'epoch': 11.71}\n",
      "{'loss': 0.1832, 'learning_rate': 0.00010823529411764706, 'epoch': 11.72}\n",
      "{'loss': 0.1825, 'learning_rate': 0.00010794117647058824, 'epoch': 11.73}\n",
      "{'loss': 0.2303, 'learning_rate': 0.00010764705882352942, 'epoch': 11.74}\n",
      "{'loss': 0.1991, 'learning_rate': 0.00010735294117647059, 'epoch': 11.75}\n",
      "{'loss': 0.2127, 'learning_rate': 0.00010705882352941177, 'epoch': 11.76}\n",
      "{'loss': 0.211, 'learning_rate': 0.00010676470588235295, 'epoch': 11.77}\n",
      "{'loss': 0.1519, 'learning_rate': 0.00010647058823529412, 'epoch': 11.78}\n",
      "{'loss': 0.2809, 'learning_rate': 0.0001061764705882353, 'epoch': 11.79}\n",
      "{'loss': 0.1294, 'learning_rate': 0.00010588235294117647, 'epoch': 11.79}\n",
      "{'loss': 0.1784, 'learning_rate': 0.00010558823529411765, 'epoch': 11.8}\n",
      "{'loss': 0.2232, 'learning_rate': 0.00010529411764705883, 'epoch': 11.81}\n",
      "{'loss': 0.1497, 'learning_rate': 0.000105, 'epoch': 11.82}\n",
      "{'loss': 0.1943, 'learning_rate': 0.00010470588235294118, 'epoch': 11.83}\n",
      "{'loss': 0.2151, 'learning_rate': 0.00010441176470588237, 'epoch': 11.84}\n",
      "{'loss': 0.1784, 'learning_rate': 0.00010411764705882355, 'epoch': 11.85}\n",
      "{'loss': 0.1462, 'learning_rate': 0.00010382352941176472, 'epoch': 11.86}\n",
      "{'loss': 0.1682, 'learning_rate': 0.0001035294117647059, 'epoch': 11.87}\n",
      "{'loss': 0.1713, 'learning_rate': 0.00010323529411764708, 'epoch': 11.88}\n",
      "{'loss': 0.1785, 'learning_rate': 0.00010294117647058823, 'epoch': 11.88}\n",
      "{'loss': 0.1668, 'learning_rate': 0.0001026470588235294, 'epoch': 11.89}\n",
      "{'loss': 0.1864, 'learning_rate': 0.00010235294117647058, 'epoch': 11.9}\n",
      "{'loss': 0.1915, 'learning_rate': 0.00010205882352941176, 'epoch': 11.91}\n",
      "{'loss': 0.1518, 'learning_rate': 0.00010176470588235295, 'epoch': 11.92}\n",
      "{'loss': 0.1999, 'learning_rate': 0.00010147058823529412, 'epoch': 11.93}\n",
      "{'loss': 0.1781, 'learning_rate': 0.0001011764705882353, 'epoch': 11.94}\n",
      "{'loss': 0.1623, 'learning_rate': 0.00010088235294117648, 'epoch': 11.95}\n",
      "{'loss': 0.1475, 'learning_rate': 0.00010058823529411765, 'epoch': 11.96}\n",
      "{'loss': 0.2001, 'learning_rate': 0.00010029411764705883, 'epoch': 11.96}\n",
      "{'loss': 0.2011, 'learning_rate': 0.0001, 'epoch': 11.97}\n",
      "{'loss': 0.1849, 'learning_rate': 9.970588235294118e-05, 'epoch': 11.98}\n",
      "{'loss': 0.163, 'learning_rate': 9.941176470588236e-05, 'epoch': 11.99}\n",
      "{'loss': 0.1652, 'learning_rate': 9.911764705882353e-05, 'epoch': 12.0}\n",
      "{'loss': 0.1721, 'learning_rate': 9.882352941176471e-05, 'epoch': 12.01}\n",
      "{'loss': 0.1572, 'learning_rate': 9.852941176470589e-05, 'epoch': 12.02}\n",
      "{'loss': 0.1462, 'learning_rate': 9.823529411764706e-05, 'epoch': 12.03}\n",
      "{'loss': 0.188, 'learning_rate': 9.794117647058824e-05, 'epoch': 12.04}\n",
      "{'loss': 0.1709, 'learning_rate': 9.764705882352942e-05, 'epoch': 12.04}\n",
      "{'loss': 0.1422, 'learning_rate': 9.73529411764706e-05, 'epoch': 12.05}\n",
      "{'loss': 0.181, 'learning_rate': 9.705882352941177e-05, 'epoch': 12.06}\n",
      "{'loss': 0.1778, 'learning_rate': 9.676470588235295e-05, 'epoch': 12.07}\n",
      "{'loss': 0.1904, 'learning_rate': 9.647058823529412e-05, 'epoch': 12.08}\n",
      "{'loss': 0.2439, 'learning_rate': 9.61764705882353e-05, 'epoch': 12.09}\n",
      "{'loss': 0.1638, 'learning_rate': 9.588235294117648e-05, 'epoch': 12.1}\n",
      "{'loss': 0.1358, 'learning_rate': 9.558823529411765e-05, 'epoch': 12.11}\n",
      "{'loss': 0.1922, 'learning_rate': 9.529411764705883e-05, 'epoch': 12.12}\n",
      "{'loss': 0.2184, 'learning_rate': 9.5e-05, 'epoch': 12.12}\n",
      "{'loss': 0.179, 'learning_rate': 9.470588235294118e-05, 'epoch': 12.13}\n",
      "{'loss': 0.1683, 'learning_rate': 9.441176470588236e-05, 'epoch': 12.14}\n",
      "{'loss': 0.2086, 'learning_rate': 9.411764705882353e-05, 'epoch': 12.15}\n",
      "{'loss': 0.1527, 'learning_rate': 9.382352941176471e-05, 'epoch': 12.16}\n",
      "{'loss': 0.1561, 'learning_rate': 9.352941176470589e-05, 'epoch': 12.17}\n",
      "{'loss': 0.1782, 'learning_rate': 9.323529411764706e-05, 'epoch': 12.18}\n",
      "{'loss': 0.1223, 'learning_rate': 9.294117647058824e-05, 'epoch': 12.19}\n",
      "{'loss': 0.1303, 'learning_rate': 9.264705882352942e-05, 'epoch': 12.2}\n",
      "{'loss': 0.1645, 'learning_rate': 9.23529411764706e-05, 'epoch': 12.21}\n",
      "{'loss': 0.1969, 'learning_rate': 9.205882352941177e-05, 'epoch': 12.21}\n",
      "{'loss': 0.1425, 'learning_rate': 9.176470588235295e-05, 'epoch': 12.22}\n",
      "{'loss': 0.1846, 'learning_rate': 9.147058823529412e-05, 'epoch': 12.23}\n",
      "{'loss': 0.1482, 'learning_rate': 9.11764705882353e-05, 'epoch': 12.24}\n",
      "{'loss': 0.1415, 'learning_rate': 9.088235294117648e-05, 'epoch': 12.25}\n",
      "{'loss': 0.1763, 'learning_rate': 9.058823529411765e-05, 'epoch': 12.26}\n",
      "{'loss': 0.17, 'learning_rate': 9.029411764705883e-05, 'epoch': 12.27}\n",
      "{'loss': 0.1974, 'learning_rate': 9e-05, 'epoch': 12.28}\n",
      "{'loss': 0.1469, 'learning_rate': 8.970588235294118e-05, 'epoch': 12.29}\n",
      "{'loss': 0.219, 'learning_rate': 8.941176470588236e-05, 'epoch': 12.29}\n",
      "{'loss': 0.1577, 'learning_rate': 8.911764705882354e-05, 'epoch': 12.3}\n",
      "{'loss': 0.1589, 'learning_rate': 8.882352941176471e-05, 'epoch': 12.31}\n",
      "{'loss': 0.1927, 'learning_rate': 8.852941176470589e-05, 'epoch': 12.32}\n",
      "{'loss': 0.1772, 'learning_rate': 8.823529411764706e-05, 'epoch': 12.33}\n",
      "{'loss': 0.1377, 'learning_rate': 8.794117647058824e-05, 'epoch': 12.34}\n",
      "{'loss': 0.1664, 'learning_rate': 8.764705882352942e-05, 'epoch': 12.35}\n",
      "{'loss': 0.1349, 'learning_rate': 8.73529411764706e-05, 'epoch': 12.36}\n",
      "{'loss': 0.1602, 'learning_rate': 8.705882352941177e-05, 'epoch': 12.37}\n",
      "{'loss': 0.1745, 'learning_rate': 8.676470588235295e-05, 'epoch': 12.38}\n",
      "{'loss': 0.1933, 'learning_rate': 8.647058823529412e-05, 'epoch': 12.38}\n",
      "{'loss': 0.2194, 'learning_rate': 8.61764705882353e-05, 'epoch': 12.39}\n",
      "{'loss': 0.1486, 'learning_rate': 8.588235294117646e-05, 'epoch': 12.4}\n",
      "{'loss': 0.1434, 'learning_rate': 8.558823529411765e-05, 'epoch': 12.41}\n",
      "{'loss': 0.1769, 'learning_rate': 8.529411764705883e-05, 'epoch': 12.42}\n",
      "{'loss': 0.1757, 'learning_rate': 8.5e-05, 'epoch': 12.43}\n",
      "{'loss': 0.2098, 'learning_rate': 8.470588235294118e-05, 'epoch': 12.44}\n",
      "{'loss': 0.171, 'learning_rate': 8.441176470588236e-05, 'epoch': 12.45}\n",
      "{'loss': 0.1544, 'learning_rate': 8.411764705882354e-05, 'epoch': 12.46}\n",
      "{'loss': 0.1458, 'learning_rate': 8.382352941176471e-05, 'epoch': 12.46}\n",
      "{'loss': 0.1742, 'learning_rate': 8.352941176470589e-05, 'epoch': 12.47}\n",
      "{'loss': 0.1865, 'learning_rate': 8.323529411764707e-05, 'epoch': 12.48}\n",
      "{'loss': 0.1357, 'learning_rate': 8.294117647058824e-05, 'epoch': 12.49}\n",
      "{'loss': 0.1752, 'learning_rate': 8.26470588235294e-05, 'epoch': 12.5}\n",
      "{'loss': 0.154, 'learning_rate': 8.23529411764706e-05, 'epoch': 12.51}\n",
      "{'loss': 0.1895, 'learning_rate': 8.205882352941177e-05, 'epoch': 12.52}\n",
      "{'loss': 0.1592, 'learning_rate': 8.176470588235295e-05, 'epoch': 12.53}\n",
      "{'loss': 0.1559, 'learning_rate': 8.147058823529412e-05, 'epoch': 12.54}\n",
      "{'loss': 0.1545, 'learning_rate': 8.11764705882353e-05, 'epoch': 12.54}\n",
      "{'loss': 0.1422, 'learning_rate': 8.088235294117648e-05, 'epoch': 12.55}\n",
      "{'loss': 0.1493, 'learning_rate': 8.058823529411765e-05, 'epoch': 12.56}\n",
      "{'loss': 0.1875, 'learning_rate': 8.029411764705883e-05, 'epoch': 12.57}\n",
      "{'loss': 0.1592, 'learning_rate': 8e-05, 'epoch': 12.58}\n",
      "{'loss': 0.1463, 'learning_rate': 7.970588235294118e-05, 'epoch': 12.59}\n",
      "{'loss': 0.1586, 'learning_rate': 7.941176470588235e-05, 'epoch': 12.6}\n",
      "{'loss': 0.1401, 'learning_rate': 7.911764705882354e-05, 'epoch': 12.61}\n",
      "{'loss': 0.1565, 'learning_rate': 7.882352941176471e-05, 'epoch': 12.62}\n",
      "{'loss': 0.1723, 'learning_rate': 7.852941176470589e-05, 'epoch': 12.62}\n",
      "{'loss': 0.1916, 'learning_rate': 7.823529411764707e-05, 'epoch': 12.63}\n",
      "{'loss': 0.2027, 'learning_rate': 7.794117647058824e-05, 'epoch': 12.64}\n",
      "{'loss': 0.1936, 'learning_rate': 7.764705882352942e-05, 'epoch': 12.65}\n",
      "{'loss': 0.2059, 'learning_rate': 7.73529411764706e-05, 'epoch': 12.66}\n",
      "{'loss': 0.1686, 'learning_rate': 7.705882352941177e-05, 'epoch': 12.67}\n",
      "{'loss': 0.1343, 'learning_rate': 7.676470588235295e-05, 'epoch': 12.68}\n",
      "{'loss': 0.133, 'learning_rate': 7.647058823529411e-05, 'epoch': 12.69}\n",
      "{'loss': 0.1666, 'learning_rate': 7.617647058823529e-05, 'epoch': 12.7}\n",
      "{'loss': 0.1376, 'learning_rate': 7.588235294117648e-05, 'epoch': 12.71}\n",
      "{'loss': 0.188, 'learning_rate': 7.558823529411765e-05, 'epoch': 12.71}\n",
      "{'loss': 0.1904, 'learning_rate': 7.529411764705883e-05, 'epoch': 12.72}\n",
      "{'loss': 0.1479, 'learning_rate': 7.500000000000001e-05, 'epoch': 12.73}\n",
      "{'loss': 0.1557, 'learning_rate': 7.470588235294118e-05, 'epoch': 12.74}\n",
      "{'loss': 0.1228, 'learning_rate': 7.441176470588236e-05, 'epoch': 12.75}\n",
      "{'loss': 0.1799, 'learning_rate': 7.411764705882354e-05, 'epoch': 12.76}\n",
      "{'loss': 0.1499, 'learning_rate': 7.382352941176471e-05, 'epoch': 12.77}\n",
      "{'loss': 0.1831, 'learning_rate': 7.352941176470589e-05, 'epoch': 12.78}\n",
      "{'loss': 0.1864, 'learning_rate': 7.323529411764705e-05, 'epoch': 12.79}\n",
      "{'loss': 0.1569, 'learning_rate': 7.294117647058823e-05, 'epoch': 12.79}\n",
      "{'loss': 0.1618, 'learning_rate': 7.264705882352942e-05, 'epoch': 12.8}\n",
      "{'loss': 0.148, 'learning_rate': 7.23529411764706e-05, 'epoch': 12.81}\n",
      "{'loss': 0.2166, 'learning_rate': 7.205882352941177e-05, 'epoch': 12.82}\n",
      "{'loss': 0.1395, 'learning_rate': 7.176470588235295e-05, 'epoch': 12.83}\n",
      "{'loss': 0.1839, 'learning_rate': 7.147058823529412e-05, 'epoch': 12.84}\n",
      "{'loss': 0.1815, 'learning_rate': 7.11764705882353e-05, 'epoch': 12.85}\n",
      "{'loss': 0.1621, 'learning_rate': 7.088235294117648e-05, 'epoch': 12.86}\n",
      "{'loss': 0.1536, 'learning_rate': 7.058823529411765e-05, 'epoch': 12.87}\n",
      "{'loss': 0.109, 'learning_rate': 7.029411764705882e-05, 'epoch': 12.88}\n",
      "{'loss': 0.1658, 'learning_rate': 7e-05, 'epoch': 12.88}\n",
      "{'loss': 0.2075, 'learning_rate': 6.970588235294117e-05, 'epoch': 12.89}\n",
      "{'loss': 0.1575, 'learning_rate': 6.941176470588236e-05, 'epoch': 12.9}\n",
      "{'loss': 0.166, 'learning_rate': 6.911764705882354e-05, 'epoch': 12.91}\n",
      "{'loss': 0.1561, 'learning_rate': 6.882352941176471e-05, 'epoch': 12.92}\n",
      "{'loss': 0.1449, 'learning_rate': 6.852941176470589e-05, 'epoch': 12.93}\n",
      "{'loss': 0.1398, 'learning_rate': 6.823529411764707e-05, 'epoch': 12.94}\n",
      "{'loss': 0.1551, 'learning_rate': 6.794117647058824e-05, 'epoch': 12.95}\n",
      "{'loss': 0.1613, 'learning_rate': 6.764705882352942e-05, 'epoch': 12.96}\n",
      "{'loss': 0.1318, 'learning_rate': 6.73529411764706e-05, 'epoch': 12.96}\n",
      "{'loss': 0.2183, 'learning_rate': 6.705882352941176e-05, 'epoch': 12.97}\n",
      "{'loss': 0.1595, 'learning_rate': 6.676470588235294e-05, 'epoch': 12.98}\n",
      "{'loss': 0.159, 'learning_rate': 6.647058823529411e-05, 'epoch': 12.99}\n",
      "{'loss': 0.1517, 'learning_rate': 6.61764705882353e-05, 'epoch': 13.0}\n",
      "{'loss': 0.139, 'learning_rate': 6.588235294117648e-05, 'epoch': 13.01}\n",
      "{'loss': 0.1214, 'learning_rate': 6.558823529411765e-05, 'epoch': 13.02}\n",
      "{'loss': 0.1533, 'learning_rate': 6.529411764705883e-05, 'epoch': 13.03}\n",
      "{'loss': 0.1748, 'learning_rate': 6.500000000000001e-05, 'epoch': 13.04}\n",
      "{'loss': 0.1958, 'learning_rate': 6.470588235294118e-05, 'epoch': 13.04}\n",
      "{'loss': 0.1707, 'learning_rate': 6.441176470588236e-05, 'epoch': 13.05}\n",
      "{'loss': 0.1618, 'learning_rate': 6.411764705882354e-05, 'epoch': 13.06}\n",
      "{'loss': 0.1778, 'learning_rate': 6.38235294117647e-05, 'epoch': 13.07}\n",
      "{'loss': 0.1317, 'learning_rate': 6.352941176470588e-05, 'epoch': 13.08}\n",
      "{'loss': 0.1541, 'learning_rate': 6.323529411764705e-05, 'epoch': 13.09}\n",
      "{'loss': 0.1711, 'learning_rate': 6.294117647058824e-05, 'epoch': 13.1}\n",
      "{'loss': 0.1998, 'learning_rate': 6.264705882352942e-05, 'epoch': 13.11}\n",
      "{'loss': 0.1627, 'learning_rate': 6.23529411764706e-05, 'epoch': 13.12}\n",
      "{'loss': 0.1371, 'learning_rate': 6.205882352941177e-05, 'epoch': 13.12}\n",
      "{'loss': 0.1692, 'learning_rate': 6.176470588235295e-05, 'epoch': 13.13}\n",
      "{'loss': 0.1751, 'learning_rate': 6.147058823529413e-05, 'epoch': 13.14}\n",
      "{'loss': 0.1723, 'learning_rate': 6.11764705882353e-05, 'epoch': 13.15}\n",
      "{'loss': 0.1807, 'learning_rate': 6.0882352941176465e-05, 'epoch': 13.16}\n",
      "{'loss': 0.1992, 'learning_rate': 6.058823529411765e-05, 'epoch': 13.17}\n",
      "{'loss': 0.1728, 'learning_rate': 6.0294117647058825e-05, 'epoch': 13.18}\n",
      "{'loss': 0.1615, 'learning_rate': 6e-05, 'epoch': 13.19}\n",
      "{'loss': 0.1963, 'learning_rate': 5.970588235294118e-05, 'epoch': 13.2}\n",
      "{'loss': 0.179, 'learning_rate': 5.9411764705882355e-05, 'epoch': 13.21}\n",
      "{'loss': 0.1754, 'learning_rate': 5.911764705882353e-05, 'epoch': 13.21}\n",
      "{'loss': 0.1238, 'learning_rate': 5.882352941176471e-05, 'epoch': 13.22}\n",
      "{'loss': 0.1727, 'learning_rate': 5.852941176470589e-05, 'epoch': 13.23}\n",
      "{'loss': 0.1545, 'learning_rate': 5.823529411764707e-05, 'epoch': 13.24}\n",
      "{'loss': 0.1874, 'learning_rate': 5.7941176470588244e-05, 'epoch': 13.25}\n",
      "{'loss': 0.1629, 'learning_rate': 5.764705882352941e-05, 'epoch': 13.26}\n",
      "{'loss': 0.1507, 'learning_rate': 5.735294117647059e-05, 'epoch': 13.27}\n",
      "{'loss': 0.1394, 'learning_rate': 5.7058823529411766e-05, 'epoch': 13.28}\n",
      "{'loss': 0.1915, 'learning_rate': 5.676470588235294e-05, 'epoch': 13.29}\n",
      "{'loss': 0.1357, 'learning_rate': 5.647058823529412e-05, 'epoch': 13.29}\n",
      "{'loss': 0.141, 'learning_rate': 5.6176470588235296e-05, 'epoch': 13.3}\n",
      "{'loss': 0.1379, 'learning_rate': 5.588235294117647e-05, 'epoch': 13.31}\n",
      "{'loss': 0.1269, 'learning_rate': 5.558823529411765e-05, 'epoch': 13.32}\n",
      "{'loss': 0.1231, 'learning_rate': 5.529411764705883e-05, 'epoch': 13.33}\n",
      "{'loss': 0.1488, 'learning_rate': 5.500000000000001e-05, 'epoch': 13.34}\n",
      "{'loss': 0.1277, 'learning_rate': 5.4705882352941185e-05, 'epoch': 13.35}\n",
      "{'loss': 0.1365, 'learning_rate': 5.441176470588235e-05, 'epoch': 13.36}\n",
      "{'loss': 0.1683, 'learning_rate': 5.411764705882353e-05, 'epoch': 13.37}\n",
      "{'loss': 0.1981, 'learning_rate': 5.382352941176471e-05, 'epoch': 13.38}\n",
      "{'loss': 0.172, 'learning_rate': 5.3529411764705884e-05, 'epoch': 13.38}\n",
      "{'loss': 0.1599, 'learning_rate': 5.323529411764706e-05, 'epoch': 13.39}\n",
      "{'loss': 0.1364, 'learning_rate': 5.294117647058824e-05, 'epoch': 13.4}\n",
      "{'loss': 0.1152, 'learning_rate': 5.2647058823529414e-05, 'epoch': 13.41}\n",
      "{'loss': 0.1468, 'learning_rate': 5.235294117647059e-05, 'epoch': 13.42}\n",
      "{'loss': 0.1058, 'learning_rate': 5.2058823529411774e-05, 'epoch': 13.43}\n",
      "{'loss': 0.1716, 'learning_rate': 5.176470588235295e-05, 'epoch': 13.44}\n",
      "{'loss': 0.171, 'learning_rate': 5.147058823529411e-05, 'epoch': 13.45}\n",
      "{'loss': 0.1719, 'learning_rate': 5.117647058823529e-05, 'epoch': 13.46}\n",
      "{'loss': 0.1776, 'learning_rate': 5.088235294117647e-05, 'epoch': 13.46}\n",
      "{'loss': 0.1613, 'learning_rate': 5.058823529411765e-05, 'epoch': 13.47}\n",
      "{'loss': 0.1348, 'learning_rate': 5.0294117647058826e-05, 'epoch': 13.48}\n",
      "{'loss': 0.1857, 'learning_rate': 5e-05, 'epoch': 13.49}\n",
      "{'loss': 0.156, 'learning_rate': 4.970588235294118e-05, 'epoch': 13.5}\n",
      "{'loss': 0.1401, 'learning_rate': 4.9411764705882355e-05, 'epoch': 13.51}\n",
      "{'loss': 0.1688, 'learning_rate': 4.911764705882353e-05, 'epoch': 13.52}\n",
      "{'loss': 0.1631, 'learning_rate': 4.882352941176471e-05, 'epoch': 13.53}\n",
      "{'loss': 0.1892, 'learning_rate': 4.8529411764705885e-05, 'epoch': 13.54}\n",
      "{'loss': 0.1341, 'learning_rate': 4.823529411764706e-05, 'epoch': 13.54}\n",
      "{'loss': 0.1496, 'learning_rate': 4.794117647058824e-05, 'epoch': 13.55}\n",
      "{'loss': 0.1532, 'learning_rate': 4.7647058823529414e-05, 'epoch': 13.56}\n",
      "{'loss': 0.1476, 'learning_rate': 4.735294117647059e-05, 'epoch': 13.57}\n",
      "{'loss': 0.1548, 'learning_rate': 4.705882352941177e-05, 'epoch': 13.58}\n",
      "{'loss': 0.1674, 'learning_rate': 4.6764705882352944e-05, 'epoch': 13.59}\n",
      "{'loss': 0.1405, 'learning_rate': 4.647058823529412e-05, 'epoch': 13.6}\n",
      "{'loss': 0.1505, 'learning_rate': 4.61764705882353e-05, 'epoch': 13.61}\n",
      "{'loss': 0.1551, 'learning_rate': 4.588235294117647e-05, 'epoch': 13.62}\n",
      "{'loss': 0.1752, 'learning_rate': 4.558823529411765e-05, 'epoch': 13.62}\n",
      "{'loss': 0.1542, 'learning_rate': 4.5294117647058826e-05, 'epoch': 13.63}\n",
      "{'loss': 0.1614, 'learning_rate': 4.5e-05, 'epoch': 13.64}\n",
      "{'loss': 0.1248, 'learning_rate': 4.470588235294118e-05, 'epoch': 13.65}\n",
      "{'loss': 0.1196, 'learning_rate': 4.4411764705882356e-05, 'epoch': 13.66}\n",
      "{'loss': 0.1602, 'learning_rate': 4.411764705882353e-05, 'epoch': 13.67}\n",
      "{'loss': 0.1637, 'learning_rate': 4.382352941176471e-05, 'epoch': 13.68}\n",
      "{'loss': 0.1591, 'learning_rate': 4.3529411764705885e-05, 'epoch': 13.69}\n",
      "{'loss': 0.1463, 'learning_rate': 4.323529411764706e-05, 'epoch': 13.7}\n",
      "{'loss': 0.1836, 'learning_rate': 4.294117647058823e-05, 'epoch': 13.71}\n",
      "{'loss': 0.1369, 'learning_rate': 4.2647058823529415e-05, 'epoch': 13.71}\n",
      "{'loss': 0.1235, 'learning_rate': 4.235294117647059e-05, 'epoch': 13.72}\n",
      "{'loss': 0.1444, 'learning_rate': 4.205882352941177e-05, 'epoch': 13.73}\n",
      "{'loss': 0.168, 'learning_rate': 4.1764705882352944e-05, 'epoch': 13.74}\n",
      "{'loss': 0.1298, 'learning_rate': 4.147058823529412e-05, 'epoch': 13.75}\n",
      "{'loss': 0.1974, 'learning_rate': 4.11764705882353e-05, 'epoch': 13.76}\n",
      "{'loss': 0.1281, 'learning_rate': 4.0882352941176474e-05, 'epoch': 13.77}\n",
      "{'loss': 0.15, 'learning_rate': 4.058823529411765e-05, 'epoch': 13.78}\n",
      "{'loss': 0.1429, 'learning_rate': 4.029411764705883e-05, 'epoch': 13.79}\n",
      "{'loss': 0.1494, 'learning_rate': 4e-05, 'epoch': 13.79}\n",
      "{'loss': 0.1403, 'learning_rate': 3.970588235294117e-05, 'epoch': 13.8}\n",
      "{'loss': 0.1797, 'learning_rate': 3.9411764705882356e-05, 'epoch': 13.81}\n",
      "{'loss': 0.1356, 'learning_rate': 3.911764705882353e-05, 'epoch': 13.82}\n",
      "{'loss': 0.1307, 'learning_rate': 3.882352941176471e-05, 'epoch': 13.83}\n",
      "{'loss': 0.1463, 'learning_rate': 3.8529411764705886e-05, 'epoch': 13.84}\n",
      "{'loss': 0.1226, 'learning_rate': 3.8235294117647055e-05, 'epoch': 13.85}\n",
      "{'loss': 0.1541, 'learning_rate': 3.794117647058824e-05, 'epoch': 13.86}\n",
      "{'loss': 0.1039, 'learning_rate': 3.7647058823529415e-05, 'epoch': 13.87}\n",
      "{'loss': 0.1444, 'learning_rate': 3.735294117647059e-05, 'epoch': 13.88}\n",
      "{'loss': 0.1782, 'learning_rate': 3.705882352941177e-05, 'epoch': 13.88}\n",
      "{'loss': 0.1595, 'learning_rate': 3.6764705882352945e-05, 'epoch': 13.89}\n",
      "{'loss': 0.1357, 'learning_rate': 3.6470588235294114e-05, 'epoch': 13.9}\n",
      "{'loss': 0.1703, 'learning_rate': 3.61764705882353e-05, 'epoch': 13.91}\n",
      "{'loss': 0.1721, 'learning_rate': 3.5882352941176474e-05, 'epoch': 13.92}\n",
      "{'loss': 0.1362, 'learning_rate': 3.558823529411765e-05, 'epoch': 13.93}\n",
      "{'loss': 0.1276, 'learning_rate': 3.529411764705883e-05, 'epoch': 13.94}\n",
      "{'loss': 0.1659, 'learning_rate': 3.5e-05, 'epoch': 13.95}\n",
      "{'loss': 0.2072, 'learning_rate': 3.470588235294118e-05, 'epoch': 13.96}\n",
      "{'loss': 0.1466, 'learning_rate': 3.441176470588236e-05, 'epoch': 13.96}\n",
      "{'loss': 0.132, 'learning_rate': 3.411764705882353e-05, 'epoch': 13.97}\n",
      "{'loss': 0.1587, 'learning_rate': 3.382352941176471e-05, 'epoch': 13.98}\n",
      "{'loss': 0.211, 'learning_rate': 3.352941176470588e-05, 'epoch': 13.99}\n",
      "{'loss': 0.1349, 'learning_rate': 3.3235294117647056e-05, 'epoch': 14.0}\n",
      "{'loss': 0.1273, 'learning_rate': 3.294117647058824e-05, 'epoch': 14.01}\n",
      "{'loss': 0.1568, 'learning_rate': 3.2647058823529416e-05, 'epoch': 14.02}\n",
      "{'loss': 0.1256, 'learning_rate': 3.235294117647059e-05, 'epoch': 14.03}\n",
      "{'loss': 0.1791, 'learning_rate': 3.205882352941177e-05, 'epoch': 14.04}\n",
      "{'loss': 0.1482, 'learning_rate': 3.176470588235294e-05, 'epoch': 14.04}\n",
      "{'loss': 0.1297, 'learning_rate': 3.147058823529412e-05, 'epoch': 14.05}\n",
      "{'loss': 0.1785, 'learning_rate': 3.11764705882353e-05, 'epoch': 14.06}\n",
      "{'loss': 0.2231, 'learning_rate': 3.0882352941176475e-05, 'epoch': 14.07}\n",
      "{'loss': 0.1136, 'learning_rate': 3.058823529411765e-05, 'epoch': 14.08}\n",
      "{'loss': 0.1434, 'learning_rate': 3.0294117647058824e-05, 'epoch': 14.09}\n",
      "{'loss': 0.1375, 'learning_rate': 3e-05, 'epoch': 14.1}\n",
      "{'loss': 0.1477, 'learning_rate': 2.9705882352941177e-05, 'epoch': 14.11}\n",
      "{'loss': 0.162, 'learning_rate': 2.9411764705882354e-05, 'epoch': 14.12}\n",
      "{'loss': 0.1579, 'learning_rate': 2.9117647058823534e-05, 'epoch': 14.12}\n",
      "{'loss': 0.1274, 'learning_rate': 2.8823529411764703e-05, 'epoch': 14.13}\n",
      "{'loss': 0.1315, 'learning_rate': 2.8529411764705883e-05, 'epoch': 14.14}\n",
      "{'loss': 0.1614, 'learning_rate': 2.823529411764706e-05, 'epoch': 14.15}\n",
      "{'loss': 0.116, 'learning_rate': 2.7941176470588236e-05, 'epoch': 14.16}\n",
      "{'loss': 0.1238, 'learning_rate': 2.7647058823529416e-05, 'epoch': 14.17}\n",
      "{'loss': 0.13, 'learning_rate': 2.7352941176470593e-05, 'epoch': 14.18}\n",
      "{'loss': 0.1742, 'learning_rate': 2.7058823529411766e-05, 'epoch': 14.19}\n",
      "{'loss': 0.1741, 'learning_rate': 2.6764705882352942e-05, 'epoch': 14.2}\n",
      "{'loss': 0.1422, 'learning_rate': 2.647058823529412e-05, 'epoch': 14.21}\n",
      "{'loss': 0.1499, 'learning_rate': 2.6176470588235295e-05, 'epoch': 14.21}\n",
      "{'loss': 0.1723, 'learning_rate': 2.5882352941176475e-05, 'epoch': 14.22}\n",
      "{'loss': 0.1384, 'learning_rate': 2.5588235294117645e-05, 'epoch': 14.23}\n",
      "{'loss': 0.1497, 'learning_rate': 2.5294117647058825e-05, 'epoch': 14.24}\n",
      "{'loss': 0.1476, 'learning_rate': 2.5e-05, 'epoch': 14.25}\n",
      "{'loss': 0.1857, 'learning_rate': 2.4705882352941178e-05, 'epoch': 14.26}\n",
      "{'loss': 0.1347, 'learning_rate': 2.4411764705882354e-05, 'epoch': 14.27}\n",
      "{'loss': 0.1351, 'learning_rate': 2.411764705882353e-05, 'epoch': 14.28}\n",
      "{'loss': 0.1515, 'learning_rate': 2.3823529411764707e-05, 'epoch': 14.29}\n",
      "{'loss': 0.1369, 'learning_rate': 2.3529411764705884e-05, 'epoch': 14.29}\n",
      "{'loss': 0.1404, 'learning_rate': 2.323529411764706e-05, 'epoch': 14.3}\n",
      "{'loss': 0.2045, 'learning_rate': 2.2941176470588237e-05, 'epoch': 14.31}\n",
      "{'loss': 0.16, 'learning_rate': 2.2647058823529413e-05, 'epoch': 14.32}\n",
      "{'loss': 0.1564, 'learning_rate': 2.235294117647059e-05, 'epoch': 14.33}\n",
      "{'loss': 0.1352, 'learning_rate': 2.2058823529411766e-05, 'epoch': 14.34}\n",
      "{'loss': 0.1508, 'learning_rate': 2.1764705882352943e-05, 'epoch': 14.35}\n",
      "{'loss': 0.1266, 'learning_rate': 2.1470588235294116e-05, 'epoch': 14.36}\n",
      "{'loss': 0.1471, 'learning_rate': 2.1176470588235296e-05, 'epoch': 14.37}\n",
      "{'loss': 0.2013, 'learning_rate': 2.0882352941176472e-05, 'epoch': 14.38}\n",
      "{'loss': 0.1784, 'learning_rate': 2.058823529411765e-05, 'epoch': 14.38}\n",
      "{'loss': 0.157, 'learning_rate': 2.0294117647058825e-05, 'epoch': 14.39}\n",
      "{'loss': 0.1561, 'learning_rate': 2e-05, 'epoch': 14.4}\n",
      "{'loss': 0.153, 'learning_rate': 1.9705882352941178e-05, 'epoch': 14.41}\n",
      "{'loss': 0.1905, 'learning_rate': 1.9411764705882355e-05, 'epoch': 14.42}\n",
      "{'loss': 0.1703, 'learning_rate': 1.9117647058823528e-05, 'epoch': 14.43}\n",
      "{'loss': 0.2026, 'learning_rate': 1.8823529411764708e-05, 'epoch': 14.44}\n",
      "{'loss': 0.2107, 'learning_rate': 1.8529411764705884e-05, 'epoch': 14.45}\n",
      "{'loss': 0.1444, 'learning_rate': 1.8235294117647057e-05, 'epoch': 14.46}\n",
      "{'loss': 0.1532, 'learning_rate': 1.7941176470588237e-05, 'epoch': 14.46}\n",
      "{'loss': 0.1263, 'learning_rate': 1.7647058823529414e-05, 'epoch': 14.47}\n",
      "{'loss': 0.1433, 'learning_rate': 1.735294117647059e-05, 'epoch': 14.48}\n",
      "{'loss': 0.1767, 'learning_rate': 1.7058823529411767e-05, 'epoch': 14.49}\n",
      "{'loss': 0.1493, 'learning_rate': 1.676470588235294e-05, 'epoch': 14.5}\n",
      "{'loss': 0.1699, 'learning_rate': 1.647058823529412e-05, 'epoch': 14.51}\n",
      "{'loss': 0.1758, 'learning_rate': 1.6176470588235296e-05, 'epoch': 14.52}\n",
      "{'loss': 0.1604, 'learning_rate': 1.588235294117647e-05, 'epoch': 14.53}\n",
      "{'loss': 0.1602, 'learning_rate': 1.558823529411765e-05, 'epoch': 14.54}\n",
      "{'loss': 0.1699, 'learning_rate': 1.5294117647058826e-05, 'epoch': 14.54}\n",
      "{'loss': 0.1484, 'learning_rate': 1.5e-05, 'epoch': 14.55}\n",
      "{'loss': 0.1711, 'learning_rate': 1.4705882352941177e-05, 'epoch': 14.56}\n",
      "{'loss': 0.1356, 'learning_rate': 1.4411764705882352e-05, 'epoch': 14.57}\n",
      "{'loss': 0.1325, 'learning_rate': 1.411764705882353e-05, 'epoch': 14.58}\n",
      "{'loss': 0.1427, 'learning_rate': 1.3823529411764708e-05, 'epoch': 14.59}\n",
      "{'loss': 0.1527, 'learning_rate': 1.3529411764705883e-05, 'epoch': 14.6}\n",
      "{'loss': 0.1596, 'learning_rate': 1.323529411764706e-05, 'epoch': 14.61}\n",
      "{'loss': 0.1386, 'learning_rate': 1.2941176470588238e-05, 'epoch': 14.62}\n",
      "{'loss': 0.1403, 'learning_rate': 1.2647058823529412e-05, 'epoch': 14.62}\n",
      "{'loss': 0.1312, 'learning_rate': 1.2352941176470589e-05, 'epoch': 14.63}\n",
      "{'loss': 0.1503, 'learning_rate': 1.2058823529411765e-05, 'epoch': 14.64}\n",
      "{'loss': 0.1662, 'learning_rate': 1.1764705882352942e-05, 'epoch': 14.65}\n",
      "{'loss': 0.1516, 'learning_rate': 1.1470588235294118e-05, 'epoch': 14.66}\n",
      "{'loss': 0.1783, 'learning_rate': 1.1176470588235295e-05, 'epoch': 14.67}\n",
      "{'loss': 0.1196, 'learning_rate': 1.0882352941176471e-05, 'epoch': 14.68}\n",
      "{'loss': 0.1447, 'learning_rate': 1.0588235294117648e-05, 'epoch': 14.69}\n",
      "{'loss': 0.1268, 'learning_rate': 1.0294117647058824e-05, 'epoch': 14.7}\n",
      "{'loss': 0.1607, 'learning_rate': 1e-05, 'epoch': 14.71}\n",
      "{'loss': 0.1118, 'learning_rate': 9.705882352941177e-06, 'epoch': 14.71}\n",
      "{'loss': 0.1653, 'learning_rate': 9.411764705882354e-06, 'epoch': 14.72}\n",
      "{'loss': 0.2034, 'learning_rate': 9.117647058823529e-06, 'epoch': 14.73}\n",
      "{'loss': 0.127, 'learning_rate': 8.823529411764707e-06, 'epoch': 14.74}\n",
      "{'loss': 0.1535, 'learning_rate': 8.529411764705883e-06, 'epoch': 14.75}\n",
      "{'loss': 0.1302, 'learning_rate': 8.23529411764706e-06, 'epoch': 14.76}\n",
      "{'loss': 0.1298, 'learning_rate': 7.941176470588235e-06, 'epoch': 14.77}\n",
      "{'loss': 0.1259, 'learning_rate': 7.647058823529413e-06, 'epoch': 14.78}\n",
      "{'loss': 0.1514, 'learning_rate': 7.3529411764705884e-06, 'epoch': 14.79}\n",
      "{'loss': 0.167, 'learning_rate': 7.058823529411765e-06, 'epoch': 14.79}\n",
      "{'loss': 0.1446, 'learning_rate': 6.7647058823529414e-06, 'epoch': 14.8}\n",
      "{'loss': 0.1214, 'learning_rate': 6.470588235294119e-06, 'epoch': 14.81}\n",
      "{'loss': 0.1193, 'learning_rate': 6.1764705882352944e-06, 'epoch': 14.82}\n",
      "{'loss': 0.1446, 'learning_rate': 5.882352941176471e-06, 'epoch': 14.83}\n",
      "{'loss': 0.1434, 'learning_rate': 5.588235294117647e-06, 'epoch': 14.84}\n",
      "{'loss': 0.1445, 'learning_rate': 5.294117647058824e-06, 'epoch': 14.85}\n",
      "{'loss': 0.1128, 'learning_rate': 5e-06, 'epoch': 14.86}\n",
      "{'loss': 0.1354, 'learning_rate': 4.705882352941177e-06, 'epoch': 14.87}\n",
      "{'loss': 0.1556, 'learning_rate': 4.411764705882353e-06, 'epoch': 14.88}\n",
      "{'loss': 0.1776, 'learning_rate': 4.11764705882353e-06, 'epoch': 14.88}\n",
      "{'loss': 0.1266, 'learning_rate': 3.823529411764706e-06, 'epoch': 14.89}\n",
      "{'loss': 0.1166, 'learning_rate': 3.5294117647058825e-06, 'epoch': 14.9}\n",
      "{'loss': 0.1288, 'learning_rate': 3.2352941176470594e-06, 'epoch': 14.91}\n",
      "{'loss': 0.1607, 'learning_rate': 2.9411764705882355e-06, 'epoch': 14.92}\n",
      "{'loss': 0.1648, 'learning_rate': 2.647058823529412e-06, 'epoch': 14.93}\n",
      "{'loss': 0.1026, 'learning_rate': 2.3529411764705885e-06, 'epoch': 14.94}\n",
      "{'loss': 0.1164, 'learning_rate': 2.058823529411765e-06, 'epoch': 14.95}\n",
      "{'loss': 0.1184, 'learning_rate': 1.7647058823529412e-06, 'epoch': 14.96}\n",
      "{'loss': 0.1126, 'learning_rate': 1.4705882352941177e-06, 'epoch': 14.96}\n",
      "{'loss': 0.1904, 'learning_rate': 1.1764705882352942e-06, 'epoch': 14.97}\n",
      "{'loss': 0.1763, 'learning_rate': 8.823529411764706e-07, 'epoch': 14.98}\n",
      "{'loss': 0.1422, 'learning_rate': 5.882352941176471e-07, 'epoch': 14.99}\n",
      "{'loss': 0.1464, 'learning_rate': 2.9411764705882356e-07, 'epoch': 15.0}\n",
      "{'train_runtime': 34418.62, 'train_samples_per_second': 3.123, 'train_steps_per_second': 0.049, 'train_loss': 0.7817108644749082, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1680, training_loss=0.7817108644749082, metrics={'train_runtime': 34418.62, 'train_samples_per_second': 3.123, 'train_steps_per_second': 0.049, 'train_loss': 0.7817108644749082, 'epoch': 15.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=8, \n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=1000, \n",
    "        num_train_epochs=15,\n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        report_to=\"wandb\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir='GPT_w_ner'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb63167b383249bb8aa005b970c85c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.021 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.070750"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_loss</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>1680</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1464</td></tr><tr><td>train/total_flos</td><td>2.0071882986356736e+17</td></tr><tr><td>train/train_loss</td><td>0.78171</td></tr><tr><td>train/train_runtime</td><td>34418.62</td></tr><tr><td>train/train_samples_per_second</td><td>3.123</td></tr><tr><td>train/train_steps_per_second</td><td>0.049</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BioGPT_w_ner_epoch_5_balanced_train_data_no_[]</strong> at: <a href='https://wandb.ai/tian1995/GPT2/runs/1odgi893' target=\"_blank\">https://wandb.ai/tian1995/GPT2/runs/1odgi893</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230711_190351-1odgi893/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "trainer.save_model(\"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced.peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are key-unmatches in the trainer.save_model(), we need to rename the keys and load the paras in the model\n",
    "\n",
    "embed_tokens_state_dict = torch.load(\"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced/pytorch_model.bin\")\n",
    "\n",
    "old_keys = [\"base_model.model.biogpt.embed_tokens.0.weight\", \"base_model.model.output_projection.0.weight\"]\n",
    "new_keys = [\"base_model.model.biogpt.embed_tokens.weight\", \"base_model.model.output_projection.weight\"]\n",
    "\n",
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    # Get the value of the old key\n",
    "    value = embed_tokens_state_dict[old_key]\n",
    "\n",
    "    # Create a new key-value pair with the updated name\n",
    "    embed_tokens_state_dict[new_key] = value\n",
    "\n",
    "    # Delete the old key if desired\n",
    "    del embed_tokens_state_dict[old_key]\n",
    "\n",
    "torch.save(embed_tokens_state_dict, \"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced/pytorch_model-af.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BioGptForCausalLM(\n",
       "      (biogpt): BioGptModel(\n",
       "        (embed_tokens): CastOutputToFloat(\n",
       "          (0): Embedding(42390, 1024)\n",
       "        )\n",
       "        (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x BioGptDecoderLayer(\n",
       "            (self_attn): BioGptAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output_projection): CastOutputToFloat(\n",
       "        (0): Linear(in_features=1024, out_features=42390, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "\n",
    "peft_model_id = \"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced_train_data.peft\"\n",
    "# config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model_p = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GPT_w_ner/GPT_w_ner_tokenizer\")\n",
    "\n",
    "# resize the token embeddings to match the tokenizer\n",
    "model_p.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load the Lora model\n",
    "# the resized embedding layer are still uncorrected, need to load the weights manually\n",
    "model_p = PeftModel.from_pretrained(model_p, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BioGptForCausalLM(\n",
       "      (biogpt): BioGptModel(\n",
       "        (embed_tokens): Embedding(42390, 1024)\n",
       "        (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x BioGptDecoderLayer(\n",
       "            (self_attn): BioGptAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output_projection): Linear(in_features=1024, out_features=42390, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p.load_state_dict(torch.load(\"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced/pytorch_model-af.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text: @ HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label: the new drugs are more complex and variable than ever\n"
     ]
    }
   ],
   "source": [
    "model_p.eval()\n",
    "model_p.to(\"cpu\")\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_p.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from data_preprocessing import make_GPT_re_data, GPT_w_ner_preprocess_function\n",
    "additional_tokens, _, _, _ = get_labels(mode='GPT_w_ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 line:\n",
      " []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49396200098a43478e882581afb8b6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load test data and preprocess\n",
    "test_file_path = 'data/BioRED/processed/test.tsv'\n",
    "test_data = make_GPT_re_data(file_path=test_file_path, lower=True)\n",
    "\n",
    "test_dataset_raw = Dataset.from_dict(test_data)\n",
    "# test_dataset = test_dataset_raw.map(NER_preprocess_function, batched=False)\n",
    "# with bert only:\n",
    "test_dataset = test_dataset_raw.map(lambda example: GPT_w_ner_preprocess_function(example, tokenizer, mode=\"gpt_w_ner\", infer=True), batched=True, remove_columns=['pmids', 'text', 'entities', 'outputs'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids'])\n",
    "# the test_dataset has two columns: input_ids and labels, split the labels coloumn into test_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad1dcfd5bc647c2ac753825ed3a3f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = []\n",
    "model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "    # for i in range(1):\n",
    "        output = model.generate(input_ids=test_dataset[i][\"input_ids\"].unsqueeze(0).to(\"cuda\"), max_new_tokens=50)\n",
    "        output_text = tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0]\n",
    "        outputs.append(output_text.split(\"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \")[1])\n",
    "\n",
    "    # print(tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Association] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Positive_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Negative_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Positive_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Positive_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Association] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Association] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Negative_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Association] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Negative_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['labels'][30:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {f\"[{v}]\": 0 for v in relations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lines in train_dataset['labels']:\n",
    "    relation_dict[lines.split(\" \")[-3]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[None]': 18720,\n",
       " '[Association]': 2183,\n",
       " '[Bind]': 60,\n",
       " '[Comparison]': 28,\n",
       " '[Conversion]': 3,\n",
       " '[Cotreatment]': 31,\n",
       " '[Drug_Interaction]': 11,\n",
       " '[Negative_Correlation]': 763,\n",
       " '[Positive_Correlation]': 1088}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"output\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for output, label in zip(outputs, test_dataset['labels']):\n",
    "    result['output'].append(output)\n",
    "    result['label'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result dictionary\n",
    "import pickle\n",
    "with open(\"GPT_w_ner/result/GPT_w_ner_epoch_15_balanced_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post-processing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result dictionary\n",
    "import pickle\n",
    "with open(\"GPT_w_ner/result/GPT_w_ner_epoch_15_balanced_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorrected = 0\n",
    "for i in range(len(result['output'])):\n",
    "    if result['output'][i][:-6] != result['label'][i][:-3]:\n",
    "        uncorrected += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7590"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(result['output'])):\n",
    "    if result['label'][i][-9:-3] != '[None]':\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5360, 5380):\n",
    "    print(result['output'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Cotreatment] . \n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [Cotreatment] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Cotreatment] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n"
     ]
    }
   ],
   "source": [
    "for i in range(5360, 5380):\n",
    "    print(result['label'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970    s [Positive_Correlation]\n",
      "971    s [Positive_Correlation]\n",
      "972    s [Positive_Correlation]\n",
      "973    s [Positive_Correlation]\n",
      "974    s [Positive_Correlation]\n",
      "975    s [Positive_Correlation]\n",
      "977    s [Positive_Correlation]\n",
      "978    s [Positive_Correlation]\n",
      "980    s [Positive_Correlation]\n",
      "981    s [Positive_Correlation]\n",
      "982    s [Positive_Correlation]\n",
      "983    s [Positive_Correlation]\n",
      "984    s [Positive_Correlation]\n",
      "985    s [Positive_Correlation]\n",
      "986    s [Positive_Correlation]\n",
      "989    s [Positive_Correlation]\n",
      "991    s [Positive_Correlation]\n",
      "992    s [Positive_Correlation]\n",
      "993    s [Positive_Correlation]\n",
      "996    s [Positive_Correlation]\n",
      "997    s [Positive_Correlation]\n",
      "998    s [Positive_Correlation]\n",
      "999    s [Positive_Correlation]\n",
      "1468    ntity2] is [Association]\n",
      "1483    ntity2] is [Association]\n",
      "1488    ntity2] is [Association]\n",
      "1490    ntity2] is [Association]\n",
      "1494    ntity2] is [Association]\n",
      "1502    ntity2] is [Association]\n",
      "1509    ntity2] is [Association]\n",
      "1539    ntity2] is [Association]\n",
      "1554    ntity2] is [Association]\n",
      "1557    ntity2] is [Association]\n",
      "1566    ntity2] is [Association]\n",
      "2793    ntity2] is [Association]\n",
      "4356    s [Positive_Correlation]\n",
      "4357    s [Positive_Correlation]\n",
      "4358    s [Positive_Correlation]\n",
      "4359    s [Positive_Correlation]\n",
      "4360    s [Positive_Correlation]\n",
      "4361    s [Positive_Correlation]\n",
      "4362    s [Positive_Correlation]\n",
      "4363    s [Positive_Correlation]\n",
      "4364    s [Positive_Correlation]\n",
      "4365    s [Positive_Correlation]\n",
      "4366    s [Positive_Correlation]\n",
      "4367    s [Positive_Correlation]\n",
      "4368    s [Positive_Correlation]\n",
      "4369    s [Positive_Correlation]\n",
      "4370    s [Positive_Correlation]\n",
      "4371    s [Positive_Correlation]\n",
      "4372    s [Positive_Correlation]\n",
      "4373    s [Positive_Correlation]\n",
      "4374    s [Positive_Correlation]\n",
      "4375    s [Positive_Correlation]\n",
      "4376    s [Positive_Correlation]\n",
      "4377    s [Positive_Correlation]\n",
      "4378    s [Positive_Correlation]\n",
      "4379    s [Positive_Correlation]\n",
      "4380    s [Positive_Correlation]\n",
      "4381    s [Positive_Correlation]\n",
      "4382    s [Positive_Correlation]\n",
      "4383    s [Positive_Correlation]\n",
      "4384    s [Positive_Correlation]\n",
      "4385    s [Positive_Correlation]\n",
      "4386    s [Positive_Correlation]\n",
      "4387    s [Positive_Correlation]\n",
      "4388    s [Positive_Correlation]\n",
      "4424    ntity2] is [Association]\n",
      "4426    ntity2] is [Association]\n",
      "4454    ntity2] is [Association]\n",
      "4459    ntity2] is [Association]\n",
      "4464    ntity2] is [Association]\n",
      "4545    ntity2] is [Association]\n",
      "4553    ntity2] is [Association]\n",
      "4555    ntity2] is [Association]\n",
      "4558    ntity2] is [Association]\n",
      "4561    ntity2] is [Association]\n",
      "4616    s [Positive_Correlation]\n",
      "4617    s [Positive_Correlation]\n",
      "4618    s [Positive_Correlation]\n",
      "4619    s [Positive_Correlation]\n",
      "4620    s [Positive_Correlation]\n",
      "4621    s [Positive_Correlation]\n",
      "4622    s [Positive_Correlation]\n",
      "4623    s [Positive_Correlation]\n",
      "4624    s [Positive_Correlation]\n",
      "4896    ntity2] is [Association]\n",
      "4897    ntity2] is [Association]\n",
      "4898    ntity2] is [Association]\n",
      "4899    ntity2] is [Association]\n",
      "4900    ntity2] is [Association]\n",
      "4903    ntity2] is [Association]\n",
      "4905    ntity2] is [Association]\n",
      "4906    ntity2] is [Association]\n",
      "4907    ntity2] is [Association]\n",
      "5363    s [Positive_Correlation]\n",
      "5364    s [Positive_Correlation]\n",
      "5365    s [Positive_Correlation]\n",
      "5366    s [Positive_Correlation]\n",
      "5367    s [Positive_Correlation]\n",
      "5368    s [Positive_Correlation]\n",
      "5370    s [Positive_Correlation]\n",
      "5372    s [Positive_Correlation]\n",
      "5374    s [Positive_Correlation]\n",
      "5375    s [Positive_Correlation]\n",
      "5377    s [Positive_Correlation]\n",
      "5378    s [Positive_Correlation]\n",
      "5379    s [Positive_Correlation]\n",
      "5380    s [Positive_Correlation]\n",
      "5381    s [Positive_Correlation]\n",
      "5382    s [Positive_Correlation]\n",
      "5383    s [Positive_Correlation]\n",
      "5384    s [Positive_Correlation]\n",
      "5385    s [Positive_Correlation]\n",
      "6485    s [Positive_Correlation]\n",
      "6497    s [Positive_Correlation]\n",
      "6517    s [Positive_Correlation]\n",
      "6524    s [Positive_Correlation]\n",
      "6528    s [Positive_Correlation]\n",
      "6532    s [Positive_Correlation]\n",
      "6983    s [Positive_Correlation]\n",
      "6984    s [Positive_Correlation]\n",
      "6985    s [Positive_Correlation]\n",
      "6986    s [Positive_Correlation]\n",
      "7256    ntity2] is [Association]\n",
      "7283    ntity2] is [Association]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(result['output'])):\n",
    "    if result['output'][i][-12:-6] != '[None]':\n",
    "        print(i,\"  \", result['output'][i][-30:-6])\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
