{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing#scrollTo=T-gy-LxM0yAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from labels import get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_special_tokens': ['[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]']} \n",
      " {'additional_special_tokens': ['[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]']}\n"
     ]
    }
   ],
   "source": [
    "# load labels for bert_w_ner\n",
    "additional_tokens, _, _, _ = get_labels(mode='GPT_w_ner')\n",
    "print(additional_tokens, \"\\n\", additional_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model in 8-bit quantization configuration\n",
    "# the max length of the input is 1024\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, \n",
    "    # load_in_8bit=True, \n",
    "    device_map={'':torch.cuda.current_device()},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 346763264 || all params: 346763264 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 6 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('GPT_without_ner/GPT_w_ner_tokenizer/tokenizer_config.json',\n",
       " 'GPT_without_ner/GPT_w_ner_tokenizer/special_tokens_map.json',\n",
       " 'GPT_without_ner/GPT_w_ner_tokenizer/vocab.json',\n",
       " 'GPT_without_ner/GPT_w_ner_tokenizer/merges.txt',\n",
       " 'GPT_without_ner/GPT_w_ner_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding new tokens to the tokenizer\n",
    "# since I haven't load the model so I will resize the embedding of the model later]\n",
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(\"GPT_without_ner/GPT_w_ner_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(42390, 1024)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in float32 for stability.\n",
    "\n",
    "We also cast the output of the last layer and embedding layer in float32 for the same reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.biogpt.embed_tokens = CastOutputToFloat(model.biogpt.embed_tokens)\n",
    "model.output_projection = CastOutputToFloat(model.output_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 348342272 || trainable%: 0.4515283175278825\n"
     ]
    }
   ],
   "source": [
    "# more with LoRAconfig: https://huggingface.co/docs/peft/conceptual_guides/lora\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    r=16,\n",
    "    # alpha: LoRA scaling factor.\n",
    "    lora_alpha=32, \n",
    "    # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    fan_in_fan_out=True,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 44980224 || all params: 348342272 || trainable%: 12.912651611803232\n"
     ]
    }
   ],
   "source": [
    "# make model's embed_tokens layer also trainable\n",
    "\n",
    "model.biogpt.embed_tokens[0].weight.requires_grad = True\n",
    "model.output_projection[0].weight.requires_grad = True\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BioGptForCausalLM(\n",
       "      (biogpt): BioGptModel(\n",
       "        (embed_tokens): CastOutputToFloat(\n",
       "          (0): Embedding(42390, 1024)\n",
       "        )\n",
       "        (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x BioGptDecoderLayer(\n",
       "            (self_attn): BioGptAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output_projection): CastOutputToFloat(\n",
       "        (0): Linear(in_features=1024, out_features=42390, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.biogpt.embed_tokens.0.weight torch.Size([42390, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# for model, print the layer's name if the layer is trainable, and print the precision of the layer\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape, param.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import make_GPT_re_data_no_ner, GPT_no_ner_preprocess_function\n",
    "\n",
    "# from data_preprocessing import all_line_of_pmid, get_original_text, get_identifier_and_entity, reorder_list, get_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and valid file paths\n",
    "train_file_path = 'data/BioRED/processed/train.tsv'\n",
    "valid_file_path = 'data/BioRED/processed/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 10 line:\n",
      " [4548, 6646, 6758, 6776, 6866, 10222, 11775, 14657, 18818, 21689]\n",
      "Dropped 9 line:\n",
      " [467, 941, 2220, 2233, 2261, 5335, 5337, 5378, 5490]\n"
     ]
    }
   ],
   "source": [
    "# make bert_re data\n",
    "train_data_raw = make_GPT_re_data_no_ner(file_path=train_file_path, lower=True, random_seed=42)\n",
    "valid_data_raw = make_GPT_re_data_no_ner(file_path=valid_file_path, lower=True, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pmids', 'text', 'entities', 'outputs', 'relation'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the raw data\n",
    "# import json\n",
    "\n",
    "# with open('GPT_without_ner/data/train_data_dict.json', 'w') as f:\n",
    "#     json.dump(train_data_raw, f)\n",
    "\n",
    "# with open('GPT_without_ner/data/valid_data_dict.json', 'w') as f:\n",
    "#     json.dump(valid_data_raw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pmids', 'text', 'entities', 'outputs', 'relation'])\n",
      "pmids 1283\n",
      "text 1283\n",
      "entities 1283\n",
      "outputs 1283\n",
      "relation 1283\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('GPT_without_ner/data/train_data_dict.json', 'r') as f:\n",
    "    train_data_raw = json.load(f)\n",
    "\n",
    "with open('GPT_without_ner/data/valid_data_dict.json', 'r') as f:\n",
    "    valid_data_raw = json.load(f)\n",
    "\n",
    "print(train_data_raw.keys())\n",
    "for k, v in train_data_raw.items():\n",
    "    print(k, len(v))\n",
    "\n",
    "# make into Dataset type\n",
    "train_data_raw = Dataset.from_dict(train_data_raw)\n",
    "valid_data_raw = Dataset.from_dict(valid_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_data_raw,\n",
    "    \"valid\": valid_data_raw\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmids': '10491763',\n",
       " 'text': 'hepatocyte nuclear factor-6 : associations between genetic variability and type ii diabetes and between genetic variability and estimates of insulin secretion . the transcription factor hepatocyte nuclear factor (hnf)-6 is an upstream regulator of several genes involved in the pathogenesis of maturity-onset diabetes of the young . we therefore tested the hypothesis that variability in the hnf-6 gene is associated with subsets of type ii ( non-insulin-dependent ) diabetes mellitus and estimates of insulin secretion in glucose tolerant subjects . we cloned the coding region as well as the intron-exon boundaries of the hnf-6 gene . w e then examined them on genomic dna in six mody probands without mutations in the mody1 , mody3 and mody4 genes and in 54 patients with late-onset type ii diabetes by combined single strand conformational polymorphism-heteroduplex analysis followed by direct sequencing of identified variants . an identified missense variant was examined in association studies and genotype-phenotype studies . we identified two silent and one missense ( pro75 ala ) variant . i n an association study the allelic frequency of the pro75ala polymorphism was 3.2 % ( 95 % confidence interval , 1.9 - 4.5 ) in 330 patients with type ii diabetes mellitus compared with 4.2 % ( 2.4 - 6.0 ) in 238 age-matched glucose tolerant control subjects . moreover , in studies of 238 middle-aged glucose tolerant subjects , of 226 glucose tolerant offspring of type ii diabetic patients and of 367 young healthy subjects , the carriers of the polymorphism did not differ from non-carriers in glucose induced serum insulin or c-peptide responses . mutations in the coding region of the hnf-6 gene are not associated with type ii diabetes or with changes in insulin responses to glucose among the caucasians examined .',\n",
       " 'entities': [[['type ii diabetes',\n",
       "    'type ii diabetic',\n",
       "    'maturity-onset diabetes',\n",
       "    'type ii diabetes mellitus',\n",
       "    'type ii ( non-insulin-dependent ) diabetes mellitus'],\n",
       "   ['hnf-6',\n",
       "    'hepatocyte nuclear factor-6',\n",
       "    'hepatocyte nuclear factor (hnf)-6']],\n",
       "  [['glucose'],\n",
       "   ['type ii diabetes',\n",
       "    'type ii diabetic',\n",
       "    'maturity-onset diabetes',\n",
       "    'type ii diabetes mellitus',\n",
       "    'type ii ( non-insulin-dependent ) diabetes mellitus']]],\n",
       " 'outputs': 'the source is type ii diabetes and the target is hnf-6 ; the source is glucose and the target is type ii diabetes . ',\n",
       " 'relation': 'association'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00853878e5074aafbf759c5c21d8b0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truncated 4 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3c971418644981a534f72dc7301ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truncated 2 examples\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(lambda example: GPT_no_ner_preprocess_function(example, tokenizer), batched=True, remove_columns=['pmids', 'text', 'entities', 'outputs', 'relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ea81be234641698682f8dbb1b52986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd46a9a2962415a895d0ecb6157758e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets.save_to_disk('GPT_without_ner/data/tokenized_dataset_no_ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1283\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 333\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_datasets = load_from_disk('GPT_without_ner/data/tokenized_dataset_no_ner')\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hepatocyte nuclear factor-6: associations between genetic variability and type ii diabetes and between genetic variability and estimates of insulin secretion. the transcription factor hepatocyte nuclear factor (hnf) -6 is an upstream regulator of several genes involved in the pathogenesis of maturity-onset diabetes of the young. we therefore tested the hypothesis that variability in the hnf-6 gene is associated with subsets of type ii (non-insulin-dependent) diabetes mellitus and estimates of insulin secretion in glucose tolerant subjects. we cloned the coding region as well as the intron-exon boundaries of the hnf-6 gene. w e then examined them on genomic dna in six mody probands without mutations in the mody1, mody3 and mody4 genes and in 54 patients with late-onset type ii diabetes by combined single strand conformational polymorphism-heteroduplex analysis followed by direct sequencing of identified variants. an identified missense variant was examined in association studies and genotype-phenotype studies. we identified two silent and one missense (pro75 ala) variant. i n an association study the allelic frequency of the pro75ala polymorphism was 3.2% (95% confidence interval, 1.9 - 4.5) in 330 patients with type ii diabetes mellitus compared with 4.2% (2.4 - 6.0) in 238 age-matched glucose tolerant control subjects. moreover, in studies of 238 middle-aged glucose tolerant subjects, of 226 glucose tolerant offspring of type ii diabetic patients and of 367 young healthy subjects, the carriers of the polymorphism did not differ from non-carriers in glucose induced serum insulin or c-peptide responses. mutations in the coding region of the hnf-6 gene are not associated with type ii diabetes or with changes in insulin responses to glucose among the caucasians examined. for relation association, [learn1] [learn2] [learn3] [learn4] [learn5] [learn6] the source is type ii diabetes and the target is hnf-6; the source is glucose and the target is type ii diabetes. </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/BioRED/wandb/run-20230712_221400-d4spueww</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/GPT2/runs/d4spueww' target=\"_blank\">BioGPT_no_ner_epoch_15</a></strong> to <a href='https://wandb.ai/tian1995/GPT2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/GPT2' target=\"_blank\">https://wandb.ai/tian1995/GPT2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/GPT2/runs/d4spueww' target=\"_blank\">https://wandb.ai/tian1995/GPT2/runs/d4spueww</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/GPT2/runs/d4spueww?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2d559da980>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"GPT2\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"BioGPT_no_ner_epoch_15\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd83c1781e34fb2a95c613efb1079b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0152, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.05}\n",
      "{'loss': 3.1082, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.1}\n",
      "{'loss': 3.0445, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.15}\n",
      "{'loss': 3.0369, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.2}\n",
      "{'loss': 2.9729, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.25}\n",
      "{'loss': 2.9954, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.3}\n",
      "{'loss': 3.0043, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.35}\n",
      "{'loss': 2.9991, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 3.0874, 'learning_rate': 1.8e-06, 'epoch': 0.45}\n",
      "{'loss': 3.0998, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.5}\n",
      "{'loss': 3.0013, 'learning_rate': 2.2e-06, 'epoch': 0.55}\n",
      "{'loss': 3.0844, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.6}\n",
      "{'loss': 3.1047, 'learning_rate': 2.6e-06, 'epoch': 0.65}\n",
      "{'loss': 2.9493, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.7}\n",
      "{'loss': 2.9872, 'learning_rate': 3e-06, 'epoch': 0.75}\n",
      "{'loss': 2.8727, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.8}\n",
      "{'loss': 3.0481, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.84}\n",
      "{'loss': 3.0867, 'learning_rate': 3.6e-06, 'epoch': 0.89}\n",
      "{'loss': 3.0149, 'learning_rate': 3.8e-06, 'epoch': 0.94}\n",
      "{'loss': 3.0777, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.99}\n",
      "{'loss': 3.0472, 'learning_rate': 4.2000000000000004e-06, 'epoch': 1.04}\n",
      "{'loss': 3.0004, 'learning_rate': 4.4e-06, 'epoch': 1.09}\n",
      "{'loss': 2.9611, 'learning_rate': 4.6e-06, 'epoch': 1.14}\n",
      "{'loss': 3.062, 'learning_rate': 4.800000000000001e-06, 'epoch': 1.19}\n",
      "{'loss': 3.0445, 'learning_rate': 5e-06, 'epoch': 1.24}\n",
      "{'loss': 2.9501, 'learning_rate': 5.2e-06, 'epoch': 1.29}\n",
      "{'loss': 3.0828, 'learning_rate': 5.4e-06, 'epoch': 1.34}\n",
      "{'loss': 3.0121, 'learning_rate': 5.600000000000001e-06, 'epoch': 1.39}\n",
      "{'loss': 2.9298, 'learning_rate': 5.8e-06, 'epoch': 1.44}\n",
      "{'loss': 2.9605, 'learning_rate': 6e-06, 'epoch': 1.49}\n",
      "{'loss': 3.1179, 'learning_rate': 6.2e-06, 'epoch': 1.54}\n",
      "{'loss': 2.9481, 'learning_rate': 6.4000000000000006e-06, 'epoch': 1.59}\n",
      "{'loss': 2.9683, 'learning_rate': 6.6e-06, 'epoch': 1.64}\n",
      "{'loss': 3.025, 'learning_rate': 6.800000000000001e-06, 'epoch': 1.69}\n",
      "{'loss': 2.9489, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.74}\n",
      "{'loss': 3.0345, 'learning_rate': 7.2e-06, 'epoch': 1.79}\n",
      "{'loss': 3.0604, 'learning_rate': 7.4e-06, 'epoch': 1.84}\n",
      "{'loss': 3.0311, 'learning_rate': 7.6e-06, 'epoch': 1.89}\n",
      "{'loss': 3.0522, 'learning_rate': 7.8e-06, 'epoch': 1.94}\n",
      "{'loss': 3.0316, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.99}\n",
      "{'loss': 3.0172, 'learning_rate': 8.200000000000001e-06, 'epoch': 2.04}\n",
      "{'loss': 3.0847, 'learning_rate': 8.400000000000001e-06, 'epoch': 2.09}\n",
      "{'loss': 3.0223, 'learning_rate': 8.599999999999999e-06, 'epoch': 2.14}\n",
      "{'loss': 2.9118, 'learning_rate': 8.8e-06, 'epoch': 2.19}\n",
      "{'loss': 2.9483, 'learning_rate': 9e-06, 'epoch': 2.24}\n",
      "{'loss': 2.9887, 'learning_rate': 9.2e-06, 'epoch': 2.29}\n",
      "{'loss': 2.8873, 'learning_rate': 9.4e-06, 'epoch': 2.34}\n",
      "{'loss': 2.8679, 'learning_rate': 9.600000000000001e-06, 'epoch': 2.39}\n",
      "{'loss': 2.8601, 'learning_rate': 9.800000000000001e-06, 'epoch': 2.43}\n",
      "{'loss': 2.9501, 'learning_rate': 1e-05, 'epoch': 2.48}\n",
      "{'loss': 2.9764, 'learning_rate': 1.02e-05, 'epoch': 2.53}\n",
      "{'loss': 2.9444, 'learning_rate': 1.04e-05, 'epoch': 2.58}\n",
      "{'loss': 2.9877, 'learning_rate': 1.06e-05, 'epoch': 2.63}\n",
      "{'loss': 2.9286, 'learning_rate': 1.08e-05, 'epoch': 2.68}\n",
      "{'loss': 2.9879, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.73}\n",
      "{'loss': 3.0004, 'learning_rate': 1.1200000000000001e-05, 'epoch': 2.78}\n",
      "{'loss': 3.0483, 'learning_rate': 1.1400000000000001e-05, 'epoch': 2.83}\n",
      "{'loss': 3.0477, 'learning_rate': 1.16e-05, 'epoch': 2.88}\n",
      "{'loss': 2.9226, 'learning_rate': 1.18e-05, 'epoch': 2.93}\n",
      "{'loss': 2.9765, 'learning_rate': 1.2e-05, 'epoch': 2.98}\n",
      "{'loss': 2.9178, 'learning_rate': 1.22e-05, 'epoch': 3.03}\n",
      "{'loss': 2.9454, 'learning_rate': 1.24e-05, 'epoch': 3.08}\n",
      "{'loss': 2.9745, 'learning_rate': 1.2600000000000001e-05, 'epoch': 3.13}\n",
      "{'loss': 2.8926, 'learning_rate': 1.2800000000000001e-05, 'epoch': 3.18}\n",
      "{'loss': 2.8794, 'learning_rate': 1.3000000000000001e-05, 'epoch': 3.23}\n",
      "{'loss': 2.8552, 'learning_rate': 1.32e-05, 'epoch': 3.28}\n",
      "{'loss': 2.9687, 'learning_rate': 1.3400000000000002e-05, 'epoch': 3.33}\n",
      "{'loss': 2.8493, 'learning_rate': 1.3600000000000002e-05, 'epoch': 3.38}\n",
      "{'loss': 2.9092, 'learning_rate': 1.3800000000000002e-05, 'epoch': 3.43}\n",
      "{'loss': 2.9464, 'learning_rate': 1.4000000000000001e-05, 'epoch': 3.48}\n",
      "{'loss': 2.8241, 'learning_rate': 1.42e-05, 'epoch': 3.53}\n",
      "{'loss': 2.8891, 'learning_rate': 1.44e-05, 'epoch': 3.58}\n",
      "{'loss': 2.8111, 'learning_rate': 1.4599999999999999e-05, 'epoch': 3.63}\n",
      "{'loss': 2.9806, 'learning_rate': 1.48e-05, 'epoch': 3.68}\n",
      "{'loss': 2.8043, 'learning_rate': 1.5e-05, 'epoch': 3.73}\n",
      "{'loss': 2.9028, 'learning_rate': 1.52e-05, 'epoch': 3.78}\n",
      "{'loss': 2.8907, 'learning_rate': 1.54e-05, 'epoch': 3.83}\n",
      "{'loss': 2.8929, 'learning_rate': 1.56e-05, 'epoch': 3.88}\n",
      "{'loss': 2.9032, 'learning_rate': 1.58e-05, 'epoch': 3.93}\n",
      "{'loss': 2.8532, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.98}\n",
      "{'loss': 2.8233, 'learning_rate': 1.62e-05, 'epoch': 4.02}\n",
      "{'loss': 2.7788, 'learning_rate': 1.6400000000000002e-05, 'epoch': 4.07}\n",
      "{'loss': 2.8924, 'learning_rate': 1.66e-05, 'epoch': 4.12}\n",
      "{'loss': 2.927, 'learning_rate': 1.6800000000000002e-05, 'epoch': 4.17}\n",
      "{'loss': 2.8702, 'learning_rate': 1.7000000000000003e-05, 'epoch': 4.22}\n",
      "{'loss': 2.7515, 'learning_rate': 1.7199999999999998e-05, 'epoch': 4.27}\n",
      "{'loss': 2.8386, 'learning_rate': 1.74e-05, 'epoch': 4.32}\n",
      "{'loss': 2.8129, 'learning_rate': 1.76e-05, 'epoch': 4.37}\n",
      "{'loss': 2.8899, 'learning_rate': 1.78e-05, 'epoch': 4.42}\n",
      "{'loss': 2.7719, 'learning_rate': 1.8e-05, 'epoch': 4.47}\n",
      "{'loss': 2.793, 'learning_rate': 1.8200000000000002e-05, 'epoch': 4.52}\n",
      "{'loss': 2.6934, 'learning_rate': 1.84e-05, 'epoch': 4.57}\n",
      "{'loss': 2.847, 'learning_rate': 1.86e-05, 'epoch': 4.62}\n",
      "{'loss': 2.709, 'learning_rate': 1.88e-05, 'epoch': 4.67}\n",
      "{'loss': 2.7168, 'learning_rate': 1.9e-05, 'epoch': 4.72}\n",
      "{'loss': 2.7708, 'learning_rate': 1.9200000000000003e-05, 'epoch': 4.77}\n",
      "{'loss': 2.7566, 'learning_rate': 1.94e-05, 'epoch': 4.82}\n",
      "{'loss': 2.7733, 'learning_rate': 1.9600000000000002e-05, 'epoch': 4.87}\n",
      "{'loss': 2.8113, 'learning_rate': 1.9800000000000004e-05, 'epoch': 4.92}\n",
      "{'loss': 2.817, 'learning_rate': 2e-05, 'epoch': 4.97}\n",
      "{'loss': 2.7503, 'learning_rate': 2.0200000000000003e-05, 'epoch': 5.02}\n",
      "{'loss': 2.7825, 'learning_rate': 2.04e-05, 'epoch': 5.07}\n",
      "{'loss': 2.7401, 'learning_rate': 2.06e-05, 'epoch': 5.12}\n",
      "{'loss': 2.8142, 'learning_rate': 2.08e-05, 'epoch': 5.17}\n",
      "{'loss': 2.7821, 'learning_rate': 2.1e-05, 'epoch': 5.22}\n",
      "{'loss': 2.729, 'learning_rate': 2.12e-05, 'epoch': 5.27}\n",
      "{'loss': 2.742, 'learning_rate': 2.1400000000000002e-05, 'epoch': 5.32}\n",
      "{'loss': 2.6139, 'learning_rate': 2.16e-05, 'epoch': 5.37}\n",
      "{'loss': 2.6742, 'learning_rate': 2.18e-05, 'epoch': 5.42}\n",
      "{'loss': 2.7416, 'learning_rate': 2.2000000000000003e-05, 'epoch': 5.47}\n",
      "{'loss': 2.6788, 'learning_rate': 2.22e-05, 'epoch': 5.52}\n",
      "{'loss': 2.5792, 'learning_rate': 2.2400000000000002e-05, 'epoch': 5.57}\n",
      "{'loss': 2.6799, 'learning_rate': 2.26e-05, 'epoch': 5.61}\n",
      "{'loss': 2.6865, 'learning_rate': 2.2800000000000002e-05, 'epoch': 5.66}\n",
      "{'loss': 2.6026, 'learning_rate': 2.3000000000000003e-05, 'epoch': 5.71}\n",
      "{'loss': 2.6158, 'learning_rate': 2.32e-05, 'epoch': 5.76}\n",
      "{'loss': 2.6352, 'learning_rate': 2.3400000000000003e-05, 'epoch': 5.81}\n",
      "{'loss': 2.5614, 'learning_rate': 2.36e-05, 'epoch': 5.86}\n",
      "{'loss': 2.715, 'learning_rate': 2.38e-05, 'epoch': 5.91}\n",
      "{'loss': 2.6415, 'learning_rate': 2.4e-05, 'epoch': 5.96}\n",
      "{'loss': 2.6466, 'learning_rate': 2.4200000000000002e-05, 'epoch': 6.01}\n",
      "{'loss': 2.5574, 'learning_rate': 2.44e-05, 'epoch': 6.06}\n",
      "{'loss': 2.6057, 'learning_rate': 2.46e-05, 'epoch': 6.11}\n",
      "{'loss': 2.5708, 'learning_rate': 2.48e-05, 'epoch': 6.16}\n",
      "{'loss': 2.6142, 'learning_rate': 2.5e-05, 'epoch': 6.21}\n",
      "{'loss': 2.5331, 'learning_rate': 2.5200000000000003e-05, 'epoch': 6.26}\n",
      "{'loss': 2.5549, 'learning_rate': 2.54e-05, 'epoch': 6.31}\n",
      "{'loss': 2.5621, 'learning_rate': 2.5600000000000002e-05, 'epoch': 6.36}\n",
      "{'loss': 2.542, 'learning_rate': 2.58e-05, 'epoch': 6.41}\n",
      "{'loss': 2.5504, 'learning_rate': 2.6000000000000002e-05, 'epoch': 6.46}\n",
      "{'loss': 2.527, 'learning_rate': 2.6200000000000003e-05, 'epoch': 6.51}\n",
      "{'loss': 2.4341, 'learning_rate': 2.64e-05, 'epoch': 6.56}\n",
      "{'loss': 2.5538, 'learning_rate': 2.6600000000000003e-05, 'epoch': 6.61}\n",
      "{'loss': 2.5925, 'learning_rate': 2.6800000000000004e-05, 'epoch': 6.66}\n",
      "{'loss': 2.5848, 'learning_rate': 2.7000000000000002e-05, 'epoch': 6.71}\n",
      "{'loss': 2.5566, 'learning_rate': 2.7200000000000004e-05, 'epoch': 6.76}\n",
      "{'loss': 2.5644, 'learning_rate': 2.7400000000000002e-05, 'epoch': 6.81}\n",
      "{'loss': 2.478, 'learning_rate': 2.7600000000000003e-05, 'epoch': 6.86}\n",
      "{'loss': 2.543, 'learning_rate': 2.7800000000000005e-05, 'epoch': 6.91}\n",
      "{'loss': 2.4915, 'learning_rate': 2.8000000000000003e-05, 'epoch': 6.96}\n",
      "{'loss': 2.5513, 'learning_rate': 2.8199999999999998e-05, 'epoch': 7.01}\n",
      "{'loss': 2.4931, 'learning_rate': 2.84e-05, 'epoch': 7.06}\n",
      "{'loss': 2.5156, 'learning_rate': 2.86e-05, 'epoch': 7.11}\n",
      "{'loss': 2.4373, 'learning_rate': 2.88e-05, 'epoch': 7.16}\n",
      "{'loss': 2.3933, 'learning_rate': 2.9e-05, 'epoch': 7.2}\n",
      "{'loss': 2.4324, 'learning_rate': 2.9199999999999998e-05, 'epoch': 7.25}\n",
      "{'loss': 2.4397, 'learning_rate': 2.94e-05, 'epoch': 7.3}\n",
      "{'loss': 2.3283, 'learning_rate': 2.96e-05, 'epoch': 7.35}\n",
      "{'loss': 2.4575, 'learning_rate': 2.98e-05, 'epoch': 7.4}\n",
      "{'loss': 2.3957, 'learning_rate': 3e-05, 'epoch': 7.45}\n",
      "{'loss': 2.2903, 'learning_rate': 3.02e-05, 'epoch': 7.5}\n",
      "{'loss': 2.369, 'learning_rate': 3.04e-05, 'epoch': 7.55}\n",
      "{'loss': 2.3926, 'learning_rate': 3.06e-05, 'epoch': 7.6}\n",
      "{'loss': 2.3086, 'learning_rate': 3.08e-05, 'epoch': 7.65}\n",
      "{'loss': 2.2889, 'learning_rate': 3.1e-05, 'epoch': 7.7}\n",
      "{'loss': 2.3443, 'learning_rate': 3.12e-05, 'epoch': 7.75}\n",
      "{'loss': 2.3132, 'learning_rate': 3.1400000000000004e-05, 'epoch': 7.8}\n",
      "{'loss': 2.3495, 'learning_rate': 3.16e-05, 'epoch': 7.85}\n",
      "{'loss': 2.2292, 'learning_rate': 3.18e-05, 'epoch': 7.9}\n",
      "{'loss': 2.3154, 'learning_rate': 3.2000000000000005e-05, 'epoch': 7.95}\n",
      "{'loss': 2.3345, 'learning_rate': 3.2200000000000003e-05, 'epoch': 8.0}\n",
      "{'loss': 2.1695, 'learning_rate': 3.24e-05, 'epoch': 8.05}\n",
      "{'loss': 2.3245, 'learning_rate': 3.26e-05, 'epoch': 8.1}\n",
      "{'loss': 2.2236, 'learning_rate': 3.2800000000000004e-05, 'epoch': 8.15}\n",
      "{'loss': 2.2097, 'learning_rate': 3.3e-05, 'epoch': 8.2}\n",
      "{'loss': 2.1492, 'learning_rate': 3.32e-05, 'epoch': 8.25}\n",
      "{'loss': 2.1109, 'learning_rate': 3.3400000000000005e-05, 'epoch': 8.3}\n",
      "{'loss': 2.1936, 'learning_rate': 3.3600000000000004e-05, 'epoch': 8.35}\n",
      "{'loss': 2.2416, 'learning_rate': 3.38e-05, 'epoch': 8.4}\n",
      "{'loss': 2.2235, 'learning_rate': 3.4000000000000007e-05, 'epoch': 8.45}\n",
      "{'loss': 2.2181, 'learning_rate': 3.4200000000000005e-05, 'epoch': 8.5}\n",
      "{'loss': 2.1385, 'learning_rate': 3.4399999999999996e-05, 'epoch': 8.55}\n",
      "{'loss': 2.1948, 'learning_rate': 3.46e-05, 'epoch': 8.6}\n",
      "{'loss': 2.1899, 'learning_rate': 3.48e-05, 'epoch': 8.65}\n",
      "{'loss': 2.2003, 'learning_rate': 3.5e-05, 'epoch': 8.7}\n",
      "{'loss': 2.1988, 'learning_rate': 3.52e-05, 'epoch': 8.75}\n",
      "{'loss': 2.1892, 'learning_rate': 3.54e-05, 'epoch': 8.8}\n",
      "{'loss': 2.2164, 'learning_rate': 3.56e-05, 'epoch': 8.84}\n",
      "{'loss': 2.111, 'learning_rate': 3.58e-05, 'epoch': 8.89}\n",
      "{'loss': 2.1357, 'learning_rate': 3.6e-05, 'epoch': 8.94}\n",
      "{'loss': 2.1236, 'learning_rate': 3.62e-05, 'epoch': 8.99}\n",
      "{'loss': 2.0535, 'learning_rate': 3.6400000000000004e-05, 'epoch': 9.04}\n",
      "{'loss': 2.1058, 'learning_rate': 3.66e-05, 'epoch': 9.09}\n",
      "{'loss': 2.0302, 'learning_rate': 3.68e-05, 'epoch': 9.14}\n",
      "{'loss': 2.0814, 'learning_rate': 3.7e-05, 'epoch': 9.19}\n",
      "{'loss': 2.1235, 'learning_rate': 3.72e-05, 'epoch': 9.24}\n",
      "{'loss': 2.0325, 'learning_rate': 3.74e-05, 'epoch': 9.29}\n",
      "{'loss': 2.0722, 'learning_rate': 3.76e-05, 'epoch': 9.34}\n",
      "{'loss': 2.0216, 'learning_rate': 3.7800000000000004e-05, 'epoch': 9.39}\n",
      "{'loss': 2.1239, 'learning_rate': 3.8e-05, 'epoch': 9.44}\n",
      "{'loss': 2.07, 'learning_rate': 3.82e-05, 'epoch': 9.49}\n",
      "{'loss': 2.1493, 'learning_rate': 3.8400000000000005e-05, 'epoch': 9.54}\n",
      "{'loss': 2.0714, 'learning_rate': 3.86e-05, 'epoch': 9.59}\n",
      "{'loss': 2.016, 'learning_rate': 3.88e-05, 'epoch': 9.64}\n",
      "{'loss': 2.0151, 'learning_rate': 3.9000000000000006e-05, 'epoch': 9.69}\n",
      "{'loss': 2.0779, 'learning_rate': 3.9200000000000004e-05, 'epoch': 9.74}\n",
      "{'loss': 2.0463, 'learning_rate': 3.94e-05, 'epoch': 9.79}\n",
      "{'loss': 1.9681, 'learning_rate': 3.960000000000001e-05, 'epoch': 9.84}\n",
      "{'loss': 1.9936, 'learning_rate': 3.9800000000000005e-05, 'epoch': 9.89}\n",
      "{'loss': 2.0302, 'learning_rate': 4e-05, 'epoch': 9.94}\n",
      "{'loss': 2.0826, 'learning_rate': 4.02e-05, 'epoch': 9.99}\n",
      "{'loss': 1.9432, 'learning_rate': 4.0400000000000006e-05, 'epoch': 10.04}\n",
      "{'loss': 1.9696, 'learning_rate': 4.0600000000000004e-05, 'epoch': 10.09}\n",
      "{'loss': 1.985, 'learning_rate': 4.08e-05, 'epoch': 10.14}\n",
      "{'loss': 2.0122, 'learning_rate': 4.1e-05, 'epoch': 10.19}\n",
      "{'loss': 1.8854, 'learning_rate': 4.12e-05, 'epoch': 10.24}\n",
      "{'loss': 1.9646, 'learning_rate': 4.14e-05, 'epoch': 10.29}\n",
      "{'loss': 1.937, 'learning_rate': 4.16e-05, 'epoch': 10.34}\n",
      "{'loss': 1.9834, 'learning_rate': 4.18e-05, 'epoch': 10.39}\n",
      "{'loss': 1.9707, 'learning_rate': 4.2e-05, 'epoch': 10.43}\n",
      "{'loss': 1.9389, 'learning_rate': 4.22e-05, 'epoch': 10.48}\n",
      "{'loss': 1.9269, 'learning_rate': 4.24e-05, 'epoch': 10.53}\n",
      "{'loss': 1.9949, 'learning_rate': 4.26e-05, 'epoch': 10.58}\n",
      "{'loss': 1.969, 'learning_rate': 4.2800000000000004e-05, 'epoch': 10.63}\n",
      "{'loss': 1.8877, 'learning_rate': 4.3e-05, 'epoch': 10.68}\n",
      "{'loss': 1.8714, 'learning_rate': 4.32e-05, 'epoch': 10.73}\n",
      "{'loss': 1.8595, 'learning_rate': 4.3400000000000005e-05, 'epoch': 10.78}\n",
      "{'loss': 1.8127, 'learning_rate': 4.36e-05, 'epoch': 10.83}\n",
      "{'loss': 2.016, 'learning_rate': 4.38e-05, 'epoch': 10.88}\n",
      "{'loss': 1.9413, 'learning_rate': 4.4000000000000006e-05, 'epoch': 10.93}\n",
      "{'loss': 1.8989, 'learning_rate': 4.4200000000000004e-05, 'epoch': 10.98}\n",
      "{'loss': 1.8955, 'learning_rate': 4.44e-05, 'epoch': 11.03}\n",
      "{'loss': 1.7913, 'learning_rate': 4.46e-05, 'epoch': 11.08}\n",
      "{'loss': 1.8694, 'learning_rate': 4.4800000000000005e-05, 'epoch': 11.13}\n",
      "{'loss': 1.8974, 'learning_rate': 4.5e-05, 'epoch': 11.18}\n",
      "{'loss': 1.914, 'learning_rate': 4.52e-05, 'epoch': 11.23}\n",
      "{'loss': 1.7586, 'learning_rate': 4.5400000000000006e-05, 'epoch': 11.28}\n",
      "{'loss': 1.8132, 'learning_rate': 4.5600000000000004e-05, 'epoch': 11.33}\n",
      "{'loss': 1.8285, 'learning_rate': 4.58e-05, 'epoch': 11.38}\n",
      "{'loss': 1.888, 'learning_rate': 4.600000000000001e-05, 'epoch': 11.43}\n",
      "{'loss': 1.8297, 'learning_rate': 4.6200000000000005e-05, 'epoch': 11.48}\n",
      "{'loss': 1.8331, 'learning_rate': 4.64e-05, 'epoch': 11.53}\n",
      "{'loss': 1.7783, 'learning_rate': 4.660000000000001e-05, 'epoch': 11.58}\n",
      "{'loss': 1.8612, 'learning_rate': 4.6800000000000006e-05, 'epoch': 11.63}\n",
      "{'loss': 1.8295, 'learning_rate': 4.7e-05, 'epoch': 11.68}\n",
      "{'loss': 1.7635, 'learning_rate': 4.72e-05, 'epoch': 11.73}\n",
      "{'loss': 1.8612, 'learning_rate': 4.74e-05, 'epoch': 11.78}\n",
      "{'loss': 1.7678, 'learning_rate': 4.76e-05, 'epoch': 11.83}\n",
      "{'loss': 1.8822, 'learning_rate': 4.78e-05, 'epoch': 11.88}\n",
      "{'loss': 1.8431, 'learning_rate': 4.8e-05, 'epoch': 11.93}\n",
      "{'loss': 1.7581, 'learning_rate': 4.82e-05, 'epoch': 11.98}\n",
      "{'loss': 1.7714, 'learning_rate': 4.8400000000000004e-05, 'epoch': 12.02}\n",
      "{'loss': 1.8018, 'learning_rate': 4.86e-05, 'epoch': 12.07}\n",
      "{'loss': 1.8365, 'learning_rate': 4.88e-05, 'epoch': 12.12}\n",
      "{'loss': 1.7717, 'learning_rate': 4.9e-05, 'epoch': 12.17}\n",
      "{'loss': 1.7764, 'learning_rate': 4.92e-05, 'epoch': 12.22}\n",
      "{'loss': 1.7431, 'learning_rate': 4.94e-05, 'epoch': 12.27}\n",
      "{'loss': 1.7124, 'learning_rate': 4.96e-05, 'epoch': 12.32}\n",
      "{'loss': 1.851, 'learning_rate': 4.9800000000000004e-05, 'epoch': 12.37}\n",
      "{'loss': 1.7015, 'learning_rate': 5e-05, 'epoch': 12.42}\n",
      "{'loss': 1.7357, 'learning_rate': 5.02e-05, 'epoch': 12.47}\n",
      "{'loss': 1.85, 'learning_rate': 5.0400000000000005e-05, 'epoch': 12.52}\n",
      "{'loss': 1.751, 'learning_rate': 5.0600000000000003e-05, 'epoch': 12.57}\n",
      "{'loss': 1.6961, 'learning_rate': 5.08e-05, 'epoch': 12.62}\n",
      "{'loss': 1.7257, 'learning_rate': 5.1000000000000006e-05, 'epoch': 12.67}\n",
      "{'loss': 1.7506, 'learning_rate': 5.1200000000000004e-05, 'epoch': 12.72}\n",
      "{'loss': 1.6711, 'learning_rate': 5.14e-05, 'epoch': 12.77}\n",
      "{'loss': 1.6909, 'learning_rate': 5.16e-05, 'epoch': 12.82}\n",
      "{'loss': 1.6953, 'learning_rate': 5.1800000000000005e-05, 'epoch': 12.87}\n",
      "{'loss': 1.7273, 'learning_rate': 5.2000000000000004e-05, 'epoch': 12.92}\n",
      "{'loss': 1.6904, 'learning_rate': 5.22e-05, 'epoch': 12.97}\n",
      "{'loss': 1.7671, 'learning_rate': 5.2400000000000007e-05, 'epoch': 13.02}\n",
      "{'loss': 1.6681, 'learning_rate': 5.2600000000000005e-05, 'epoch': 13.07}\n",
      "{'loss': 1.7076, 'learning_rate': 5.28e-05, 'epoch': 13.12}\n",
      "{'loss': 1.6828, 'learning_rate': 5.300000000000001e-05, 'epoch': 13.17}\n",
      "{'loss': 1.7252, 'learning_rate': 5.3200000000000006e-05, 'epoch': 13.22}\n",
      "{'loss': 1.6675, 'learning_rate': 5.3400000000000004e-05, 'epoch': 13.27}\n",
      "{'loss': 1.6768, 'learning_rate': 5.360000000000001e-05, 'epoch': 13.32}\n",
      "{'loss': 1.6453, 'learning_rate': 5.380000000000001e-05, 'epoch': 13.37}\n",
      "{'loss': 1.6667, 'learning_rate': 5.4000000000000005e-05, 'epoch': 13.42}\n",
      "{'loss': 1.7588, 'learning_rate': 5.420000000000001e-05, 'epoch': 13.47}\n",
      "{'loss': 1.636, 'learning_rate': 5.440000000000001e-05, 'epoch': 13.52}\n",
      "{'loss': 1.7391, 'learning_rate': 5.4600000000000006e-05, 'epoch': 13.57}\n",
      "{'loss': 1.6544, 'learning_rate': 5.4800000000000004e-05, 'epoch': 13.61}\n",
      "{'loss': 1.6703, 'learning_rate': 5.500000000000001e-05, 'epoch': 13.66}\n",
      "{'loss': 1.6403, 'learning_rate': 5.520000000000001e-05, 'epoch': 13.71}\n",
      "{'loss': 1.6688, 'learning_rate': 5.5400000000000005e-05, 'epoch': 13.76}\n",
      "{'loss': 1.653, 'learning_rate': 5.560000000000001e-05, 'epoch': 13.81}\n",
      "{'loss': 1.7178, 'learning_rate': 5.580000000000001e-05, 'epoch': 13.86}\n",
      "{'loss': 1.6497, 'learning_rate': 5.6000000000000006e-05, 'epoch': 13.91}\n",
      "{'loss': 1.6917, 'learning_rate': 5.620000000000001e-05, 'epoch': 13.96}\n",
      "{'loss': 1.6381, 'learning_rate': 5.6399999999999995e-05, 'epoch': 14.01}\n",
      "{'loss': 1.6741, 'learning_rate': 5.66e-05, 'epoch': 14.06}\n",
      "{'loss': 1.5774, 'learning_rate': 5.68e-05, 'epoch': 14.11}\n",
      "{'loss': 1.6525, 'learning_rate': 5.6999999999999996e-05, 'epoch': 14.16}\n",
      "{'loss': 1.6202, 'learning_rate': 5.72e-05, 'epoch': 14.21}\n",
      "{'loss': 1.6102, 'learning_rate': 5.74e-05, 'epoch': 14.26}\n",
      "{'loss': 1.6185, 'learning_rate': 5.76e-05, 'epoch': 14.31}\n",
      "{'loss': 1.5939, 'learning_rate': 5.7799999999999995e-05, 'epoch': 14.36}\n",
      "{'loss': 1.6416, 'learning_rate': 5.8e-05, 'epoch': 14.41}\n",
      "{'loss': 1.6114, 'learning_rate': 5.82e-05, 'epoch': 14.46}\n",
      "{'loss': 1.6611, 'learning_rate': 5.8399999999999997e-05, 'epoch': 14.51}\n",
      "{'loss': 1.6475, 'learning_rate': 5.86e-05, 'epoch': 14.56}\n",
      "{'loss': 1.6103, 'learning_rate': 5.88e-05, 'epoch': 14.61}\n",
      "{'loss': 1.5752, 'learning_rate': 5.9e-05, 'epoch': 14.66}\n",
      "{'loss': 1.6716, 'learning_rate': 5.92e-05, 'epoch': 14.71}\n",
      "{'loss': 1.5837, 'learning_rate': 5.94e-05, 'epoch': 14.76}\n",
      "{'loss': 1.5501, 'learning_rate': 5.96e-05, 'epoch': 14.81}\n",
      "{'loss': 1.6701, 'learning_rate': 5.9800000000000003e-05, 'epoch': 14.86}\n",
      "{'loss': 1.6486, 'learning_rate': 6e-05, 'epoch': 14.91}\n",
      "{'train_runtime': 6143.1653, 'train_samples_per_second': 3.133, 'train_steps_per_second': 0.049, 'train_loss': 2.3627151997884113, 'epoch': 14.91}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=2.3627151997884113, metrics={'train_runtime': 6143.1653, 'train_samples_per_second': 3.133, 'train_steps_per_second': 0.049, 'train_loss': 2.3627151997884113, 'epoch': 14.91})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=8, \n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=1000, \n",
    "        num_train_epochs=15,\n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        report_to=\"wandb\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir='GPT_without_ner'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>██▇█▇█▇█▇▇▇▇▆▇▆▆▆▅▅▅▅▄▄▄▃▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>14.91</td></tr><tr><td>train/global_step</td><td>300</td></tr><tr><td>train/learning_rate</td><td>6e-05</td></tr><tr><td>train/loss</td><td>1.6486</td></tr><tr><td>train/total_flos</td><td>3.571695470247936e+16</td></tr><tr><td>train/train_loss</td><td>2.36272</td></tr><tr><td>train/train_runtime</td><td>6143.1653</td></tr><tr><td>train/train_samples_per_second</td><td>3.133</td></tr><tr><td>train/train_steps_per_second</td><td>0.049</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BioGPT_no_ner_epoch_15</strong> at: <a href='https://wandb.ai/tian1995/GPT2/runs/d4spueww' target=\"_blank\">https://wandb.ai/tian1995/GPT2/runs/d4spueww</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230712_221400-d4spueww/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "trainer.save_model(\"GPT_without_ner/models/GPT_no_ner_epoch_15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"GPT_without_ner/models/GPT_no_ner_epoch_15.peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are key-unmatches in the trainer.save_model(), we need to rename the keys and load the paras in the model\n",
    "\n",
    "embed_tokens_state_dict = torch.load(\"GPT_without_ner/models/GPT_no_ner_epoch_15/pytorch_model.bin\")\n",
    "\n",
    "old_keys = [\"base_model.model.biogpt.embed_tokens.0.weight\", \"base_model.model.output_projection.0.weight\"]\n",
    "new_keys = [\"base_model.model.biogpt.embed_tokens.weight\", \"base_model.model.output_projection.weight\"]\n",
    "\n",
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    # Get the value of the old key\n",
    "    value = embed_tokens_state_dict[old_key]\n",
    "\n",
    "    # Create a new key-value pair with the updated name\n",
    "    embed_tokens_state_dict[new_key] = value\n",
    "\n",
    "    # Delete the old key if desired\n",
    "    del embed_tokens_state_dict[old_key]\n",
    "\n",
    "torch.save(embed_tokens_state_dict, \"GPT_without_ner/models/GPT_no_ner_epoch_15/pytorch_model-af.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BioGptForCausalLM(\n",
       "      (biogpt): BioGptModel(\n",
       "        (embed_tokens): CastOutputToFloat(\n",
       "          (0): Embedding(42390, 1024)\n",
       "        )\n",
       "        (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x BioGptDecoderLayer(\n",
       "            (self_attn): BioGptAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output_projection): CastOutputToFloat(\n",
       "        (0): Linear(in_features=1024, out_features=42390, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/tian/mambaforge/envs/BioRED did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/lib/mesa-diverted/x86_64-linux-gnu'), PosixPath('/usr/lib/x86_64-linux-gnu/gallium-pipe'), PosixPath('/usr/lib/x86_64-linux-gnu/mesa')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib/mesa-diverted/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/mesa:/usr/lib/x86_64-linux-gnu/dri:/usr/lib/x86_64-linux-gnu/gallium-pipe did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/4742,unix/tian-desktop'), PosixPath('local/tian-desktop')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/home/tian/mambaforge/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "\n",
    "peft_model_id = \"GPT_without_ner/models/GPT_no_ner_epoch_15.peft\"\n",
    "# config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GPT_without_ner/GPT_w_ner_tokenizer\")\n",
    "\n",
    "# resize the token embeddings to match the tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load the Lora model\n",
    "# the resized embedding layer are still uncorrected, need to load the weights manually\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BioGptForCausalLM(\n",
       "      (biogpt): BioGptModel(\n",
       "        (embed_tokens): Embedding(42390, 1024)\n",
       "        (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x BioGptDecoderLayer(\n",
       "            (self_attn): BioGptAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output_projection): Linear(in_features=1024, out_features=42390, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"GPT_without_ner/models/GPT_no_ner_epoch_15/pytorch_model-af.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text: @ HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label: 'the best' is the best for the patient\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from data_preprocessing import make_GPT_re_data_no_ner, GPT_no_ner_preprocess_function\n",
    "additional_tokens, _, _, _ = get_labels(mode='GPT_w_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf9532c2b84482485f9ca2ac8a8baeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truncated 4 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268deb2fb14040f0ab024ba657b2da11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\"\"\"# load test data and preprocess\n",
    "test_file_path = 'data/BioRED/processed/test.tsv'\n",
    "test_data = make_GPT_re_data_no_ner(file_path=test_file_path, lower=True, random_seed=42)\n",
    "\n",
    "# save the raw data\n",
    "with open('GPT_without_ner/data/test_data_dict.json', 'w') as f:\n",
    "    json.dump(test_data, f)\"\"\"\n",
    "with open('GPT_without_ner/data/test_data_dict.json', 'r') as f:\n",
    "    test_data= json.load(f)\n",
    "\n",
    "\n",
    "test_dataset_raw = Dataset.from_dict(test_data)\n",
    "\n",
    "test_dataset = test_dataset_raw.map(lambda example: GPT_no_ner_preprocess_function(example, tokenizer,  infer=True), batched=True, remove_columns=['pmids', 'text', 'entities', 'outputs', 'relation'])\n",
    "\n",
    "test_dataset.save_to_disk('GPT_without_ner/data/test_tokenized_dataset_no_ner')\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "test_dataset = load_from_disk('GPT_without_ner/data/test_tokenized_dataset_no_ner')\n",
    "\n",
    "test_dataset.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 322\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a novel scn5a mutation manifests as a malignant form of long qt syndrome with perinatal onset of tachycardia / bradycardia. objective: congenital long qt syndrome (lqts) with in utero onset of the rhythm disturbances is associated with a poor prognosis. in this study we investigated a newborn patient with fetal bradycardia, 2: 1 atrioventricular block and ventricular tachycardia soon after birth. methods: mutational analysis and dna sequencing were conducted in a newborn. the 2: 1 atrioventricular block improved to 1: 1 conduction only after intravenous lidocaine infusion or a high dose of mexiletine, which also controlled the ventricular tachycardia. results: a novel, spontaneous lqts-3 mutation was identified in the transmembrane segment 6 of domain iv of the na (v) 1.5 cardiac sodium channel, with a g-- > a substitution at codon 1763, which changed a valine (gtg) to a methionine (atg). the proband was heterozygous but the mutation was absent in the parents and the sister. expression of this mutant channel in tsa201 mammalian cells by site-directed mutagenesis revealed a persistent tetrodotoxin-sensitive but lidocaine-resistant current that was associated with a positive shift of the steady-state inactivation curve, steeper activation curve and faster recovery from inactivation. we also found a similar electrophysiological profile for the neighboring v1764 m mutant. but, the other neighboring i1762a mutant had no persistent current and was still associated with a positive shift of inactivation. conclusions: these findings suggest that the na (v) 1.5 / v1763 m channel dysfunction and possible neighboring mutants contribute to a persistent inward current due to altered inactivation kinetics and clinically congenital lqts with perinatal onset of arrhythmias that responded to lidocaine and mexiletine. for relation negative _ correlation, [learn1] [learn2] [learn3] [learn4] [learn5] [learn6]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e963a423624c5c98502d017677190d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is hbv and the target is rtl180m m m; the source is hbv and the target is rt180m m; the source is hbv and the target is rt180m m; the source is\n",
      "the source is none. </s>\n",
      "the source is pethidine and the target is pethidine. </s>\n",
      "the source is cambium and the target is lrr-rlk; the source is cambium and the target is cle41 / pxy; the source is cambium and the target is lrr-rlk; the\n",
      "the source is hawthorn and the target is isoproterenol; the source is hawthorn and the target is isoproterenol; the source is crataegus and the target is isoproterenol; the source is crataegus and the target is\n",
      "the source is meth and the target is 5-ht6 receptor; the source is meth and the target is 5-ht6 receptor; the source is meth and the target is 5-ht6 receptor; the source is meth and the target is\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is rsv and the target is lif; the source is rsv and the target is mif; the source is rsv and the target is ccl27; the source is rsv and the target is ccl2;\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is ssh1 and the target is actin. </s>\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is cmcs and the target is mmp-2 / 9; the source is cmcs and the target is vcam-1; the source is cmcs and the target is icam-1; the source is cmcs and\n",
      "the source is dach1 and the target is bfgf; the source is dach1 and the target is bfgf; the source is dach1 and the target is bfgf; the source is dach\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n",
      "the source is amisulpride and the target is hypertension; the source is amisulpride and the target is hypertension; the source is amisulpride and the target is hypertension; the source is amisulpride\n",
      "the source is none. </s>\n",
      "the source is pilocarpine and the target is gap43-ir; the source is pilocarpine and the target is gap43-ir; the source is pilocarpine and the target is gap43-ir; the source is pilocarpine and the target is\n",
      "the source is ma and the target is da; the source is ma and the target is da; the source is ma and the target is da; the source is ma and the target is da; the source is ma and the target is da;\n",
      "the source is vpa and the target is glutamate and the source is aspartate. </s>\n",
      "the source is none. </s>\n",
      "the source is none. </s>\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "model.eval()\n",
    "outputs = []\n",
    "model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "    # for i in range(1):\n",
    "        output = model.generate(input_ids=test_dataset[i][\"input_ids\"].unsqueeze(0).to(\"cuda\"), max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)\n",
    "        output_text = tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0]\n",
    "        try:\n",
    "            outputs.append(output_text.split(\"[learn6]\")[1].strip())\n",
    "        except:\n",
    "            outputs.append(output_text.strip())\n",
    "        if i % 10 == 0:\n",
    "            print(outputs[-1])\n",
    "\n",
    "    # print(tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the source is lithium and the target is polyuria ; the source is ndi and the target is pkca ; the source is lithium and the target is pkca ; the source is lithium and the target is ndi . ',\n",
       " 'the source is none . ',\n",
       " 'the source is alcohol and the target is il-1beta ; the source is alcohol and the target is tnf-alpha ; the source is alcohol and the target is lc3b-i and -ii ; the source is alcohol and the target is s6 ; the source is alcohol and the target is atg12 - 5 ; the source is alcohol and the target is eif4b ; the source is alcohol and the target is atg12 - 5 ; the source is alcohol and the target is il-6 ; the source is alcohol and the target is hsl ; the source is alcohol and the target is ulk1 ; the source is alcohol and the target is atgl . ',\n",
       " 'the source is alcohol and the target is ppargamma ; the source is alcohol and the target is c/ebpalpha . ',\n",
       " 'the source is alcohol and the target is s6k1 . ',\n",
       " 'the source is none . ',\n",
       " 'the source is scopolamine-induced and the target is learning and post-training consolidation deficits ; the source is apomorphine and the target is amnesia ; the source is dopaminergic agonism and the target is amnesia . ',\n",
       " 'the source is dm-9384 and the target is scopolamine-induced ; the source is dm-9384 and the target is amnesia ; the source is dm-9384 and the target is learning and post-training consolidation deficits ; the source is dm-9384 and the target is apomorphine ; the source is dm-9384 and the target is dopaminergic agonism . ',\n",
       " 'the source is [3h]sch 23390 and the target is d1 or d2 dopamine receptor ; the source is [3h]spiperone and the target is d1 or d2 dopamine receptor ; the source is [3h]sch 23390 and the target is d1 or d2 dopamine receptor ; the source is [3h]spiperone and the target is d1 or d2 dopamine receptor . ',\n",
       " 'the source is none . ',\n",
       " 'the source is pethidine and the target is postoperative pain . ',\n",
       " 'the source is pethidine and the target is seizure . ',\n",
       " 'the source is none . ',\n",
       " 'the source is everolimus and the target is myelitis ; the source is everolimus and the target is brachial neuritis . ',\n",
       " 'the source is tac and the target is neurotoxicity ; the source is tac and the target is myelitis ; the source is tac and the target is brachial neuritis . ',\n",
       " 'the source is tac and the target is everolimus . ',\n",
       " 'the source is none . ',\n",
       " ' . ',\n",
       " 'the source is jasmonic acid and the target is mol1 ; the source is ethylene and the target is mol1 . ',\n",
       " 'the source is mol1 and the target is clv1 . ',\n",
       " 'the source is none . ',\n",
       " 'the source is vcm and the target is necrosis ; the source is erdosteine and the target is sod ; the source is vcm and the target is mda ; the source is vcm and the target is desquamation ; the source is vcm and the target is nag ; the source is vcm and the target is atrophy ; the source is vcm and the target is kidney damage . ',\n",
       " 'the source is vcm and the target is ros ; the source is kidney damage and the target is nag . ',\n",
       " 'the source is vcm and the target is erdosteine ; the source is erdosteine and the target is nag ; the source is vcm and the target is cat ; the source is erdosteine and the target is mda ; the source is vcm and the target is sod ; the source is erdosteine and the target is kidney damage . ',\n",
       " 'the source is none . ',\n",
       " 'the source is cone rod dystrophy and the target is c.171t > c ; the source is cone rod dystrophy and the target is c.-17t > c ; the source is cone dystrophy and the target is c.-17t > c ; the source is cone dystrophy and the target is c.171t > c ; the source is cone rod dystrophy and the target is c.465g > t ; the source is cone dystrophy and the target is c.465g > t . ',\n",
       " 'the source is cone dystrophy and the target is guca1a ; the source is retinal degenerations and the target is gcap2 ; the source is cone rod dystrophy and the target is gcap2 ; the source is cone dystrophy and the target is gcap2 ; the source is calcium and the target is c.465g > t ; the source is retinal disease and the target is gcap2 ; the source is macular dystrophy and the target is guca1a ; the source is cone rod dystrophy and the target is guca1a . ',\n",
       " 'the source is calcium and the target is gcap2 . ',\n",
       " 'the source is none . ',\n",
       " 'the source is isoproterenol and the target is myocardial infarction ; the source is adp-stimulated and the target is oxygen ; the source is tcr and the target is adp-stimulated ; the source is tcr and the target is oxygen . ',\n",
       " 'the source is tcr and the target is lipid . ',\n",
       " 'the source is tcr and the target is myocardial infarction ; the source is tcr and the target is isoproterenol . ',\n",
       " 'the source is none . ',\n",
       " 'the source is pseudotumor cerebri and the target is guanine to cytosine ; the source is gs and the target is guanine to cytosine ; the source is pseudotumor cerebri and the target is p.ser555leu ; the source is pseudotumor cerebri and the target is c.2633 + 1g > c ; the source is gs and the target is c.2633 + 1g > c ; the source is pseudotumor cerebri and the target is slc12a3 ; the source is gs and the target is p.ser555leu ; the source is gs and the target is slc12a3 . ',\n",
       " 'the source is magnesium and the target is gs ; the source is potassium and the target is gs . ',\n",
       " 'the source is potassium and the target is magnesium . ',\n",
       " 'the source is none . ',\n",
       " 'the source is t2dm and the target is slc30a8 ; the source is t2dm and the target is cdkn2a/b ; the source is t2dm and the target is cdkn2a/b ; the source is t2dm and the target is cdkal1 ; the source is t2dm and the target is kcnq1 ; the source is t2dm and the target is hhex . ',\n",
       " 'the source is t2dm and the target is rs10811661 ; the source is t2dm and the target is rs2237892 ; the source is t2dm and the target is rs1111875 ; the source is t2dm and the target is rs7754840 ; the source is t2dm and the target is rs13266634 . ',\n",
       " 'the source is none . ',\n",
       " 'the source is meth-induced and the target is rs1805054 ; the source is psychosis and the target is rs1805054 ; the source is meth-induced and the target is rs6693503 ; the source is 5-ht6 receptor antagonist and the target is phencyclidine ; the source is meth-induced and the target is htr6 ; the source is antipsychotics and the target is htr6 ; the source is d-amphetamine and the target is 5-ht6 receptor antagonist ; the source is psychosis and the target is htr6 ; the source is meth-induced and the target is rs4912138 ; the source is olanzapine and the target is htr6 ; the source is 5-ht6 receptor antagonist and the target is schizophrenia ; the source is hyperactivity and the target is htr6 ; the source is psychosis and the target is rs6693503 ; the source is psychosis and the target is rs4912138 ; the source is clozapine and the target is htr6 . ',\n",
       " 'the source is meth-induced and the target is psychosis ; the source is d-amphetamine and the target is hyperactivity . ',\n",
       " 'the source is olanzapine and the target is psychosis ; the source is clozapine and the target is psychosis . ',\n",
       " 'the source is none . ',\n",
       " 'the source is nnk and the target is cdkn3 ; the source is nnk and the target is slurp-1 ; the source is nnk and the target is casp8 ; the source is nnk and the target is foxd3 ; the source is nnk and the target is serpinb5 ; the source is nnk and the target is ctnnb1 ; the source is nnk and the target is bax ; the source is nnk and the target is tnf . ',\n",
       " 'the source is nnk and the target is bcl2 ; the source is nnk and the target is runx3 ; the source is nnk and the target is nras ; the source is nnk and the target is kit ; the source is nnk and the target is egf ; the source is slurp-1 and the target is runx3 ; the source is nnk and the target is ets1 ; the source is nnk and the target is stat3 ; the source is nnk and the target is hgf ; the source is nnk and the target is rb1 ; the source is nnk and the target is akt1 ; the source is nnk and the target is myb ; the source is nnk and the target is cdkn2a ; the source is nnk and the target is pik3ca ; the source is nnk and the target is src . ',\n",
       " 'the source is tumor and the target is slurp-1 ; the source is tumor and the target is cdkn3 ; the source is tumor and the target is stat3 ; the source is tumor and the target is serpinb5 ; the source is tumor and the target is cdkn2a ; the source is tumor and the target is foxd3 ; the source is nnk and the target is tumor ; the source is tumor and the target is hgf ; the source is tumor and the target is egf . ',\n",
       " 'the source is none . ',\n",
       " 'the source is malignancy and the target is krt15 ; the source is malignancy and the target is steap4 ; the source is malignancy and the target is samd9 ; the source is malignancy and the target is tp63 ; the source is malignancy and the target is ceacam ; the source is malignancy and the target is serpina3 ; the source is malignancy and the target is pgr ; the source is ethanol and the target is malignancy ; the source is malignancy and the target is interferon ; the source is malignancy and the target is hla ; the source is malignancy and the target is gdf15 ; the source is malignancy and the target is itgb6 . ',\n",
       " 'the source is alcohol and the target is nanog ; the source is ethanol and the target is mt1x ; the source is ethanol and the target is ceacam ; the source is ethanol and the target is itgb6 ; the source is ethanol and the target is steap4 ; the source is ethanol and the target is gdf15 ; the source is ethanol and the target is pgr ; the source is ethanol and the target is serpina3 ; the source is ethanol and the target is interferon ; the source is ethanol and the target is breast cancer ; the source is ethanol and the target is alcohol ; the source is alcohol and the target is breast cancer ; the source is ethanol and the target is tp63 ; the source is ethanol and the target is krt15 ; the source is ethanol and the target is oct4 ; the source is ethanol and the target is nanog ; the source is alcohol and the target is oct4 ; the source is ethanol and the target is samd9 ; the source is ethanol and the target is hla . ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['labels'][30:80]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post-processing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing for the outputs without ner\n",
    "pairs = []\n",
    "for output in outputs:\n",
    "    pair = []\n",
    "    # if the output doesn't end with \"<|endoftext|>\", find the lastest \";\" of the output and only take the previous part\n",
    "    if output.endswith(\"<|endoftext|>\"):\n",
    "        string = output\n",
    "\n",
    "        for line in string.split(\";\"):\n",
    "            try:\n",
    "                source = line.split(\"the source is\")[1].strip()\n",
    "                source = source.split(\"and the target is\")[0].strip()\n",
    "                if source.startswith(\"none\"):\n",
    "                    source = \"none\"\n",
    "                    target = \"none\"\n",
    "                    continue\n",
    "                target = line.split(\"the target is\")[1].strip()\n",
    "                if target.endswith(\". <|endoftext|>\"):\n",
    "                    target = target.split(\". <|endoftext|>\")[0].strip()\n",
    "                    \n",
    "                if (source, target) not in pair:\n",
    "                    pair.append((source, target))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    else:\n",
    "        string = output.split(\";\")[:-1]\n",
    "        string = [line.strip() for line in string]\n",
    "        for line in string:\n",
    "            try:\n",
    "                source = line.split(\"the source is\")[1].strip()\n",
    "                source = source.split(\"and the target is\")[0].strip()\n",
    "\n",
    "                target = line.split(\"the target is\")[1].strip()\n",
    "                if (source, target) not in pair:\n",
    "                    pair.append((source, target))\n",
    "            except:\n",
    "                continue\n",
    "    if len(pair) == 0:\n",
    "        pair.append((\"none\", \"none\"))\n",
    "    pairs.append(pair)\n",
    "    \n",
    "output_pairs = pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing for the labels without ner\n",
    "pairs = []\n",
    "for output in test_dataset['labels']:\n",
    "    pair = []\n",
    "    # if the output doesn't end with \"<|endoftext|>\", find the lastest \";\" of the output and only take the previous part\n",
    "    if output.endswith(\"<|endoftext|>\"):\n",
    "        string = output\n",
    "\n",
    "        for line in string.split(\";\"):\n",
    "            try:\n",
    "                source = line.split(\"the source is\")[1].strip()\n",
    "                source = source.split(\"and the target is\")[0].strip()\n",
    "                if source.startswith(\"none\"):\n",
    "                    source = \"none\"\n",
    "                    target = \"none\"\n",
    "                    continue\n",
    "                target = line.split(\"the target is\")[1].strip()\n",
    "                if target.endswith(\". <|endoftext|>\"):\n",
    "                    target = target.split(\". <|endoftext|>\")[0].strip()\n",
    "                    \n",
    "                if (source, target) not in pair:\n",
    "                    pair.append((source, target))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    else:\n",
    "        string = output.split(\";\")[:-1]\n",
    "        string = [line.strip() for line in string]\n",
    "        for line in string:\n",
    "            try:\n",
    "                source = line.split(\"the source is\")[1].strip()\n",
    "                source = source.split(\"and the target is\")[0].strip()\n",
    "\n",
    "                target = line.split(\"the target is\")[1].strip()\n",
    "                if (source, target) not in pair:\n",
    "                    pair.append((source, target))\n",
    "            except:\n",
    "                continue\n",
    "    if len(pair) == 0:\n",
    "        pair.append((\"none\", \"none\"))\n",
    "    pairs.append(pair)\n",
    "\n",
    "label_pairs = pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"output\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for output, label in zip(output_pairs, label_pairs):\n",
    "    result['output'].append(output)\n",
    "    result['label'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result dictionary\n",
    "import pickle\n",
    "with open(\"GPT_without_ner/result/epoch_15_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"GPT_without_ner/result/epoch_15_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length: 322, 322\n",
      "instance:\n",
      "[('none', 'none')]\n",
      "[('lidocaine', 'ventricular tachycardia'), ('mexiletine', 'arrhythmias'), ('lidocaine', 'lqts'), ('mexiletine', 'lqts'), ('mexiletine', 'atrioventricular block'), ('lidocaine', 'atrioventricular block'), ('mexiletine', 'ventricular tachycardia')]\n"
     ]
    }
   ],
   "source": [
    "print(f'the length: {len(result[\"output\"])}, {len(result[\"label\"])}')\n",
    "print(f'instance:\\n{result[\"output\"][0]}\\n{result[\"label\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    for pair in output:\n",
    "        true_tuple = False\n",
    "        for label_pair in label:\n",
    "            if pair[0] == label_pair[0] and pair[1] == label_pair[1]:\n",
    "                tuple_tp += 1\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fp += 1\n",
    "    \n",
    "    for label_pair in label:\n",
    "        true_tuple = False\n",
    "        for pair in output:\n",
    "            if pair[0] == label_pair[0] and pair[1] == label_pair[1]:\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple precision: 0.28835978835978837, recall: 0.11077235772357724, f1: 0.16005873715124816\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple precision: 0.30687830687830686, recall: 0.11776649746192894, f1: 0.17021276595744683\n"
     ]
    }
   ],
   "source": [
    "# loosen the condition for the tp\n",
    "\n",
    "\n",
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    for pair in output:\n",
    "        true_tuple = False\n",
    "        for label_pair in label:\n",
    "            if (pair[0] in label_pair[0]) and (pair[1] in label_pair[1]):\n",
    "                tuple_tp += 1\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fp += 1\n",
    "    \n",
    "    for label_pair in label:\n",
    "        true_tuple = False\n",
    "        for pair in output:\n",
    "            if (pair[0] in label_pair[0]) and (pair[1] in label_pair[1]):\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fn += 1\n",
    "\n",
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
