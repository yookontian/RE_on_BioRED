{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_tokens = {'additional_special_tokens': ['[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 124439808 || all params: 124439808 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open('DocRED/data/train_annotated.json') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {\n",
    "    'id': [],\n",
    "    'text': [],\n",
    "    'head': [],\n",
    "    'tail': [],\n",
    "    'head_first': [],\n",
    "    'relation': [],\n",
    "    'head_start_pos' : [],\n",
    "    'tail_start_pos' : []\n",
    "}\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    sents = \"\"\n",
    "    for sent in train_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # relation_dict['text'].append(sents)\n",
    "\n",
    "    for relation_pair in train_set[i]['labels']:\n",
    "        relation_dict['id'].append(i)\n",
    "        relation_dict['text'].append(sents)\n",
    "        head = []\n",
    "        head_ = []\n",
    "        head_start_pos = []\n",
    "        head.append([[item['name'].lower()] for item in train_set[i]['vertexSet'][relation_pair['h']]])\n",
    "        for j, item in enumerate(head[0]):\n",
    "            if item not in head_:\n",
    "                head_.append(item)\n",
    "                head_start_pos.append(train_set[i]['vertexSet'][relation_pair['h']][j]['pos'][0])\n",
    "\n",
    "        relation_dict['head'].append(head_)\n",
    "        relation_dict['head_start_pos'].append(head_start_pos)\n",
    "\n",
    "        tail = []\n",
    "        tail_ = []\n",
    "        tail_start_pos = []\n",
    "        tail.append([[item['name'].lower()] for item in train_set[i]['vertexSet'][relation_pair['t']]])\n",
    "        for j, item in enumerate(tail[0]):\n",
    "            if item not in tail_:\n",
    "                tail_.append(item)\n",
    "                tail_start_pos.append(train_set[i]['vertexSet'][relation_pair['t']][j]['pos'][0])\n",
    "        relation_dict['tail'].append(tail_)\n",
    "        relation_dict['tail_start_pos'].append(tail_start_pos)\n",
    "\n",
    "        \n",
    "        if train_set[i]['vertexSet'][relation_pair['h']][0]['pos'][0] < train_set[i]['vertexSet'][relation_pair['t']][0]['pos'][0]:\n",
    "            relation_dict['head_first'].append(1)\n",
    "        else:\n",
    "            relation_dict['head_first'].append(0)\n",
    "        \n",
    "        relation_dict['relation'].append(relation_pair['r'])\n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/DocRED_baseline_metadata/relation_dict.json', 'w') as f:\n",
    "    json.dump(relation_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a8bb72fd144517a7b731c4ceafc43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "relation_dict = {\n",
    "    # 'id': [],\n",
    "    'text': [],\n",
    "    'pair': [],\n",
    "    # 'head_first': [],\n",
    "    'relation': [],\n",
    "    # 'head_start_pos' : [],\n",
    "    # 'tail_start_pos' : []\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(train_set))):\n",
    "    sents = \"\"\n",
    "    for sent in train_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # relation_dict['text'].append(sents)\n",
    "    relation_lines = {k['r']: [] for k in train_set[i]['labels']}\n",
    "    # print(relation_lines)\n",
    "    for relation_pair in train_set[i]['labels']:\n",
    "        # print(relation_pair)\n",
    "        # relation_dict['text'].append(sents)\n",
    "        heads = [item['name'].lower() for item in train_set[i]['vertexSet'][relation_pair['h']]]\n",
    "\n",
    "        max_length, max_index = 0, 0\n",
    "        for k, item in enumerate(heads):\n",
    "            # head = the longest string in the example['head'], also record the index of the longest string\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = k\n",
    "            \n",
    "        head = heads[max_index]\n",
    "\n",
    "        # print(heads)\n",
    "        # print(head)\n",
    "\n",
    "\n",
    "        tails = [item['name'].lower() for item in train_set[i]['vertexSet'][relation_pair['t']]]\n",
    "        max_length, max_index = 0, 0\n",
    "        for k, item in enumerate(tails):\n",
    "            # tail = the longest string in the example['tail'], also record the index of the longest string\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = k\n",
    "\n",
    "        tail = tails[max_index]\n",
    "\n",
    "        # print(tails)\n",
    "        # print(tail)\n",
    "\n",
    "        relation_lines[relation_pair['r']].append((head, tail))\n",
    "        \n",
    "    # random choosing a relation in the rel_info that not be included in the relation_lines.keys()\n",
    "\n",
    "    none_relation = random.choice(list(rel_info.keys()))\n",
    "    while none_relation in relation_lines.keys():\n",
    "        none_relation = random.choice(list(rel_info.keys()))\n",
    "    relation_lines[none_relation] = [(\"none\", \"none\")]\n",
    "\n",
    "    for relation, pair in relation_lines.items():\n",
    "        relation_dict['text'].append(sents)\n",
    "        relation_dict['pair'].append(pair)\n",
    "        relation_dict['relation'].append(rel_info[relation])\n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/DocRED_baseline_metadata/relation_dict_ner.json', 'w') as f:\n",
    "    json.dump(relation_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "if ner:\n",
    "    with open('DocRED/data/DocRED_baseline_metadata/relation_dict.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'head': relation_dict['head'],\n",
    "            'tail': relation_dict['tail'],\n",
    "            'head_first': relation_dict['head_first'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    with open('DocRED/data/DocRED_baseline_metadata/relation_dict_ner.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'pair': relation_dict['pair'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'pair', 'relation'],\n",
       "    num_rows: 19431\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_without_ner(example, tokenizer, padding=True):\n",
    "\n",
    "    padding=True\n",
    "    texts = example['text']\n",
    "\n",
    "    # special_tokens = [50259, 50260, 50261, 50262, 50263, 50264]\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    text_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    output_lines = []\n",
    "\n",
    "    for i in range(len(example['pair'])):\n",
    "        if example['pair'][i][0][0] != \"none\":\n",
    "            output_line = f\"for relation {example['relation'][i]} , \" + f\"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            for pair in example['pair'][i]:\n",
    "                output_line = output_line  + f\"the source is {pair[0]} and the target is {pair[1]} ; \"\n",
    "            \n",
    "            output_line = output_line[:-2] + \". \" + tokenizer.eos_token\n",
    "            output_lines.append(output_line)\n",
    "\n",
    "        else:\n",
    "            output_line = f\"for relation {example['relation'][i]} , \" + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] the source is none . \" + tokenizer.eos_token\n",
    "            output_lines.append(output_line)\n",
    "\n",
    "    # print(output_lines)\n",
    "\n",
    "\n",
    "    output_ids = tokenizer(output_lines, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    count = 0\n",
    "    for i, ids in enumerate(output_ids):\n",
    "        if len(text_ids[i]) + len(ids) > 1024:\n",
    "            text_ids[i] = text_ids[:1024 - len(ids)]\n",
    "            count += 1\n",
    "        text_ids[i] = text_ids[i] + output_ids[i]\n",
    "        assert len(text_ids[i]) <= 1024\n",
    "        attention_mask.append([1] * len(text_ids[i]) + [0] * (1024 - len(text_ids[i])))\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    if padding:\n",
    "        for i, ids in enumerate(output_ids):\n",
    "            output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "            text_ids[i] = text_ids[i] + [tokenizer.pad_token_id] * (1024 - len(text_ids[i]))\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_ner(example, tokenizer, padding=True):\n",
    "    texts = example['text']\n",
    "\n",
    "    # special_tokens = [50259, 50260, 50261, 50262, 50263, 50264]\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    text_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    for i in range(len(example['head'])):\n",
    "        head = \"\"\n",
    "        for item in example['head'][i]:\n",
    "            head += item[0] + \" ; \"\n",
    "        head = head[:-2]\n",
    "        head += \". \"\n",
    "\n",
    "        tail = \"\"\n",
    "        for item in example['tail'][i]:\n",
    "            tail += item[0] + \" ; \"\n",
    "        tail = tail[:-2]\n",
    "        tail += \". \"\n",
    "        \n",
    "        if example['head_first'][i] == 1:\n",
    "            output_line = \" entity 1 : \" + head + \"entity 2 : \" + tail + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            output_line = output_line + f\"the relation between source entity 1 and target entity 2 is {rel_info[example['relation'][i]]} . \" + tokenizer.eos_token\n",
    "\n",
    "        else:\n",
    "            output_line = \" entity 1 : \" + tail + \"entity 2 : \" + head + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            output_line = output_line + f\"the relation between source entity 2 and target entity 1 is {rel_info[example['relation'][i]]} . \" + tokenizer.eos_token\n",
    "        \n",
    "        output_texts.append(output_line)\n",
    "\n",
    "\n",
    "    output_ids = tokenizer(output_texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    count = 0\n",
    "    for i, ids in enumerate(output_ids):\n",
    "        if len(text_ids[i]) + len(ids) > 1024:\n",
    "            text_ids[i] = text_ids[:1024 - len(ids)]\n",
    "            count += 1\n",
    "        text_ids[i] = text_ids[i] + output_ids[i]\n",
    "        assert len(text_ids[i]) <= 1024\n",
    "        attention_mask.append([1] * len(text_ids[i]) + [0] * (1024 - len(text_ids[i])))\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    if padding:\n",
    "        for i, ids in enumerate(output_ids):\n",
    "            output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "            text_ids[i] = text_ids[i] + [tokenizer.pad_token_id] * (1024 - len(text_ids[i]))\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d53cf304fbb4cf1bc5f915572f61d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19431 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_dataset = dataset.map(lambda example: pro_processing_ner(example, tokenizer), batched=True, remove_columns=['text', 'head', 'tail', 'head_first', 'relation'])\n",
    "\n",
    "tokenized_dataset = dataset.map(lambda example: pro_processing_without_ner(example, tokenizer), batched=True, remove_columns=['text', 'pair', 'relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3d067f751847729301fa6d062e3373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/19431 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the datasets type tokenized_dataset\n",
    "\n",
    "tokenized_dataset.save_to_disk('DocRED/data/DocRED_baseline_metadata/tokenized_dataset_without_ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenized_dataset\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = load_from_disk('DocRED/data/DocRED_baseline_metadata/tokenized_dataset_without_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/BioRED/wandb/run-20230707_222615-u0fmms5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v' target=\"_blank\">GPT2-DocRED-without-ner-5epochs</a></strong> to <a href='https://wandb.ai/tian1995/GPT2-normal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/GPT2-normal' target=\"_blank\">https://wandb.ai/tian1995/GPT2-normal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v' target=\"_blank\">https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd87ca81240>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"GPT2-normal\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"GPT2-DocRED-without-ner-5epochs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, \n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=1000, \n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-4, \n",
    "        # fp16=True,\n",
    "        logging_steps=100, \n",
    "        report_to=\"wandb\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir='DocRED/GPT_without_ner'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcfa0271efc4d498a323011c30fe91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 28.43, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6175, 'learning_rate': 4e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2075, 'learning_rate': 6e-05, 'epoch': 0.06}\n",
      "{'loss': 3.0989, 'learning_rate': 8e-05, 'epoch': 0.08}\n",
      "{'loss': 3.0115, 'learning_rate': 0.0001, 'epoch': 0.1}\n",
      "{'loss': 2.8918, 'learning_rate': 0.00012, 'epoch': 0.12}\n",
      "{'loss': 2.8831, 'learning_rate': 0.00014, 'epoch': 0.14}\n",
      "{'loss': 2.8157, 'learning_rate': 0.00016, 'epoch': 0.16}\n",
      "{'loss': 2.73, 'learning_rate': 0.00018, 'epoch': 0.19}\n",
      "{'loss': 2.7119, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.6979, 'learning_rate': 0.00019914126234435382, 'epoch': 0.23}\n",
      "{'loss': 2.6357, 'learning_rate': 0.0001982825246887076, 'epoch': 0.25}\n",
      "{'loss': 2.6032, 'learning_rate': 0.00019742378703306143, 'epoch': 0.27}\n",
      "{'loss': 2.5512, 'learning_rate': 0.0001965650493774152, 'epoch': 0.29}\n",
      "{'loss': 2.4882, 'learning_rate': 0.000195706311721769, 'epoch': 0.31}\n",
      "{'loss': 2.4568, 'learning_rate': 0.00019484757406612282, 'epoch': 0.33}\n",
      "{'loss': 2.3863, 'learning_rate': 0.0001939888364104766, 'epoch': 0.35}\n",
      "{'loss': 2.4046, 'learning_rate': 0.00019313009875483042, 'epoch': 0.37}\n",
      "{'loss': 2.3109, 'learning_rate': 0.0001922713610991842, 'epoch': 0.39}\n",
      "{'loss': 2.2869, 'learning_rate': 0.000191412623443538, 'epoch': 0.41}\n",
      "{'loss': 2.3034, 'learning_rate': 0.0001905538857878918, 'epoch': 0.43}\n",
      "{'loss': 2.1946, 'learning_rate': 0.0001896951481322456, 'epoch': 0.45}\n",
      "{'loss': 2.2203, 'learning_rate': 0.0001888364104765994, 'epoch': 0.47}\n",
      "{'loss': 2.1452, 'learning_rate': 0.0001879776728209532, 'epoch': 0.49}\n",
      "{'loss': 2.0762, 'learning_rate': 0.00018711893516530702, 'epoch': 0.51}\n",
      "{'loss': 2.0585, 'learning_rate': 0.00018626019750966083, 'epoch': 0.54}\n",
      "{'loss': 2.0818, 'learning_rate': 0.0001854014598540146, 'epoch': 0.56}\n",
      "{'loss': 1.9827, 'learning_rate': 0.0001845427221983684, 'epoch': 0.58}\n",
      "{'loss': 1.9438, 'learning_rate': 0.0001836839845427222, 'epoch': 0.6}\n",
      "{'loss': 1.9437, 'learning_rate': 0.000182825246887076, 'epoch': 0.62}\n",
      "{'loss': 1.9876, 'learning_rate': 0.00018196650923142982, 'epoch': 0.64}\n",
      "{'loss': 1.9105, 'learning_rate': 0.0001811077715757836, 'epoch': 0.66}\n",
      "{'loss': 1.789, 'learning_rate': 0.0001802490339201374, 'epoch': 0.68}\n",
      "{'loss': 1.874, 'learning_rate': 0.00017939029626449119, 'epoch': 0.7}\n",
      "{'loss': 1.77, 'learning_rate': 0.000178531558608845, 'epoch': 0.72}\n",
      "{'loss': 1.7492, 'learning_rate': 0.00017767282095319882, 'epoch': 0.74}\n",
      "{'loss': 1.6937, 'learning_rate': 0.0001768140832975526, 'epoch': 0.76}\n",
      "{'loss': 1.6315, 'learning_rate': 0.00017595534564190642, 'epoch': 0.78}\n",
      "{'loss': 1.6714, 'learning_rate': 0.0001750966079862602, 'epoch': 0.8}\n",
      "{'loss': 1.639, 'learning_rate': 0.000174237870330614, 'epoch': 0.82}\n",
      "{'loss': 1.5652, 'learning_rate': 0.0001733791326749678, 'epoch': 0.84}\n",
      "{'loss': 1.603, 'learning_rate': 0.0001725203950193216, 'epoch': 0.86}\n",
      "{'loss': 1.5197, 'learning_rate': 0.0001716616573636754, 'epoch': 0.89}\n",
      "{'loss': 1.5292, 'learning_rate': 0.0001708029197080292, 'epoch': 0.91}\n",
      "{'loss': 1.467, 'learning_rate': 0.00016994418205238301, 'epoch': 0.93}\n",
      "{'loss': 1.4443, 'learning_rate': 0.0001690854443967368, 'epoch': 0.95}\n",
      "{'loss': 1.442, 'learning_rate': 0.0001682267067410906, 'epoch': 0.97}\n",
      "{'loss': 1.4727, 'learning_rate': 0.0001673679690854444, 'epoch': 0.99}\n",
      "{'loss': 1.337, 'learning_rate': 0.0001665092314297982, 'epoch': 1.01}\n",
      "{'loss': 1.2009, 'learning_rate': 0.000165650493774152, 'epoch': 1.03}\n",
      "{'loss': 1.1859, 'learning_rate': 0.00016479175611850582, 'epoch': 1.05}\n",
      "{'loss': 1.1621, 'learning_rate': 0.0001639330184628596, 'epoch': 1.07}\n",
      "{'loss': 1.1745, 'learning_rate': 0.0001630742808072134, 'epoch': 1.09}\n",
      "{'loss': 1.1124, 'learning_rate': 0.00016221554315156718, 'epoch': 1.11}\n",
      "{'loss': 1.1129, 'learning_rate': 0.000161356805495921, 'epoch': 1.13}\n",
      "{'loss': 1.1435, 'learning_rate': 0.00016049806784027481, 'epoch': 1.15}\n",
      "{'loss': 1.0897, 'learning_rate': 0.0001596393301846286, 'epoch': 1.17}\n",
      "{'loss': 1.0557, 'learning_rate': 0.00015878059252898242, 'epoch': 1.19}\n",
      "{'loss': 1.0264, 'learning_rate': 0.0001579218548733362, 'epoch': 1.21}\n",
      "{'loss': 1.093, 'learning_rate': 0.00015706311721769, 'epoch': 1.24}\n",
      "{'loss': 1.0324, 'learning_rate': 0.0001562043795620438, 'epoch': 1.26}\n",
      "{'loss': 1.0288, 'learning_rate': 0.0001553456419063976, 'epoch': 1.28}\n",
      "{'loss': 1.0433, 'learning_rate': 0.0001544869042507514, 'epoch': 1.3}\n",
      "{'loss': 0.9878, 'learning_rate': 0.0001536281665951052, 'epoch': 1.32}\n",
      "{'loss': 1.0081, 'learning_rate': 0.000152769428939459, 'epoch': 1.34}\n",
      "{'loss': 0.9349, 'learning_rate': 0.0001519106912838128, 'epoch': 1.36}\n",
      "{'loss': 0.9252, 'learning_rate': 0.0001510519536281666, 'epoch': 1.38}\n",
      "{'loss': 0.933, 'learning_rate': 0.0001501932159725204, 'epoch': 1.4}\n",
      "{'loss': 0.8933, 'learning_rate': 0.0001493344783168742, 'epoch': 1.42}\n",
      "{'loss': 0.8764, 'learning_rate': 0.000148475740661228, 'epoch': 1.44}\n",
      "{'loss': 0.8524, 'learning_rate': 0.00014761700300558182, 'epoch': 1.46}\n",
      "{'loss': 0.8842, 'learning_rate': 0.0001467582653499356, 'epoch': 1.48}\n",
      "{'loss': 0.8748, 'learning_rate': 0.0001458995276942894, 'epoch': 1.5}\n",
      "{'loss': 0.8963, 'learning_rate': 0.00014504079003864318, 'epoch': 1.52}\n",
      "{'loss': 0.8453, 'learning_rate': 0.000144182052382997, 'epoch': 1.54}\n",
      "{'loss': 0.8147, 'learning_rate': 0.0001433233147273508, 'epoch': 1.56}\n",
      "{'loss': 0.7874, 'learning_rate': 0.0001424645770717046, 'epoch': 1.59}\n",
      "{'loss': 0.8046, 'learning_rate': 0.00014160583941605841, 'epoch': 1.61}\n",
      "{'loss': 0.7834, 'learning_rate': 0.0001407471017604122, 'epoch': 1.63}\n",
      "{'loss': 0.7781, 'learning_rate': 0.000139888364104766, 'epoch': 1.65}\n",
      "{'loss': 0.7468, 'learning_rate': 0.0001390296264491198, 'epoch': 1.67}\n",
      "{'loss': 0.7705, 'learning_rate': 0.0001381708887934736, 'epoch': 1.69}\n",
      "{'loss': 0.7008, 'learning_rate': 0.0001373121511378274, 'epoch': 1.71}\n",
      "{'loss': 0.7005, 'learning_rate': 0.0001364534134821812, 'epoch': 1.73}\n",
      "{'loss': 0.7104, 'learning_rate': 0.000135594675826535, 'epoch': 1.75}\n",
      "{'loss': 0.7232, 'learning_rate': 0.0001347359381708888, 'epoch': 1.77}\n",
      "{'loss': 0.6478, 'learning_rate': 0.00013387720051524259, 'epoch': 1.79}\n",
      "{'loss': 0.696, 'learning_rate': 0.0001330184628595964, 'epoch': 1.81}\n",
      "{'loss': 0.72, 'learning_rate': 0.0001321597252039502, 'epoch': 1.83}\n",
      "{'loss': 0.6838, 'learning_rate': 0.000131300987548304, 'epoch': 1.85}\n",
      "{'loss': 0.6136, 'learning_rate': 0.00013044224989265782, 'epoch': 1.87}\n",
      "{'loss': 0.687, 'learning_rate': 0.0001295835122370116, 'epoch': 1.89}\n",
      "{'loss': 0.6562, 'learning_rate': 0.0001287247745813654, 'epoch': 1.91}\n",
      "{'loss': 0.6013, 'learning_rate': 0.00012786603692571918, 'epoch': 1.93}\n",
      "{'loss': 0.6566, 'learning_rate': 0.000127007299270073, 'epoch': 1.96}\n",
      "{'loss': 0.6016, 'learning_rate': 0.0001261485616144268, 'epoch': 1.98}\n",
      "{'loss': 0.6257, 'learning_rate': 0.0001252898239587806, 'epoch': 2.0}\n",
      "{'loss': 0.4766, 'learning_rate': 0.0001244310863031344, 'epoch': 2.02}\n",
      "{'loss': 0.5081, 'learning_rate': 0.0001235723486474882, 'epoch': 2.04}\n",
      "{'loss': 0.4996, 'learning_rate': 0.000122713610991842, 'epoch': 2.06}\n",
      "{'loss': 0.5075, 'learning_rate': 0.0001218548733361958, 'epoch': 2.08}\n",
      "{'loss': 0.5102, 'learning_rate': 0.00012099613568054959, 'epoch': 2.1}\n",
      "{'loss': 0.4718, 'learning_rate': 0.0001201373980249034, 'epoch': 2.12}\n",
      "{'loss': 0.4578, 'learning_rate': 0.0001192786603692572, 'epoch': 2.14}\n",
      "{'loss': 0.4695, 'learning_rate': 0.000118419922713611, 'epoch': 2.16}\n",
      "{'loss': 0.5269, 'learning_rate': 0.00011756118505796481, 'epoch': 2.18}\n",
      "{'loss': 0.4688, 'learning_rate': 0.0001167024474023186, 'epoch': 2.2}\n",
      "{'loss': 0.4584, 'learning_rate': 0.0001158437097466724, 'epoch': 2.22}\n",
      "{'loss': 0.4356, 'learning_rate': 0.00011498497209102619, 'epoch': 2.24}\n",
      "{'loss': 0.4369, 'learning_rate': 0.00011412623443538, 'epoch': 2.26}\n",
      "{'loss': 0.4029, 'learning_rate': 0.0001132674967797338, 'epoch': 2.28}\n",
      "{'loss': 0.432, 'learning_rate': 0.00011240875912408759, 'epoch': 2.31}\n",
      "{'loss': 0.4199, 'learning_rate': 0.0001115500214684414, 'epoch': 2.33}\n",
      "{'loss': 0.409, 'learning_rate': 0.00011069128381279519, 'epoch': 2.35}\n",
      "{'loss': 0.4134, 'learning_rate': 0.000109832546157149, 'epoch': 2.37}\n",
      "{'loss': 0.4672, 'learning_rate': 0.00010897380850150281, 'epoch': 2.39}\n",
      "{'loss': 0.4084, 'learning_rate': 0.0001081150708458566, 'epoch': 2.41}\n",
      "{'loss': 0.4143, 'learning_rate': 0.0001072563331902104, 'epoch': 2.43}\n",
      "{'loss': 0.4191, 'learning_rate': 0.00010639759553456419, 'epoch': 2.45}\n",
      "{'loss': 0.3953, 'learning_rate': 0.000105538857878918, 'epoch': 2.47}\n",
      "{'loss': 0.3813, 'learning_rate': 0.0001046801202232718, 'epoch': 2.49}\n",
      "{'loss': 0.3917, 'learning_rate': 0.00010382138256762559, 'epoch': 2.51}\n",
      "{'loss': 0.3844, 'learning_rate': 0.0001029626449119794, 'epoch': 2.53}\n",
      "{'loss': 0.3772, 'learning_rate': 0.00010210390725633319, 'epoch': 2.55}\n",
      "{'loss': 0.3758, 'learning_rate': 0.000101245169600687, 'epoch': 2.57}\n",
      "{'loss': 0.3656, 'learning_rate': 0.00010038643194504081, 'epoch': 2.59}\n",
      "{'loss': 0.3642, 'learning_rate': 9.95276942893946e-05, 'epoch': 2.61}\n",
      "{'loss': 0.3857, 'learning_rate': 9.86689566337484e-05, 'epoch': 2.63}\n",
      "{'loss': 0.3549, 'learning_rate': 9.78102189781022e-05, 'epoch': 2.66}\n",
      "{'loss': 0.3138, 'learning_rate': 9.6951481322456e-05, 'epoch': 2.68}\n",
      "{'loss': 0.3474, 'learning_rate': 9.609274366680979e-05, 'epoch': 2.7}\n",
      "{'loss': 0.3485, 'learning_rate': 9.523400601116359e-05, 'epoch': 2.72}\n",
      "{'loss': 0.3496, 'learning_rate': 9.43752683555174e-05, 'epoch': 2.74}\n",
      "{'loss': 0.339, 'learning_rate': 9.351653069987119e-05, 'epoch': 2.76}\n",
      "{'loss': 0.325, 'learning_rate': 9.265779304422499e-05, 'epoch': 2.78}\n",
      "{'loss': 0.3136, 'learning_rate': 9.17990553885788e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3321, 'learning_rate': 9.09403177329326e-05, 'epoch': 2.82}\n",
      "{'loss': 0.3223, 'learning_rate': 9.00815800772864e-05, 'epoch': 2.84}\n",
      "{'loss': 0.3242, 'learning_rate': 8.92228424216402e-05, 'epoch': 2.86}\n",
      "{'loss': 0.3106, 'learning_rate': 8.8364104765994e-05, 'epoch': 2.88}\n",
      "{'loss': 0.3147, 'learning_rate': 8.750536711034779e-05, 'epoch': 2.9}\n",
      "{'loss': 0.3067, 'learning_rate': 8.664662945470159e-05, 'epoch': 2.92}\n",
      "{'loss': 0.3057, 'learning_rate': 8.57878917990554e-05, 'epoch': 2.94}\n",
      "{'loss': 0.298, 'learning_rate': 8.492915414340919e-05, 'epoch': 2.96}\n",
      "{'loss': 0.305, 'learning_rate': 8.407041648776299e-05, 'epoch': 2.98}\n",
      "{'loss': 0.2773, 'learning_rate': 8.32116788321168e-05, 'epoch': 3.01}\n",
      "{'loss': 0.2597, 'learning_rate': 8.23529411764706e-05, 'epoch': 3.03}\n",
      "{'loss': 0.2292, 'learning_rate': 8.14942035208244e-05, 'epoch': 3.05}\n",
      "{'loss': 0.2348, 'learning_rate': 8.06354658651782e-05, 'epoch': 3.07}\n",
      "{'loss': 0.2332, 'learning_rate': 7.9776728209532e-05, 'epoch': 3.09}\n",
      "{'loss': 0.2494, 'learning_rate': 7.891799055388579e-05, 'epoch': 3.11}\n",
      "{'loss': 0.2556, 'learning_rate': 7.805925289823959e-05, 'epoch': 3.13}\n",
      "{'loss': 0.2205, 'learning_rate': 7.72005152425934e-05, 'epoch': 3.15}\n",
      "{'loss': 0.2437, 'learning_rate': 7.634177758694719e-05, 'epoch': 3.17}\n",
      "{'loss': 0.2461, 'learning_rate': 7.548303993130099e-05, 'epoch': 3.19}\n",
      "{'loss': 0.2576, 'learning_rate': 7.462430227565479e-05, 'epoch': 3.21}\n",
      "{'loss': 0.2209, 'learning_rate': 7.37655646200086e-05, 'epoch': 3.23}\n",
      "{'loss': 0.2183, 'learning_rate': 7.29068269643624e-05, 'epoch': 3.25}\n",
      "{'loss': 0.2239, 'learning_rate': 7.20480893087162e-05, 'epoch': 3.27}\n",
      "{'loss': 0.2461, 'learning_rate': 7.118935165307e-05, 'epoch': 3.29}\n",
      "{'loss': 0.2189, 'learning_rate': 7.033061399742379e-05, 'epoch': 3.31}\n",
      "{'loss': 0.2118, 'learning_rate': 6.947187634177759e-05, 'epoch': 3.33}\n",
      "{'loss': 0.2278, 'learning_rate': 6.86131386861314e-05, 'epoch': 3.36}\n",
      "{'loss': 0.2245, 'learning_rate': 6.775440103048519e-05, 'epoch': 3.38}\n",
      "{'loss': 0.2215, 'learning_rate': 6.689566337483899e-05, 'epoch': 3.4}\n",
      "{'loss': 0.2209, 'learning_rate': 6.603692571919279e-05, 'epoch': 3.42}\n",
      "{'loss': 0.2299, 'learning_rate': 6.517818806354658e-05, 'epoch': 3.44}\n",
      "{'loss': 0.1983, 'learning_rate': 6.43194504079004e-05, 'epoch': 3.46}\n",
      "{'loss': 0.2237, 'learning_rate': 6.34607127522542e-05, 'epoch': 3.48}\n",
      "{'loss': 0.2213, 'learning_rate': 6.260197509660798e-05, 'epoch': 3.5}\n",
      "{'loss': 0.2073, 'learning_rate': 6.174323744096178e-05, 'epoch': 3.52}\n",
      "{'loss': 0.2056, 'learning_rate': 6.0884499785315586e-05, 'epoch': 3.54}\n",
      "{'loss': 0.2072, 'learning_rate': 6.0025762129669394e-05, 'epoch': 3.56}\n",
      "{'loss': 0.2149, 'learning_rate': 5.916702447402319e-05, 'epoch': 3.58}\n",
      "{'loss': 0.2113, 'learning_rate': 5.830828681837699e-05, 'epoch': 3.6}\n",
      "{'loss': 0.1917, 'learning_rate': 5.7449549162730784e-05, 'epoch': 3.62}\n",
      "{'loss': 0.1998, 'learning_rate': 5.6590811507084585e-05, 'epoch': 3.64}\n",
      "{'loss': 0.1947, 'learning_rate': 5.5732073851438393e-05, 'epoch': 3.66}\n",
      "{'loss': 0.1984, 'learning_rate': 5.487333619579219e-05, 'epoch': 3.68}\n",
      "{'loss': 0.1846, 'learning_rate': 5.401459854014599e-05, 'epoch': 3.71}\n",
      "{'loss': 0.1873, 'learning_rate': 5.3155860884499784e-05, 'epoch': 3.73}\n",
      "{'loss': 0.1838, 'learning_rate': 5.2297123228853585e-05, 'epoch': 3.75}\n",
      "{'loss': 0.182, 'learning_rate': 5.143838557320739e-05, 'epoch': 3.77}\n",
      "{'loss': 0.1889, 'learning_rate': 5.057964791756119e-05, 'epoch': 3.79}\n",
      "{'loss': 0.1823, 'learning_rate': 4.972091026191499e-05, 'epoch': 3.81}\n",
      "{'loss': 0.1844, 'learning_rate': 4.886217260626878e-05, 'epoch': 3.83}\n",
      "{'loss': 0.1895, 'learning_rate': 4.800343495062259e-05, 'epoch': 3.85}\n",
      "{'loss': 0.1855, 'learning_rate': 4.7144697294976386e-05, 'epoch': 3.87}\n",
      "{'loss': 0.1756, 'learning_rate': 4.628595963933019e-05, 'epoch': 3.89}\n",
      "{'loss': 0.1719, 'learning_rate': 4.542722198368399e-05, 'epoch': 3.91}\n",
      "{'loss': 0.1832, 'learning_rate': 4.456848432803778e-05, 'epoch': 3.93}\n",
      "{'loss': 0.1722, 'learning_rate': 4.370974667239159e-05, 'epoch': 3.95}\n",
      "{'loss': 0.1721, 'learning_rate': 4.2851009016745385e-05, 'epoch': 3.97}\n",
      "{'loss': 0.1885, 'learning_rate': 4.199227136109919e-05, 'epoch': 3.99}\n",
      "{'loss': 0.1441, 'learning_rate': 4.113353370545299e-05, 'epoch': 4.01}\n",
      "{'loss': 0.1541, 'learning_rate': 4.027479604980678e-05, 'epoch': 4.03}\n",
      "{'loss': 0.1615, 'learning_rate': 3.941605839416059e-05, 'epoch': 4.06}\n",
      "{'loss': 0.1554, 'learning_rate': 3.8557320738514385e-05, 'epoch': 4.08}\n",
      "{'loss': 0.1645, 'learning_rate': 3.7698583082868186e-05, 'epoch': 4.1}\n",
      "{'loss': 0.1383, 'learning_rate': 3.683984542722199e-05, 'epoch': 4.12}\n",
      "{'loss': 0.1402, 'learning_rate': 3.598110777157578e-05, 'epoch': 4.14}\n",
      "{'loss': 0.1456, 'learning_rate': 3.512237011592959e-05, 'epoch': 4.16}\n",
      "{'loss': 0.1438, 'learning_rate': 3.4263632460283385e-05, 'epoch': 4.18}\n",
      "{'loss': 0.141, 'learning_rate': 3.340489480463718e-05, 'epoch': 4.2}\n",
      "{'loss': 0.1299, 'learning_rate': 3.254615714899099e-05, 'epoch': 4.22}\n",
      "{'loss': 0.134, 'learning_rate': 3.168741949334478e-05, 'epoch': 4.24}\n",
      "{'loss': 0.1376, 'learning_rate': 3.082868183769859e-05, 'epoch': 4.26}\n",
      "{'loss': 0.137, 'learning_rate': 2.9969944182052384e-05, 'epoch': 4.28}\n",
      "{'loss': 0.1375, 'learning_rate': 2.9111206526406182e-05, 'epoch': 4.3}\n",
      "{'loss': 0.142, 'learning_rate': 2.8252468870759983e-05, 'epoch': 4.32}\n",
      "{'loss': 0.1369, 'learning_rate': 2.739373121511378e-05, 'epoch': 4.34}\n",
      "{'loss': 0.1524, 'learning_rate': 2.6534993559467586e-05, 'epoch': 4.36}\n",
      "{'loss': 0.1414, 'learning_rate': 2.5676255903821384e-05, 'epoch': 4.38}\n",
      "{'loss': 0.1497, 'learning_rate': 2.4817518248175185e-05, 'epoch': 4.41}\n",
      "{'loss': 0.1198, 'learning_rate': 2.3958780592528983e-05, 'epoch': 4.43}\n",
      "{'loss': 0.1273, 'learning_rate': 2.3100042936882784e-05, 'epoch': 4.45}\n",
      "{'loss': 0.1351, 'learning_rate': 2.2241305281236582e-05, 'epoch': 4.47}\n",
      "{'loss': 0.1295, 'learning_rate': 2.1382567625590383e-05, 'epoch': 4.49}\n",
      "{'loss': 0.1239, 'learning_rate': 2.0523829969944185e-05, 'epoch': 4.51}\n",
      "{'loss': 0.1351, 'learning_rate': 1.9665092314297983e-05, 'epoch': 4.53}\n",
      "{'loss': 0.13, 'learning_rate': 1.8806354658651784e-05, 'epoch': 4.55}\n",
      "{'loss': 0.1251, 'learning_rate': 1.7947617003005582e-05, 'epoch': 4.57}\n",
      "{'loss': 0.1466, 'learning_rate': 1.7088879347359383e-05, 'epoch': 4.59}\n",
      "{'loss': 0.1343, 'learning_rate': 1.623014169171318e-05, 'epoch': 4.61}\n",
      "{'loss': 0.1308, 'learning_rate': 1.5371404036066982e-05, 'epoch': 4.63}\n",
      "{'loss': 0.1239, 'learning_rate': 1.4512666380420784e-05, 'epoch': 4.65}\n",
      "{'loss': 0.1453, 'learning_rate': 1.3653928724774581e-05, 'epoch': 4.67}\n",
      "{'loss': 0.1383, 'learning_rate': 1.2795191069128381e-05, 'epoch': 4.69}\n",
      "{'loss': 0.1209, 'learning_rate': 1.1936453413482182e-05, 'epoch': 4.71}\n",
      "{'loss': 0.1259, 'learning_rate': 1.1077715757835982e-05, 'epoch': 4.73}\n",
      "{'loss': 0.1471, 'learning_rate': 1.0218978102189781e-05, 'epoch': 4.76}\n",
      "{'loss': 0.1263, 'learning_rate': 9.360240446543583e-06, 'epoch': 4.78}\n",
      "{'loss': 0.1161, 'learning_rate': 8.50150279089738e-06, 'epoch': 4.8}\n",
      "{'loss': 0.1238, 'learning_rate': 7.642765135251182e-06, 'epoch': 4.82}\n",
      "{'loss': 0.1287, 'learning_rate': 6.784027479604981e-06, 'epoch': 4.84}\n",
      "{'loss': 0.1313, 'learning_rate': 5.925289823958781e-06, 'epoch': 4.86}\n",
      "{'loss': 0.1307, 'learning_rate': 5.066552168312581e-06, 'epoch': 4.88}\n",
      "{'loss': 0.1369, 'learning_rate': 4.20781451266638e-06, 'epoch': 4.9}\n",
      "{'loss': 0.1224, 'learning_rate': 3.3490768570201806e-06, 'epoch': 4.92}\n",
      "{'loss': 0.1283, 'learning_rate': 2.49033920137398e-06, 'epoch': 4.94}\n",
      "{'loss': 0.116, 'learning_rate': 1.6316015457277802e-06, 'epoch': 4.96}\n",
      "{'loss': 0.1358, 'learning_rate': 7.728638900815801e-07, 'epoch': 4.98}\n",
      "{'train_runtime': 20677.7711, 'train_samples_per_second': 4.699, 'train_steps_per_second': 1.175, 'train_loss': 0.8649780624047284, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24290, training_loss=0.8649780624047284, metrics={'train_runtime': 20677.7711, 'train_samples_per_second': 4.699, 'train_steps_per_second': 1.175, 'train_loss': 0.8649780624047284, 'epoch': 5.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▇███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>24290</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1358</td></tr><tr><td>train/total_flos</td><td>5.077165473792e+16</td></tr><tr><td>train/train_loss</td><td>0.86498</td></tr><tr><td>train/train_runtime</td><td>20677.7711</td></tr><tr><td>train/train_samples_per_second</td><td>4.699</td></tr><tr><td>train/train_steps_per_second</td><td>1.175</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT2-DocRED-without-ner-5epochs</strong> at: <a href='https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v' target=\"_blank\">https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230707_222615-u0fmms5v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "trainer.save_model(\"DocRED/GPT_without_ner/model\")\n",
    "\n",
    "# save the tokenizer\n",
    "# tokenizer.save_pretrained(\"DocRED/GPT_w_ner/tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"DocRED/GPT_w_ner/model\"\n",
    "# checkpoint = \"DocRED/GPT_without_ner/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"DocRED/GPT_w_ner/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label : uk supreme motorsports   uk supreme motors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open('DocRED/data/dev.json') as f:\n",
    "    test_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b41da423a6d4577aa7a8f1a843f87b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with ner\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "relation_dict = {\n",
    "    'id': [],\n",
    "    'text': [],\n",
    "    'head': [],\n",
    "    'tail': [],\n",
    "    'head_first': [],\n",
    "    'relation': [],\n",
    "    'head_start_pos' : [],\n",
    "    'tail_start_pos' : []\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(test_set))):\n",
    "    sents = \"\"\n",
    "    for sent in test_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # relation_dict['text'].append(sents)\n",
    "\n",
    "    for relation_pair in test_set[i]['labels']:\n",
    "        relation_dict['id'].append(i)\n",
    "        relation_dict['text'].append(sents)\n",
    "        head = []\n",
    "        head_ = []\n",
    "        head_start_pos = []\n",
    "        head.append([[item['name'].lower()] for item in test_set[i]['vertexSet'][relation_pair['h']]])\n",
    "        for j, item in enumerate(head[0]):\n",
    "            if item not in head_:\n",
    "                head_.append(item)\n",
    "                head_start_pos.append(test_set[i]['vertexSet'][relation_pair['h']][j]['pos'][0])\n",
    "\n",
    "        relation_dict['head'].append(head_)\n",
    "        relation_dict['head_start_pos'].append(head_start_pos)\n",
    "\n",
    "        tail = []\n",
    "        tail_ = []\n",
    "        tail_start_pos = []\n",
    "        tail.append([[item['name'].lower()] for item in test_set[i]['vertexSet'][relation_pair['t']]])\n",
    "        for j, item in enumerate(tail[0]):\n",
    "            if item not in tail_:\n",
    "                tail_.append(item)\n",
    "                tail_start_pos.append(test_set[i]['vertexSet'][relation_pair['t']][j]['pos'][0])\n",
    "        relation_dict['tail'].append(tail_)\n",
    "        relation_dict['tail_start_pos'].append(tail_start_pos)\n",
    "\n",
    "        \n",
    "        if test_set[i]['vertexSet'][relation_pair['h']][0]['pos'][0] < test_set[i]['vertexSet'][relation_pair['t']][0]['pos'][0]:\n",
    "            relation_dict['head_first'].append(1)\n",
    "        else:\n",
    "            relation_dict['head_first'].append(0)\n",
    "        \n",
    "        relation_dict['relation'].append(relation_pair['r'])\n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/DocRED_baseline_metadata/dev_relation_dict.json', 'w') as f:\n",
    "    json.dump(relation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257f2305217e4f708a6c6a29cf057ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# without ner\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "relation_dict = {\n",
    "    # 'id': [],\n",
    "    'text': [],\n",
    "    'pair': [],\n",
    "    # 'head_first': [],\n",
    "    'relation': [],\n",
    "    # 'head_start_pos' : [],\n",
    "    # 'tail_start_pos' : []\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(test_set))):\n",
    "    sents = \"\"\n",
    "    for sent in test_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # relation_dict['text'].append(sents)\n",
    "    relation_lines = {k['r']: [] for k in test_set[i]['labels']}\n",
    "    # print(relation_lines)\n",
    "    for relation_pair in test_set[i]['labels']:\n",
    "        # print(relation_pair)\n",
    "        # relation_dict['text'].append(sents)\n",
    "        heads = [item['name'].lower() for item in test_set[i]['vertexSet'][relation_pair['h']]]\n",
    "\n",
    "        max_length, max_index = 0, 0\n",
    "        for k, item in enumerate(heads):\n",
    "            # head = the longest string in the example['head'], also record the index of the longest string\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = k\n",
    "            \n",
    "        head = heads[max_index]\n",
    "\n",
    "        # print(heads)\n",
    "        # print(head)\n",
    "\n",
    "\n",
    "        tails = [item['name'].lower() for item in test_set[i]['vertexSet'][relation_pair['t']]]\n",
    "        max_length, max_index = 0, 0\n",
    "        for k, item in enumerate(tails):\n",
    "            # tail = the longest string in the example['tail'], also record the index of the longest string\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = k\n",
    "\n",
    "        tail = tails[max_index]\n",
    "\n",
    "        # print(tails)\n",
    "        # print(tail)\n",
    "\n",
    "        relation_lines[relation_pair['r']].append((head, tail))\n",
    "        \n",
    "    # random choosing a relation in the rel_info that not be included in the relation_lines.keys()\n",
    "\n",
    "    none_relation = random.choice(list(rel_info.keys()))\n",
    "    while none_relation in relation_lines.keys():\n",
    "        none_relation = random.choice(list(rel_info.keys()))\n",
    "    relation_lines[none_relation] = [(\"none\", \"none\")]\n",
    "\n",
    "    for relation, pair in relation_lines.items():\n",
    "        relation_dict['text'].append(sents)\n",
    "        relation_dict['pair'].append(pair)\n",
    "        relation_dict['relation'].append(rel_info[relation])\n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/DocRED_baseline_metadata/dev_relation_dict_without_ner.json', 'w') as f:\n",
    "    json.dump(relation_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "if ner:\n",
    "    with open('DocRED/data/DocRED_baseline_metadata/dev_relation_dict.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'head': relation_dict['head'],\n",
    "            'tail': relation_dict['tail'],\n",
    "            'head_first': relation_dict['head_first'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    with open('DocRED/data/DocRED_baseline_metadata/dev_relation_dict_without_ner.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'pair': relation_dict['pair'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_info[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country of citizenship\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'conrad oberon johnson ( november 15 , 1915 – february 3 , 2008 ) was an american music educator , long associated with the city of houston , who was inducted into the texas bandmasters hall of fame in 2000 . born in victoria , texas , conrad johnson was nine when his family moved to houston . following studies at yates high school , he attended houston college for negroes and graduated from wiley college . he was an active member of omega psi phi fraternity . he started his career in music education in 1941 and , following a thirty - seven - year career , retired from his position at kashmere high school in 1978 , but continued to remain active in shaping music in houston by conducting summer programs and in - home tutoring . johnson was a proficient musician in his own right and , at one point , played with count basie . erskine hawkins tried to convince him to join his orchestra , but johnson declined , citing a love of teaching and obligations to his family . later , johnson made his lasting contribution to music by forming the kashmere stage band , a renowned school orchestra that won a number of awards during its decade - long run . the conrad o. johnson school of fine arts at kashmere high school is named after him . conrad o. johnson died in houston days after his former students staged a celebration in his honor . the gala saturday night concert , which was filmed by a documentary crew , was described by the students as \" the greatest 92nd birthday gift that he could have ever requested . \" ',\n",
       " 'head': [['conrad oberon johnson'],\n",
       "  ['conrad johnson'],\n",
       "  ['conrad o. johnson'],\n",
       "  ['johnson']],\n",
       " 'tail': [['american']],\n",
       " 'head_first': 1,\n",
       " 'relation': 'P27'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 45\n",
    "print(rel_info[dataset[n]['relation']])\n",
    "dataset[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_without_ner_infer(example, tokenizer):\n",
    "\n",
    "    padding=True\n",
    "    texts = example['text']\n",
    "\n",
    "    # special_tokens = [50259, 50260, 50261, 50262, 50263, 50264]\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    text_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    output_lines = []\n",
    "\n",
    "    for i in range(len(example['pair'])):\n",
    "        # if example['pair'][i][0][0] != \"none\":\n",
    "        output_line = f\"for relation {example['relation'][i]} , [learn1] [learn2] [learn3][learn4] [learn5] [learn6] \"\n",
    "        # for pair in example['pair'][i]:\n",
    "        #     output_line = output_line  + f\"the source is {pair[0]} and the target is {pair[1]} ; \"\n",
    "        \n",
    "        # output_line = output_line[:-2] + \". \" + tokenizer.eos_token\n",
    "        output_lines.append(output_line)\n",
    "        if i == 5:\n",
    "            print(output_line)\n",
    "\n",
    "        # else:\n",
    "        #     output_line = f\"for relation {example['relation'][i]} , \" + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] the source is none . \" + tokenizer.eos_token\n",
    "        #     output_lines.append(output_line)\n",
    "\n",
    "    # print(output_lines)\n",
    "\n",
    "\n",
    "    output_ids = tokenizer(output_lines, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    count = 0\n",
    "    for i, ids in enumerate(output_ids):\n",
    "        if len(text_ids[i]) + len(ids) > 1024:\n",
    "            text_ids[i] = text_ids[:1024 - len(ids)]\n",
    "            count += 1\n",
    "        text_ids[i] = text_ids[i] + output_ids[i]\n",
    "        assert len(text_ids[i]) <= 1024\n",
    "        # attention_mask.append([1] * len(text_ids[i]) + [0] * (1024 - len(text_ids[i])))\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    # if padding:\n",
    "    #     for i, ids in enumerate(output_ids):\n",
    "    #         output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "    #         text_ids[i] = text_ids[i] + [tokenizer.pad_token_id] * (1024 - len(text_ids[i]))\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_ids,\n",
    "        # 'attention_mask': attention_mask,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_ner_infer(example, tokenizer):\n",
    "    texts = example['text']\n",
    "\n",
    "    # special_tokens = [50259, 50260, 50261, 50262, 50263, 50264]\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    text_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    for i in range(len(example['head'])):\n",
    "        head = \"\"\n",
    "        for item in example['head'][i]:\n",
    "            head += item[0] + \" ; \"\n",
    "        head = head[:-2]\n",
    "        head += \". \"\n",
    "\n",
    "        tail = \"\"\n",
    "        for item in example['tail'][i]:\n",
    "            tail += item[0] + \" ; \"\n",
    "        tail = tail[:-2]\n",
    "        tail += \". \"\n",
    "        \n",
    "        if example['head_first'][i] == 1:\n",
    "            output_line = \"entity 1 : \" + head + \"entity 2 : \" + tail + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            # output_line = output_line + f\"the relation between source entity 1 and target entity 2 is {rel_info[example['relation'][i]]} . \" + tokenizer.eos_token\n",
    "\n",
    "        else:\n",
    "            output_line = \" entity 1 : \" + tail + \"entity 2 : \" + head + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            # output_line = output_line + f\"the relation between source entity 2 and target entity 1 is {rel_info[example['relation'][i]]} . \" + tokenizer.eos_token\n",
    "        \n",
    "        output_texts.append(output_line)\n",
    "\n",
    "\n",
    "    output_ids = tokenizer(output_texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    # attention_mask = []\n",
    "\n",
    "    count = 0\n",
    "    for i, ids in enumerate(output_ids):\n",
    "        if len(text_ids[i]) + len(ids) > 1024:\n",
    "            text_ids[i] = text_ids[:1024 - len(ids)]\n",
    "            count += 1\n",
    "        text_ids[i] = text_ids[i] + output_ids[i]\n",
    "        assert len(text_ids[i]) <= 1024\n",
    "        # attention_mask.append([1] * len(text_ids[i]) + [0] * (1024 - len(text_ids[i])))\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    # if padding:\n",
    "    #     for i, ids in enumerate(output_ids):\n",
    "    #         output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "    #         text_ids[i] = text_ids[i] + [tokenizer.pad_token_id] * (1024 - len(text_ids[i]))\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_ids,\n",
    "        # 'attention_mask': attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'pair', 'relation'],\n",
       "    num_rows: 6254\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cc23de5eff44feb3f538738da93230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m example: pro_processing_without_ner_infer(example, tokenizer), batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpair\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrelation\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m \u001b[39m# tokenized_dataset = dataset.map(lambda example: pro_processing_ner_infer(example, tokenizer), batched=True, remove_columns=['text', 'head', 'tail', 'head_first', 'relation'])\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    579\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    580\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    581\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3066\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3067\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3068\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3071\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3074\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3075\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/datasets/arrow_dataset.py:3449\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3445\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3446\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3447\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3448\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3449\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3450\u001b[0m         batch,\n\u001b[1;32m   3451\u001b[0m         indices,\n\u001b[1;32m   3452\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3453\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3454\u001b[0m     )\n\u001b[1;32m   3455\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3456\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3458\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/BioRED/lib/python3.10/site-packages/datasets/arrow_dataset.py:3330\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3328\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3329\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3330\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3332\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3333\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3334\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(example)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m example: pro_processing_without_ner_infer(example, tokenizer), batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpair\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrelation\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[39m# tokenized_dataset = dataset.map(lambda example: pro_processing_ner_infer(example, tokenizer), batched=True, remove_columns=['text', 'head', 'tail', 'head_first', 'relation'])\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# tokenized_dataset = dataset.map(lambda example: pro_processing_without_ner_infer(example, tokenizer), batched=True, remove_columns=['text', 'pair', 'relation'])\n",
    "tokenized_dataset = dataset.map(lambda example: pro_processing_ner_infer(example, tokenizer), batched=True, remove_columns=['text', 'head', 'tail', 'head_first', 'relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = load_from_disk('DocRED/data/DocRED_baseline_metadata/dev_tokenized_dataset_w_ner')\n",
    "# tokenized_dataset = load_from_disk('DocRED/data/DocRED_baseline_metadata/dev_tokenized_dataset_without_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conrad oberon johnson ( november 15, 1915 – february 3, 2008 ) was an american music educator, long associated with the city of houston, who was inducted into the texas bandmasters hall of fame in 2000. born in victoria, texas, conrad johnson was nine when his family moved to houston. following studies at yates high school, he attended houston college for negroes and graduated from wiley college. he was an active member of omega psi phi fraternity. he started his career in music education in 1941 and, following a thirty - seven - year career, retired from his position at kashmere high school in 1978, but continued to remain active in shaping music in houston by conducting summer programs and in - home tutoring. johnson was a proficient musician in his own right and, at one point, played with count basie. erskine hawkins tried to convince him to join his orchestra, but johnson declined, citing a love of teaching and obligations to his family. later, johnson made his lasting contribution to music by forming the kashmere stage band, a renowned school orchestra that won a number of awards during its decade - long run. the conrad o. johnson school of fine arts at kashmere high school is named after him. conrad o. johnson died in houston days after his former students staged a celebration in his honor. the gala saturday night concert, which was filmed by a documentary crew, was described by the students as \" the greatest 92nd birthday gift that he could have ever requested. \" entity 1 : conrad oberon johnson ; conrad johnson ; conrad o. johnson ; johnson. entity 2 : american. [learn1] [learn2] [learn3] [learn4] [learn5] [learn6]'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset[45]['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5602fdf9f24625adb8b3c3576873a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is mother. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is member of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is award received. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is author. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is publication date. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country of origin. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is present in work. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is member of political party. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is publisher. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is religion. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is sibling. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is conflict. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is director. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is instance of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is performer. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is continent. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of death. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is follows. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is narrative location. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is place of birth. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is author. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is author. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is has part. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is author. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is participant of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is part of. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country of origin. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is member of sports team. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is sibling. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is member of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is member of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is father. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is cast member. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is follows. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is applies to jurisdiction. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is present in work. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is ethnic group. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is performer. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is award received. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is present in work. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is record label. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is religion. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is composer. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is contains administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is part of. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is cast member. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is producer. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is performer. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in or next to body of water. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of death. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is follows. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is religion. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is has part. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "outputs = []\n",
    "model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(tokenized_dataset))):\n",
    "    # for i in range(1):\n",
    "        output = model.generate(input_ids=tokenized_dataset[\"input_ids\"][i].unsqueeze(0).to(\"cuda\"), max_new_tokens=100, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "        output_text = tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0]\n",
    "        try:\n",
    "            outputs.append(output_text.split(\"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6]\")[1].strip())\n",
    "        except:\n",
    "            outputs.append(output_text.split(\"[learn1][learn2][learn3][learn4][learn5][learn6]\")[1].strip())\n",
    "        if i % 100 == 0:\n",
    "            print(outputs[-1])\n",
    "\n",
    "    # print(tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"skai tv is a greek free - to - air television network based in piraeus . it is part of the skai group , one of the largest media groups in the country . it was relaunched in its present form on 1st of april 2006 in the athens metropolitan area , and gradually spread its coverage nationwide . besides digital terrestrial transmission , it is available on the subscription - based encrypted services of nova and cosmote tv . skai tv is also a member of digea , a consortium of private television networks introducing digital terrestrial transmission in greece . at launch , skai tv opted for dubbing all foreign language content into greek , instead of using subtitles . this is very uncommon in greece for anything except documentaries ( using voiceover dubbing ) and children 's programmes ( using lip - synced dubbing ) , so after intense criticism the station switched to using subtitles for almost all foreign shows . \",\n",
       " 'pair': [['piraeus', 'greece'],\n",
       "  ['skai group', 'greece'],\n",
       "  ['athens', 'greece'],\n",
       "  ['skai tv', 'greece']],\n",
       " 'relation': 'country'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'conrad oberon johnson ( november 15 , 1915 – february 3 , 2008 ) was an american music educator , long associated with the city of houston , who was inducted into the texas bandmasters hall of fame in 2000 . born in victoria , texas , conrad johnson was nine when his family moved to houston . following studies at yates high school , he attended houston college for negroes and graduated from wiley college . he was an active member of omega psi phi fraternity . he started his career in music education in 1941 and , following a thirty - seven - year career , retired from his position at kashmere high school in 1978 , but continued to remain active in shaping music in houston by conducting summer programs and in - home tutoring . johnson was a proficient musician in his own right and , at one point , played with count basie . erskine hawkins tried to convince him to join his orchestra , but johnson declined , citing a love of teaching and obligations to his family . later , johnson made his lasting contribution to music by forming the kashmere stage band , a renowned school orchestra that won a number of awards during its decade - long run . the conrad o. johnson school of fine arts at kashmere high school is named after him . conrad o. johnson died in houston days after his former students staged a celebration in his honor . the gala saturday night concert , which was filmed by a documentary crew , was described by the students as \" the greatest 92nd birthday gift that he could have ever requested . \" ',\n",
       " 'head': [['conrad oberon johnson'],\n",
       "  ['conrad johnson'],\n",
       "  ['conrad o. johnson'],\n",
       "  ['johnson']],\n",
       " 'tail': [['american']],\n",
       " 'head_first': 1,\n",
       " 'relation': 'P27'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the rel_info\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)\n",
    "\n",
    "rel2id = {v: k for k, v in rel_info.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 12275\n"
     ]
    }
   ],
   "source": [
    "# post processing for the outputs w ner\n",
    "# (source, target, relation)\n",
    "# (2, 1, relation)\n",
    "pairs = []\n",
    "count = 0\n",
    "for output in outputs:\n",
    "    # if the output doesn't end with \"<|endoftext|>\", find the lastest \";\" of the output and only take the previous part\n",
    "    try:\n",
    "        source = output.split(\"between source \")[1].strip()\n",
    "        source = source.split(\" and target \")[0].strip()\n",
    "        source = source.split(\"entity\")[1].strip()\n",
    "\n",
    "        target = output.split(\" and target \")[1].strip()\n",
    "        target = target.split(\" is \")[0].strip()\n",
    "        target = target.split(\"entity\")[1].strip()\n",
    "\n",
    "        relation = output.split(\" is \")[-1].strip()\n",
    "        relation = relation.split(\". <|endoftext|>\")[0].strip()\n",
    "\n",
    "        try:\n",
    "            relation = rel2id[relation]\n",
    "        except:\n",
    "            count += 1\n",
    "            pass\n",
    "\n",
    "        pairs.append((source, target, relation))\n",
    "    except:\n",
    "        pairs.append((\"none\", \"none\", \"none\"))\n",
    "\n",
    "print(f\"{count} / {len(outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"output\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for i, output in enumerate(pairs):\n",
    "    result['output'].append(output)\n",
    "    if dataset[i]['head_first'] == 1:\n",
    "        result['label'].append(('1', '2', dataset[i]['relation']))\n",
    "    else:\n",
    "        result['label'].append(('2', '1', dataset[i]['relation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the result dictionary\n",
    "# import pickle\n",
    "# with open(\"DocRED/GPT_w_ner/result/epoch_5_result.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing for the outputs without ner\n",
    "pairs = []\n",
    "for output in outputs:\n",
    "    pair = []\n",
    "    # if the output doesn't end with \"<|endoftext|>\", find the lastest \";\" of the output and only take the previous part\n",
    "    if output.endswith(\"<|endoftext|>\"):\n",
    "        string = output\n",
    "\n",
    "        for line in string.split(\";\"):\n",
    "            try:\n",
    "                source = line.split(\"the source is\")[1].strip()\n",
    "                source = source.split(\"and the target is\")[0].strip()\n",
    "                if source.startswith(\"none\"):\n",
    "                    source = \"none\"\n",
    "                    target = \"none\"\n",
    "                    continue\n",
    "                target = line.split(\"the target is\")[1].strip()\n",
    "                if target.endswith(\". <|endoftext|>\"):\n",
    "                    target = target.split(\". <|endoftext|>\")[0].strip()\n",
    "                    \n",
    "                if (source, target) not in pair:\n",
    "                    pair.append((source, target))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    else:\n",
    "        string = output.split(\";\")[:-1]\n",
    "        string = [line.strip() for line in string]\n",
    "        for line in string:\n",
    "            try:\n",
    "                source = line.split(\"the source is\")[1].strip()\n",
    "                source = source.split(\"and the target is\")[0].strip()\n",
    "\n",
    "                target = line.split(\"the target is\")[1].strip()\n",
    "                if (source, target) not in pair:\n",
    "                    pair.append((source, target))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    pairs.append(pair)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('piraeus', 'greece'), ('athens metropolitan area', 'greece')],\n",
       " [('skai tv', 'athens')],\n",
       " [('skai tv', 'skai tv')],\n",
       " [],\n",
       " [('hampshire county', 'united states'),\n",
       "  ('west virginia route 28', 'united states'),\n",
       "  ('william washington house', 'united states'),\n",
       "  ('north romney', 'united states'),\n",
       "  ('washington place homestead', 'united states'),\n",
       "  ('ralph w. haines', 'united states')],\n",
       " [('hampshire county', 'united states'),\n",
       "  ('west virginia', 'united states'),\n",
       "  ('virginia route 28', 'west virginia'),\n",
       "  ('washington place homestead', 'west virginia'),\n",
       "  ('ralph w. haines', 'west virginia'),\n",
       "  ('washington place', 'west virginia')],\n",
       " [('united states', 'hampshire county'),\n",
       "  ('united states', 'west virginia'),\n",
       "  ('united states', 'virginia'),\n",
       "  ('united states', 'north romney'),\n",
       "  ('united states', 'washington place homestead')],\n",
       " [('william washington house', 'united states'),\n",
       "  ('susan blue parsons', 'united states'),\n",
       "  ('william washington', 'united states'),\n",
       "  ('william washington place', 'united states')],\n",
       " [],\n",
       " [('ibm research – brazil', 'ibm')],\n",
       " [('ibm research – brazil', 'june 2010')],\n",
       " [('ibm research – brazil', 'ibm')],\n",
       " [('south america', 'europe')],\n",
       " [('ibm research – brazil', 'south america'),\n",
       "  ('são paulo', 'south america'),\n",
       "  ('rio de janeiro', 'south america'),\n",
       "  ('ibm', 'south america')],\n",
       " [('ulisses mello', 'ibm research'), ('ulisses mello', 'ibm')],\n",
       " [('são paulo', 'south america'), ('rio de janeiro', 'south america')],\n",
       " [('south america', 'south america')],\n",
       " [],\n",
       " [('lookin ass', 'detail'), ('lookin ass nigga', 'young money entertainment')],\n",
       " [('nicki minaj', 'the united states')],\n",
       " [('lookin ass', 'nicki minaj')],\n",
       " [('lookin ass', 'february 14, 2014'), ('lookin ass', 'march 11, 2014')],\n",
       " [],\n",
       " [('conrad oberon johnson', 'texas')],\n",
       " [('conrad oberon johnson', 'wiley college'),\n",
       "  ('conrad o. johnson', 'yates high school')],\n",
       " [('conrad johnson', 'american')],\n",
       " [('conrad oberon johnson', 'november 15, 1915')],\n",
       " [('conrad oberon johnson', 'february 3, 2008')],\n",
       " [('texas', 'american'),\n",
       "  ('houston college for negroes', 'american'),\n",
       "  ('yates high school', 'american'),\n",
       "  ('wiley college', 'american'),\n",
       "  ('omega psi', 'american'),\n",
       "  ('kashmere high school', 'american'),\n",
       "  ('kashmere theatre', 'american')],\n",
       " [('american', 'texas')],\n",
       " [('texas', 'american')],\n",
       " [('conrad oberon johnson', 'american')],\n",
       " [],\n",
       " [],\n",
       " [('samsung galaxy s9', 'south korea'),\n",
       "  ('samsung galaxy s9', 'june of the july 2014'),\n",
       "  ('samsung galaxy s9', 'south korean')],\n",
       " [('samsung', 'samsung electronics'), ('samsung', 'samsung galaxy s devices')],\n",
       " [('samsung galaxy s9', 'samsung')],\n",
       " [('samsung galaxy s9', 'samsung galaxy s'),\n",
       "  ('samsung galaxy s', 'samsung galaxy s')],\n",
       " [('south korea', 'samsung')],\n",
       " [('edward p. \" ned\" mcevoy', '1886')],\n",
       " [('edward p. \" ned\" mcevoy', 'abbeyleix')],\n",
       " [('edward p. \" ned\" mcevoy', 'irish')],\n",
       " [('dublin', 'irish'), ('county laois', 'irish'), ('laois', 'irish')],\n",
       " [('dublin', 'county laois'), ('dublin', 'laois'), ('county laois', 'irish')],\n",
       " [('ireland', 'county laois'), ('ireland', 'dublin')],\n",
       " [],\n",
       " [('portland golf club', 'the united states'),\n",
       "  ('portland', 'the united states'),\n",
       "  ('oregon', 'the united states'),\n",
       "  ('raleigh hills', 'the united states'),\n",
       "  ('eastern washington county', 'the united states')],\n",
       " [('portland golf club', 'oregon'), ('portland', 'oregon')],\n",
       " [('portland golf club', 'summer of 1914')],\n",
       " [('oregon', 'portland'), ('united states', 'oregon')],\n",
       " [],\n",
       " [('samarqand', 'uzbekistan'), ('urgut district', 'uzbekistan')],\n",
       " [('samarqand', 'uzbekistan'),\n",
       "  ('urgut district', 'uzbekistan'),\n",
       "  ('urgut', 'uzbekistan')],\n",
       " [('samarqand', 'urgut district')],\n",
       " [('uzbekistan', 'samarqand'), ('uzbekistan', 'urgut district')],\n",
       " [('samarqand', 'uzbekistan'), ('samarqand', 'uzbek')],\n",
       " [('samarqand', 'urgut district')],\n",
       " [('uzbekistan', 'tajik')],\n",
       " [],\n",
       " [('robert kingsbury huntington', 'los angeles')],\n",
       " [('robert kingsbury huntington', 'united states navy'),\n",
       "  ('robert kingsbury huntington', 'marine corps'),\n",
       "  ('robert kingsbury huntington', 'u.s. navy')],\n",
       " [('robert kingsbury huntington', 'battle of midway')],\n",
       " [('robert kingsbury huntington', '5 june 1942')],\n",
       " [('robert kingsbury huntington', 'marine'),\n",
       "  ('robert kingsbury huntington', 'united states navy')],\n",
       " [('robert kingsbury huntington', '13 march 1921')],\n",
       " [('vt-8', 'u.s. navy'), ('vt-8', 'marine air support')],\n",
       " [('battle of midway', '4–5 june 1942')],\n",
       " [('battle of midway', '4–5 june 1942')],\n",
       " [('battle of midway', '4–5 june 1942')],\n",
       " [],\n",
       " [('more', 'sisters of mercy'),\n",
       "  ('vision thing', 'sisters of mercy'),\n",
       "  ('we are', 'sisters of mercy'),\n",
       "  ('we are', 'meat loaf')],\n",
       " [('more', 'vision thing')],\n",
       " [('more', '15 december 1990')],\n",
       " [('more', 'andrew eldritch')],\n",
       " [],\n",
       " [('delaware', 'u.s.')],\n",
       " [('state of delaware', 'u.s.'),\n",
       "  ('delaware house of representatives', 'u.s.'),\n",
       "  ('delaware senate', 'u.s.'),\n",
       "  ('delaware', 'u.s.'),\n",
       "  ('delaware senate', 'delaware')],\n",
       " [('u.s.', 'delaware general assembly')],\n",
       " [('u.s.', 'delaware')],\n",
       " [('delaware', 'u.s.'),\n",
       "  ('delaware', 'delaware legislature'),\n",
       "  ('delaware', 'delaware'),\n",
       "  ('delaware house of representatives', 'delaware'),\n",
       "  ('delaware senate', 'delaware')],\n",
       " [],\n",
       " [('u.s.', 'ohio'), ('ohio', 'lima'), ('ohio', 'allen county')],\n",
       " [('allen county', 'ohio')],\n",
       " [('allen county', 'u.s.'),\n",
       "  ('ohio', 'u.s.'),\n",
       "  ('lima', 'u.s.'),\n",
       "  ('battle of frenchtown', 'u.s.')],\n",
       " [],\n",
       " [],\n",
       " [('allen county', 'u.s.'),\n",
       "  ('john allen', 'u.s.'),\n",
       "  ('ethan allen', 'u.s.'),\n",
       "  ('allen', 'u.s.')],\n",
       " [],\n",
       " [('allen county', '1820')],\n",
       " [],\n",
       " [('new york', 'american'), ('palestinian ministry of culture', 'new york')],\n",
       " [('new york', 'palestinian')],\n",
       " [('republic of china', 'taiwan')],\n",
       " [('republic of china', 'republic of china')],\n",
       " [('taipei', 'republic of china'), ('taiwan', 'republic of china')],\n",
       " [('taiwan', 'republic of china'),\n",
       "  ('taiwanese expatriates and descendants', 'republic of china'),\n",
       "  ('taiwanese expatriates', 'republic of china')],\n",
       " [('republic of china', 'republic of china')],\n",
       " [],\n",
       " [('antonio', 'september 30, 1869')],\n",
       " [('allen francis moore', 'august 18, 1945')],\n",
       " [('antonio', 'u.s.')],\n",
       " [('antonio', 'st. charles, kane county')],\n",
       " [('allen francis moore', 'lombard college'),\n",
       "  ('allen francis moore', 'university of illinois')],\n",
       " [('antonio', 'republican')],\n",
       " [('antonio', 'monticello')],\n",
       " [('u.s.', 'illinois'),\n",
       "  ('u.s.', 'kane county'),\n",
       "  ('u.s.', 'piatt county'),\n",
       "  ('u.s.', 'galesburg'),\n",
       "  ('u.s.', 'lombard college')],\n",
       " [('illinois', 'u.s.'),\n",
       "  ('piatt county', 'illinois'),\n",
       "  ('lombard college', 'illinois'),\n",
       "  ('university of illinois', 'illinois'),\n",
       "  ('university of illinois', 'galesburg')],\n",
       " [('illinois', 'u.s.'),\n",
       "  ('st. charles', 'u.s.'),\n",
       "  ('piatt county', 'u.s.'),\n",
       "  ('lombard college', 'u.s.'),\n",
       "  ('university of illinois', 'u.s.')],\n",
       " [],\n",
       " [],\n",
       " [('royal swedish academy of sciences', 'swedish')],\n",
       " [('johan gottlieb gahn', 'swedish')],\n",
       " [('johan gottlieb gahn', 'royal swedish academy of sciences')],\n",
       " [('johan gottlieb gahn', '19 august 1745')],\n",
       " [('johan gottlieb gahn', '8 december 1818')],\n",
       " [('johan gottlieb gahn', 'carl wilhelm scheele')],\n",
       " [('tsardom', 'russian empire'),\n",
       "  ('bordered china', 'russian empire'),\n",
       "  ('archangelgorod governorate', 'russian empire'),\n",
       "  ('siberia governorate', 'russian empire'),\n",
       "  ('moscow', 'russian empire')],\n",
       " [('bordered china', 'russian empire'),\n",
       "  ('kazan governorate', 'russian empire'),\n",
       "  ('archangelgorod governorate', 'russian empire'),\n",
       "  ('siberia governorate', 'russian empire'),\n",
       "  ('siberia', 'russian empire')],\n",
       " [('siberia governorate', '1708')],\n",
       " [],\n",
       " [('bordered china', 'arctic'), ('russian empire', 'arctic')],\n",
       " [],\n",
       " [('western australia', 'australia'),\n",
       "  ('albany', 'western australia'),\n",
       "  ('perth', 'western australia'),\n",
       "  ('quokka', 'western australia'),\n",
       "  ('setonix brachyurus', 'western australia')],\n",
       " [('australia', 'western australia'),\n",
       "  ('australia', 'perth'),\n",
       "  ('western australia', 'albany')],\n",
       " [],\n",
       " [('dollar general corporation', 'goodlettsville'),\n",
       "  ('j.l. turner', 'scottsville')],\n",
       " [('goodlettsville', 'united states'),\n",
       "  ('tennessee', 'united states'),\n",
       "  ('idaho', 'united states'),\n",
       "  ('washington', 'united states'),\n",
       "  ('kentucky', 'united states'),\n",
       "  ('cal turner', 'united states'),\n",
       "  ('fortune 500', 'united states')],\n",
       " [('dollar general corporation', '1939')],\n",
       " [('goodlettsville', 'tennessee'),\n",
       "  ('tennessee', 'the united states'),\n",
       "  ('idaho', 'the united states'),\n",
       "  ('washington', 'the united states'),\n",
       "  ('montana', 'the united states')],\n",
       " [('the united states', 'tennessee'), ('the united states', 'washington')],\n",
       " [('dollar general corporation', 'j.l. turner'),\n",
       "  ('j.l. turner', 'dollar general corporation')],\n",
       " [('haqqani', 'pakistan'), ('husain haqqani', 'pakistan')],\n",
       " [],\n",
       " [('mullen memo', 'pakistan'), ('mullen memo', 'pakistani')],\n",
       " [],\n",
       " [],\n",
       " [('live at town hall', '2002'),\n",
       "  ('the united states live', '2002'),\n",
       "  ('big science', '2001'),\n",
       "  ('bright red', '1986'),\n",
       "  ('united states live', '1986'),\n",
       "  ('the dream before', '1986')],\n",
       " [('live at town hall', 'laurie anderson'),\n",
       "  ('the dream before', 'laurie anderson'),\n",
       "  ('the strange angel', 'laurie anderson'),\n",
       "  ('o superman', 'laurie anderson'),\n",
       "  ('big science', 'laurie anderson')],\n",
       " [('laurie anderson', 'nonesuch records'),\n",
       "  ('live', 'nonesuch records'),\n",
       "  ('the dream before', 'nonesuch records'),\n",
       "  ('the strange angel', 'nonesuch records'),\n",
       "  ('o superman', 'nonesuch records')],\n",
       " [],\n",
       " [('mount pleasant township', 'westmoreland county'),\n",
       "  ('mount pleasant township', 'pennsylvania')],\n",
       " [('mount pleasant township', 'united states'),\n",
       "  ('westmoreland county', 'united states'),\n",
       "  ('pennsylvania', 'united states'),\n",
       "  ('calumet', 'united states'),\n",
       "  ('norvelt cdp', 'united states')],\n",
       " [('pennsylvania', 'westmoreland county'), ('united states', 'pennsylvania')],\n",
       " [('jack gantos', 'mount pleasant')],\n",
       " [],\n",
       " [('julian reinard', '5 march 1983')],\n",
       " [('julian reinard', 'fc wil')],\n",
       " [('besliga', 'bundesliga'),\n",
       "  ('german bundesliga', 'bundesliga'),\n",
       "  ('german bundesliga', 'sc freiburg'),\n",
       "  ('german bundesliga', 'bayern munich'),\n",
       "  ('german bundesliga', 'fc wil')],\n",
       " [('julian reinard', 'german')],\n",
       " [('bundesliga', 'german'),\n",
       "  ('german bundesliga', 'german'),\n",
       "  ('german', 'german')],\n",
       " [],\n",
       " [('china', 'vietnam'),\n",
       "  ('china', 'vietnamese'),\n",
       "  ('beibu gulf', 'vietnam'),\n",
       "  ('beibu gulf', 'vietnamese')],\n",
       " [('beibu gulf', 'china'),\n",
       "  ('beibu gulf', 'chinese'),\n",
       "  ('guangxi', 'china'),\n",
       "  ('guangxi', 'beibu gulf'),\n",
       "  ('hainan', 'china'),\n",
       "  ('hainan', 'chinese')],\n",
       " [('beibu gulf', 'china'), ('guangxi', 'china'), ('hainan', 'china')],\n",
       " [('china', 'vietnamese')],\n",
       " [('china', 'guangxi'), ('china', 'hainan')],\n",
       " [],\n",
       " [('houshang seyhoun', 'august 22, 1920')],\n",
       " [('houshang seyhoun', 'may 26, 2014')],\n",
       " [('houshang seyhoun', 'iranian')],\n",
       " [('houshang seyhoun', 'école nationale supérieure des beaux - arts')],\n",
       " [('houshang seyhoun', 'university of tehran')],\n",
       " [('école nationale supérieure des beaux - arts', 'iranian'),\n",
       "  ('university of tehran', 'iranian'),\n",
       "  ('central railway station', 'iranian'),\n",
       "  ('tombs of scientific/literation figures', 'iranian'),\n",
       "  ('hall of mirrors', 'iranian'),\n",
       "  ('mausoleum', 'iranian')],\n",
       " [('iranian', 'paris')],\n",
       " [('university of tehran', 'paris')],\n",
       " [],\n",
       " [('university of tehran', 'paris')],\n",
       " [('houshang seyhoun', 'paris')],\n",
       " [],\n",
       " [('charlottesville', 'commonwealth of virginia'),\n",
       "  ('charlottesville metropolitan statistical area',\n",
       "   'commonwealth of virginia')],\n",
       " [('commonwealth of virginia', 'charlottesville'),\n",
       "  ('commonwealth of virginia',\n",
       "   'charlottesville metropolitan statistical area')],\n",
       " [('thomas jefferson', 'albemarle county'),\n",
       "  ('thomas jefferson', 'charlottesville')],\n",
       " [],\n",
       " [],\n",
       " [('kurt tucholsky', '9 january 1890')],\n",
       " [('kurt tucholsky', 'berlin')],\n",
       " [('kurt tucholsky', 'berlin')],\n",
       " [('kurt tucholsky', '21 december 1935')],\n",
       " [('kurt tucholsky', 'berlin - moabit')],\n",
       " [('berlin-moabit', 'sweden'),\n",
       "  ('sweden', 'weimar republic'),\n",
       "  ('berlin-moabit', 'weimar republic')],\n",
       " [],\n",
       " [],\n",
       " [('yorkshire post', 'england'),\n",
       "  ('yorkshire', 'england'),\n",
       "  ('sheffield', 'england')],\n",
       " [('yorkshire post', 'england'),\n",
       "  ('yorkshire', 'united kingdom'),\n",
       "  ('sheffield', 'united kingdom'),\n",
       "  ('sheffield', 'england')],\n",
       " [('england', 'yorkshire')],\n",
       " [],\n",
       " [('johnston press', 'united kingdom'),\n",
       "  ('james mitchinson', 'united kingdom')],\n",
       " [('yorkshire post', '1754')],\n",
       " [('yorkshire post', '1754')],\n",
       " [('michelle chia', 'singapore'),\n",
       "  ('bryan wong', 'singapore'),\n",
       "  ('mark lee', 'singapore'),\n",
       "  ('bryan wong', 'malaysia')],\n",
       " [('michelle chia', 'penang')],\n",
       " [],\n",
       " [('wilhelm lanzky-otto', 'danish')],\n",
       " [('wilhelm lanzky-otto', 'january 30, 1909')],\n",
       " [('wilhelm lanzky-otto', 'april 13, 1991')],\n",
       " [('royal danish orchestra', 'royal danish symphony orchestra')],\n",
       " [],\n",
       " [('lunéville', 'france'),\n",
       "  ('lunéville', 'french'),\n",
       "  ('nancy', 'france'),\n",
       "  ('duchy of lorraine', 'france'),\n",
       "  ('duchy of lorraine', 'french')],\n",
       " [('lunéville', 'france'),\n",
       "  ('lunéville', 'duchy of lorraine'),\n",
       "  ('lunéville', 'french'),\n",
       "  ('lunéville', 'duke léopold i')],\n",
       " [],\n",
       " [('france', 'lunéville'), ('france', 'nancy')],\n",
       " [('jeanne thérèse du han de martigny',\n",
       "   'catherine françoise de roquefeuil de puydebar'),\n",
       "  ('jeanne thérèse du han de martigny', 'duke léopold i'),\n",
       "  ('jeanne thérèse du han de martigny', 'duchess and regent of lorraine')],\n",
       " [('jeanne thérèse du han de martigny', '30 october 1707')],\n",
       " [('jeanne thérèse du han de martigny', 'lunéville')],\n",
       " [('jeanne thérèse du han de martigny', '7 may 1748')],\n",
       " [('jeanne thérèse du han de martigny', 'lunéville')],\n",
       " [('jeanne thérèse du han de martigny', 'philippe-louis du han de martigny')],\n",
       " [('duke léopold i', 'france'),\n",
       "  ('duchess', 'france'),\n",
       "  ('duke léopold i', 'french'),\n",
       "  ('louis-philippe', 'france')],\n",
       " [('jeanne thérèse du han de martigny', 'philippe-louis du han de martigny')],\n",
       " [],\n",
       " [('london', 'björn ulvaeus'), ('london', 'benny andersson')],\n",
       " [('björn ulvaeus', 'london')],\n",
       " [('time', 'björn ulvaeus')],\n",
       " [('time', '1983'), ('the seeker', '1983')],\n",
       " [('time', 'b. a. robertson')],\n",
       " [('kim nam-ji', 'south korean')],\n",
       " [('mbc', 'south korean')],\n",
       " [('housewives', '2009')],\n",
       " [],\n",
       " [],\n",
       " [('housewives', 'kim nam-ji'), ('my wife is a superwoman', 'kim nam-ji')],\n",
       " [('housewives', 'mbc'), ('my wife is a superwoman', 'mbc')],\n",
       " [('housewives', 'south korean')],\n",
       " [],\n",
       " [('lycoming county', 'pennsylvania'),\n",
       "  ('loyalsock state forest', 'pennsylvania'),\n",
       "  ('old loggers path', 'lycoming county')],\n",
       " [('lycoming county', 'the united states'),\n",
       "  ('loyalsock state forest', 'the united states'),\n",
       "  ('old loggers path', 'the united states')],\n",
       " [('pennsylvania', 'lycoming county'), ('the united states', 'pennsylvania')],\n",
       " [],\n",
       " [('grand wing servo-tech co.', '1993')],\n",
       " [('grand wing servo-tech co.', 'city of industry'),\n",
       "  ('gws', 'city of industry')],\n",
       " [('u.s.', 'california')],\n",
       " [('china', 'american'), ('taiwan', 'american')],\n",
       " [],\n",
       " [('city of industry', 'california'),\n",
       "  ('city of industry', 'taiwan'),\n",
       "  ('taiwanese', 'u.s.'),\n",
       "  ('taiwanese', 'china')],\n",
       " [('city of industry', 'u.s.'),\n",
       "  ('city of industry', 'china'),\n",
       "  ('taiwan', 'u.s.'),\n",
       "  ('china', 'u.s.'),\n",
       "  ('taiwan', 'american'),\n",
       "  ('china', 'american')],\n",
       " [('north american', 'china')],\n",
       " [],\n",
       " [('paul charles morphy', 'june 22, 1837')],\n",
       " [('paul charles morphy', 'july 10, 1884')],\n",
       " [('paul charles morphy', 'new orleans')],\n",
       " [('paul charles morphy', 'the united states'),\n",
       "  ('johann löwenthal', 'the united states')],\n",
       " [('new orleans', 'the united states'), ('new orleans', 'american')],\n",
       " [('new orleans', 'the united states')],\n",
       " [('paul charles morphy', 'european'),\n",
       "  ('johann löwenthal', 'european'),\n",
       "  ('alexander meek', 'european'),\n",
       "  ('louis paulsen', 'european'),\n",
       "  ('alexander meek', 'american')],\n",
       " [],\n",
       " [('lane county', 'oregon'),\n",
       "  ('oregon', 'united states'),\n",
       "  ('oregon route 36', 'oregon'),\n",
       "  ('mountain pass high pass', 'oregon'),\n",
       "  ('low pass', 'oregon')],\n",
       " [('lane county', 'united states'),\n",
       "  ('oregon', 'united states'),\n",
       "  ('blachly', 'united states'),\n",
       "  ('oregon route 36', 'united states'),\n",
       "  ('mountain pass high pass', 'united states'),\n",
       "  ('low pass', 'united states')],\n",
       " [('oregon', 'lane county'), ('united states', 'oregon')],\n",
       " [('united states board of geographic names', 'oregon')],\n",
       " [],\n",
       " [('south sudan', 'east - central africa'), ('south sudan', 'south sudan')],\n",
       " [('south sudan', 'east - central africa'),\n",
       "  ('south sudan', 'south sudan – landlocked country')],\n",
       " [('south sudan', '9 july 2011')],\n",
       " [('south sudan', 'united nations'),\n",
       "  ('south sudan', 'south sudan – landlocked country')],\n",
       " [('juba', 'south sudan')],\n",
       " [('south sudan', 'east - central africa'), ('south sudan', 'south sudan')],\n",
       " [('juba', 'south sudan')],\n",
       " [],\n",
       " [('pointr', 'london')],\n",
       " [],\n",
       " [('ojai', 'the united states'),\n",
       "  ('california town', 'the united states'),\n",
       "  ('california', 'the united states')],\n",
       " [('ojai', 'the united states'),\n",
       "  ('california town', 'the united states'),\n",
       "  ('california', 'the united states')],\n",
       " [('university of ojai', 'october 1924')],\n",
       " [('the united states', 'ojai')],\n",
       " [('john joseph cantwell', 'united states')],\n",
       " [],\n",
       " [('sweetheart', 'february 10, 2004')],\n",
       " [('courtney love', 'virgin records')],\n",
       " [('sweetheart', 'courtney love'), ('sweetheart', 'emilie autumn')],\n",
       " [('courtney love', 'james barber')],\n",
       " [('sweetheart', 'american')],\n",
       " [('courtney love', 'american')],\n",
       " [],\n",
       " [('new england pharmacy industry', 'the united states'),\n",
       "  ('warwick', 'the united states'),\n",
       "  ('rhode island', 'the united states'),\n",
       "  ('new england', 'the united states'),\n",
       "  ('victoria', 'the united states'),\n",
       "  ('vermont', 'the united states'),\n",
       "  ('new hampshire', 'the united states')],\n",
       " [],\n",
       " [('new england pharmacy industry', 'brooks pharmacy'),\n",
       "  ('new england pharmacy industry', 'rox awards regional chain of the year')],\n",
       " [('warwick', 'rhode island'),\n",
       "  ('rhode island', 'new england'),\n",
       "  ('new york', 'the united states'),\n",
       "  ('victoria', 'the united states')],\n",
       " [('the united states', 'rhode island'), ('the united states', 'new york')],\n",
       " [],\n",
       " [('chinese martial arts', 'february 18, 2016')],\n",
       " [('chinese', 'american')],\n",
       " [('chinese martial arts', 'yuen woo-pai')],\n",
       " [('chinese martial arts', 'yuen woo-pai'),\n",
       "  ('chinese martial arts', 'john fusco')],\n",
       " [],\n",
       " [('chinese martial arts', 'donnie yen'),\n",
       "  ('chinese martial arts', 'jason scott lee')],\n",
       " [],\n",
       " [],\n",
       " [('bicycle & tricycles', 'orb'),\n",
       "  ('bicycle & tricycles', 'vocod'),\n",
       "  ('bicycle & tricycles', 'rapped'),\n",
       "  ('bicycle & tricycle', 'rapped')],\n",
       " [('jimmy cauty', 'rage')],\n",
       " [('london', 'united kingdom'), ('the guardian', 'united kingdom')],\n",
       " [],\n",
       " [('each time you break my heart', 'nick kamen'),\n",
       "  ('true blue', 'nick kamen'),\n",
       "  ('true blue', 'stephen bray')],\n",
       " [('nick kamen', 'sire records')],\n",
       " [('each time you break my heart', '1987'),\n",
       "  ('true blue', '1986'),\n",
       "  ('true blue', '1987'),\n",
       "  ('the bee gees', '1987')],\n",
       " [('each time you break my heart', 'nick kamen')],\n",
       " [('each time you break my heart', 'nick kamen')],\n",
       " [],\n",
       " [('unesco', 'paris')],\n",
       " [],\n",
       " [('unesco confucius prize for literacy', '2005')],\n",
       " [],\n",
       " [('mexico', 'texas'), ('mexico', 'mexican')],\n",
       " [('texas', 'united states'),\n",
       "  ('mexico', 'united states'),\n",
       "  ('central and south america', 'united states'),\n",
       "  ('south america', 'united states'),\n",
       "  ('indeterminate', 'texas'),\n",
       "  ('indeterminate', 'mexico')],\n",
       " [('texas', 'united states'),\n",
       "  ('mexico', 'united states'),\n",
       "  ('central and south america', 'united states'),\n",
       "  ('indeterminate', 'united states')],\n",
       " [('chachalacas', 'ortalis')],\n",
       " [],\n",
       " [('bbc one', 'bbc'), ('cbc one', 'bbc')],\n",
       " [('bbc one', 'uk'), ('s4c', 'uk')],\n",
       " [('bbc', 'bbc one')],\n",
       " [('bbc one', 'uk'), ('s4c', 'uk'), ('cbc one', 'uk')],\n",
       " [('cbc one', 'bbc'), ('bbc one', 'bbc'), ('s4c', 'bbc')],\n",
       " [('bbc one', 'bbc')],\n",
       " [('cbc one', 'bbc'), ('s4c', 'bbc')],\n",
       " [('bbc one', '2012')],\n",
       " [('bbc one', 'bbc one')],\n",
       " [],\n",
       " [('james de alwis', 'colombo academy')],\n",
       " [('james de alwis', '1823')],\n",
       " [('james de alwis', '1878')],\n",
       " [('james de alwis', 'louis de silva'),\n",
       "  ('louis de silva', 'james de alwis'),\n",
       "  ('james de alwis', 'frederick nell'),\n",
       "  ('frederick nell', 'james de alwis'),\n",
       "  ('frederick nell', 'louis de silva')],\n",
       " [('legislative council', 'egypt')],\n",
       " [('james de alwis', 'us')],\n",
       " [],\n",
       " [],\n",
       " [('levi', 'levi'), ('levi', 'aaronids')],\n",
       " [('levi', 'levi')],\n",
       " [('levi', 'israelite')],\n",
       " [('anthony noghès', 'monte carlo')],\n",
       " [('anthony noghès', '13 september 1890')],\n",
       " [('anthony noghès', '2 august 1978')],\n",
       " [('monte carlo', 'the united states')],\n",
       " [('monaco grand prix', '1913')],\n",
       " [],\n",
       " [],\n",
       " [('gina rinehart', 'rosario magdalena teresita lacson')],\n",
       " [('perth', 'victoria'), ('melbourne', 'victoria')],\n",
       " [('australia', 'victoria')],\n",
       " [('gina rinehart', 'rosario magdalena teresita lacson'),\n",
       "  ('rosario magdalena teresita lacson', 'rosario magdalena teresita'),\n",
       "  ('gina rinehart', 'rosario magdalena teresita')],\n",
       " [('rosario magdalena teresita lacson', '26 october 1948')],\n",
       " [('gina rinehart', 'rosario magdalena teresita lacson')],\n",
       " [],\n",
       " [('south africa', 'cape town'),\n",
       "  ('united states', 'kwazulu natal'),\n",
       "  ('united kingdom', 'free state')],\n",
       " [('cape town', 'south africa'),\n",
       "  ('kwazulu natal', 'south africa'),\n",
       "  ('free state', 'south africa')],\n",
       " [('cape town', 'kwazulu natal'), ('kwazulu natal', 'free state')],\n",
       " [('think bike campaign', 'tony day of cape town')],\n",
       " [('coimbra', 'portugal'), ('portugal', 'portugal')],\n",
       " [('portugal', 'portugal')],\n",
       " [],\n",
       " [('portugal', 'coimbra')],\n",
       " [('will weng', 'terre haute')],\n",
       " [('weng', 'columbia university school of journalism')],\n",
       " [],\n",
       " [('will weng', 'manhattan')],\n",
       " [('will weng', 'may 2, 1993')],\n",
       " [('weng', 'the united states')],\n",
       " [('will weng', 'february 25, 1907')],\n",
       " [],\n",
       " [('new york city', 'the united states'),\n",
       "  ('indiana state teachers college', 'the united states'),\n",
       "  ('the columbia university', 'the united states'),\n",
       "  ('new york times', 'the united states'),\n",
       "  ('the united states navy', 'the united states')],\n",
       " [('indiana state teachers college', 'new york city')],\n",
       " [('indiana', 'manhattan')],\n",
       " [],\n",
       " [('texas', 'united states'),\n",
       "  ('calhoun county', 'texas'),\n",
       "  ('olivia beach', 'texas'),\n",
       "  ('alamo beach', 'texas'),\n",
       "  ('magnolia beach', 'texas'),\n",
       "  ('olivia', 'texas')],\n",
       " [('texas', 'united states'),\n",
       "  ('calhoun county', 'united states'),\n",
       "  ('port lavaca', 'united states'),\n",
       "  ('linnville', 'united states'),\n",
       "  ('olivia beach', 'united states'),\n",
       "  ('alamo beach', 'united states'),\n",
       "  ('magnolia beach', 'united states')],\n",
       " [('texas', 'calhoun county'), ('united states', 'texas')],\n",
       " [('great raid', '1840')],\n",
       " [('angleton high school', 'texas'),\n",
       "  ('brazoria county', 'united states'),\n",
       "  ('angleton high school', 'brazoria county'),\n",
       "  ('angleton independent school district', 'texas')],\n",
       " [('angleton high school', 'united states'),\n",
       "  ('brazoria county', 'united states'),\n",
       "  ('angleton independent school district', 'united states')],\n",
       " [('texas', 'brazoria county'), ('united states', 'texas')],\n",
       " [],\n",
       " [('new zealand', 'new zealand'),\n",
       "  ('hollywood', 'new zealand'),\n",
       "  ('miramar', 'new zealand'),\n",
       "  ('wellington', 'new zealand')],\n",
       " [],\n",
       " [('wellington', 'wellington')],\n",
       " [],\n",
       " [('the cure', 'july 1989'), ('the cure', '1989')],\n",
       " [('the cure', 'the cure')],\n",
       " [],\n",
       " [('the cure', 'uk'), ('the cure', 'us')],\n",
       " [('london', 'uk')],\n",
       " [('the cure', 'the cure')],\n",
       " [],\n",
       " [],\n",
       " [('ontario', 'canadian')],\n",
       " [('battle of vimy ridge', '1917')],\n",
       " [('world war i', 'world war')],\n",
       " [('james llewellyn \"jimmy\" frise', '16 october 1891')],\n",
       " [('james llewellyn \"jimmy\" frise', '1948')],\n",
       " [('james llewellyn \"jimmy\" frise', 'scugog island')],\n",
       " [('james llewellyn \"jimmy \" frise', 'world war i'),\n",
       "  ('greg clark', 'world war i')],\n",
       " [],\n",
       " [('new york city', 'the united states'),\n",
       "  ('philadelphia', 'the united states'),\n",
       "  ('minneapolis', 'the united states'),\n",
       "  ('washington', 'the united states')],\n",
       " [],\n",
       " [],\n",
       " [('der nachsommer', '1972')],\n",
       " [('der nachsommer', 'valerio zurlini')],\n",
       " [('der nachsommer', 'alain delon'),\n",
       "  ('le professeur', 'valerio zurlini'),\n",
       "  ('the prima notte di quiete', 'alain delon')],\n",
       " [('der nachsommer', 'usa')],\n",
       " [('le professeur', 'valerio zurlini')],\n",
       " [],\n",
       " [('liffey', 'ireland'), ('liffey', 'us')],\n",
       " [],\n",
       " [('ireland', 'wexford'), ('ireland', 'dublin')],\n",
       " [],\n",
       " [('pokémon : the series', 'pokémon : the series'),\n",
       "  ('pokémon : the series', 'the anime')],\n",
       " [('pokémon : the series', 'nintendo')],\n",
       " [('pokémon : the series', 'japan')],\n",
       " [('pokémon : the series', 'japanese')],\n",
       " [('pokémon', 'pikachu')],\n",
       " [('pikachu', 'pokémon'), ('olm', 'pokémon')],\n",
       " [('pokémon : the series', '2018')],\n",
       " [],\n",
       " [('paris', 'france')],\n",
       " [],\n",
       " [('paris', 'france'),\n",
       "  ('paris', 'french'),\n",
       "  ('london', 'french'),\n",
       "  ('swingles', 'france')],\n",
       " [('swingles', 'france')],\n",
       " [('swingles', 'claude germain'),\n",
       "  ('swingles', 'jean cussac'),\n",
       "  ('swingles', 'claudine meunier')],\n",
       " [('swingles', '1962')],\n",
       " [('swingles', 'paris')],\n",
       " [],\n",
       " [('rogaland county municipality', 'norway'),\n",
       "  ('rogaland county', 'norway'),\n",
       "  ('norway', 'norway'),\n",
       "  ('stavanger', 'rogaland county municipality')],\n",
       " [('rogaland county municipality', 'norway'),\n",
       "  ('rogaland county', 'norway'),\n",
       "  ('norway', 'norway'),\n",
       "  ('stavanger', 'norway'),\n",
       "  ('berlin', 'norway')],\n",
       " [('rogaland county', 'bergen county municipality'),\n",
       "  ('norway', 'rogaland county municipality')],\n",
       " [('terje halleland', 'conservative party')],\n",
       " [('terje halleland', 'norway'),\n",
       "  ('janne johnsen', 'norway'),\n",
       "  ('harald thune', 'norway')],\n",
       " [],\n",
       " [('roberto león díaz gallo', 'monserrat school')],\n",
       " [('roberto león díaz gallo', 'santiago del estero')],\n",
       " [('roberto león díaz gallo', '29 june 1782')],\n",
       " [('pedro león díaz gallo', '7 february 1852')],\n",
       " [('pedro león díaz gallo', 'argentina'),\n",
       "  ('pedro miguel aráoz', 'argentina'),\n",
       "  ('josé andrés pacheco de melo', 'argentina')],\n",
       " [('congress of tucumán', 'argentina'),\n",
       "  ('tucumán', 'argentine'),\n",
       "  ('congress of tucumán', 'argentine')],\n",
       " [('argentina', 'buenos aires'), ('argentina', 'santiago del estero')],\n",
       " [('congress of tucumán', 'argentina'), ('tucumán', 'argentine')],\n",
       " [('congress of tucumán', 'argentina')],\n",
       " [('argentina', 'congress of tucumán')],\n",
       " [('congress of tucumán', '9 july 1816')],\n",
       " [('argentina', 'pedro león díaz gallo')],\n",
       " [('giacomo casanova', 'silvia balletti')],\n",
       " [('manon balletti', '1740')],\n",
       " [('manon balletti', '1776')],\n",
       " [('giacomo casanova', 'manon balletti')],\n",
       " [],\n",
       " [('robert plant', 'led zeppelin'), ('jimmy page', 'led zeppelin')],\n",
       " [('achilles last stand', 'robert plant')],\n",
       " [('achilles last stand', 'led zeppelin'),\n",
       "  ('achilles last stand', 'lead zeppelin'),\n",
       "  ('presence', 'led zeppelin'),\n",
       "  ('kashmir', 'led zeppelin'),\n",
       "  ('presence', 'lead zeppelin'),\n",
       "  ('kashmir', 'lead zeppelin')],\n",
       " [('led zeppelin', 'jimmy page'), ('led zeppelin', 'robert plant')],\n",
       " [('achilles last stand', 'robert plant')],\n",
       " [('achilles last stand', 'march 1976')],\n",
       " [],\n",
       " [('olof mörck', '12 december 1981')],\n",
       " [('olof mörck', 'gothenburg')],\n",
       " [('ethan mörck', 'sweden')],\n",
       " [('ferdin mörck', 'amaranthe'), ('ferdin mörck', 'metal band amaranthe')],\n",
       " [('gothenburg', 'sweden'), ('sweden', 'sweden')],\n",
       " [('the history of saints', 'vanisher')],\n",
       " [('the history of saints', '2010')],\n",
       " [('amaranthe', 'mörck')],\n",
       " [],\n",
       " [('the united states', 'theodore roosevelt')],\n",
       " [('the united states', 'theodore roosevelt'),\n",
       "  ('the united states', 'the american')],\n",
       " [],\n",
       " [('russo-japanese war', 'russo-japanese')],\n",
       " [('theodore roosevelt', 'the united states')],\n",
       " [('american', 'the united states')],\n",
       " [('russo-japanese war', '1905')],\n",
       " [('spanish', 'spanish')],\n",
       " [],\n",
       " [],\n",
       " [('central anatolian province', 'turkey'),\n",
       "  ('ankara', 'turkey'),\n",
       "  ('kutais', 'turkey'),\n",
       "  ('makk', 'turkey'),\n",
       "  ('vakıflar bankası', 'turkey'),\n",
       "  ('haslisan', 'turkey'),\n",
       "  ('roketsan', 'turkey')],\n",
       " [('central anatolian province', 'ankara')],\n",
       " [],\n",
       " [('roketsan', 'ankara')],\n",
       " [('roketsan', '1988')],\n",
       " [('turkey', 'ankara')],\n",
       " [],\n",
       " [('samuel c. brightman', 'poland')],\n",
       " [('samuel c. brightman', 'world war ii')],\n",
       " [('samuel c. brightman', 'medal of honor')],\n",
       " [('samuel c. brightman', '1911')],\n",
       " [('samuel c. brightman', '1992')],\n",
       " [('american', 'world war ii')],\n",
       " [('samuel c. brightman', 'the washington post')],\n",
       " [('doy marshall', '29 november 1951')],\n",
       " [('venus beauty institute', 'lady annie marshall')],\n",
       " [('venus beauty institute', 'willie marshall')],\n",
       " [],\n",
       " [('supreme court', 'united states')],\n",
       " [('jackie washington', 'united states'),\n",
       "  ('john marshall harlan ii', 'united states'),\n",
       "  ('j.s. v. illinois', 'united states'),\n",
       "  ('taylor v. illinois', 'united states')],\n",
       " [('washington v. texas', 'united states'),\n",
       "  ('taylor v. illinois', 'united states'),\n",
       "  ('federal courts', 'united states'),\n",
       "  ('supreme court', 'united states')],\n",
       " [('texas', 'the united states'), ('federal courts', 'the united states')],\n",
       " [('united states', 'texas')],\n",
       " [],\n",
       " [('elias brown', 'u.s.'),\n",
       "  ('john quincy adams', 'u.s.'),\n",
       "  ('richard rush', 'u.s.')],\n",
       " [('elias brown', 'baltimore')],\n",
       " [('emil brown', 'baltimore')],\n",
       " [('elias brown', 'jacksonian')],\n",
       " [('emil brown', 'july 7, 1857')],\n",
       " [('elias brown', 'may 9, 1793')],\n",
       " [('u.s.', 'maryland')],\n",
       " [('baltimore', 'maryland'),\n",
       "  ('maryland house of delegates', 'maryland'),\n",
       "  ('maryland senate', 'maryland')],\n",
       " [('baltimore', 'u.s.'),\n",
       "  ('maryland house of delegates', 'u.s.'),\n",
       "  ('state constitutional convention', 'u.s.'),\n",
       "  ('eldersburg', 'u.s.')],\n",
       " [('maryland house of delegates', 'maryland')],\n",
       " [],\n",
       " [('tanzania', 'kebwe steven kebwe')],\n",
       " [('tanzania', 'morogoro region'), ('tanzania', 'tanzania')],\n",
       " [('tanzania', 'tanzania'),\n",
       "  ('lindi', 'tanzania'),\n",
       "  ('iringa', 'tanzania'),\n",
       "  ('uringa', 'tanzania'),\n",
       "  ('pwani', 'tanzania')],\n",
       " [('tanzania', 'tanzania')],\n",
       " [],\n",
       " [('moscow', 'russia'),\n",
       "  ('moscow', 'russian'),\n",
       "  ('metropolitan of moscow', 'russia'),\n",
       "  ('metropolitan of moscow', 'russian')],\n",
       " [],\n",
       " [('alejandro sanz', 'wea latina'),\n",
       "  ('juanes', 'wea latina'),\n",
       "  ('shakira', 'wea latina'),\n",
       "  ('antonio carmona', 'wea latina')],\n",
       " [('alejandro sanz', 'spanish'), ('la llave de mi corazón', 'spanish')],\n",
       " [('alejandro sanz', 'november 7, 2006'), ('la llave de mi corazón', '2008')],\n",
       " [('grammy award for best latin pop album', 'miami')],\n",
       " [('national film registry of the library of congress', 'american')],\n",
       " [('united states', 'george wallace')],\n",
       " [('united states', 'alabama')],\n",
       " [('george wallace', 'united states'),\n",
       "  ('john f. kennedy', 'united states'),\n",
       "  ('george wallace', 'american broadcasting company')],\n",
       " [('national film registry of the library of congress', 'american')],\n",
       " [('crisis : behind a presidential commitment', '1963')],\n",
       " [('crisis : behind a presidential commitment', 'robert drew'),\n",
       "  ('the schoolhouse door', 'robert drew'),\n",
       "  ('the home of united states attorney general', 'robert f. kennedy'),\n",
       "  ('the president', 'robert f. kennedy'),\n",
       "  ('the president', 'george wallace')],\n",
       " [('crisis : behind a presidential commitment', 'united states')],\n",
       " [('crisis : behind a presidential commitment',\n",
       "   'american broadcasting company')],\n",
       " [('national film registry of the library of congress',\n",
       "   'american broadcasting company')],\n",
       " [('kim won-yong', 'south korean'),\n",
       "  ('wonyong sung', 'south korean'),\n",
       "  ('jung won-yong', 'south korean'),\n",
       "  ('kang won-yong', 'south korean')],\n",
       " [('kang won-yong', '1917'),\n",
       "  ('kim won-yong', '1922'),\n",
       "  ('wonyong sung', '1917'),\n",
       "  ('jung won-yong', '1917')],\n",
       " [('kim won-yong', '1993'), ('jung won-yong', '2006')],\n",
       " [('south korean', 'south korean')],\n",
       " [('kang won-yong', 'south korean')],\n",
       " [],\n",
       " [('loud tour', '2010')],\n",
       " [('loud tour', 'rihanna')],\n",
       " [('o2 arena', 'london')],\n",
       " [('o2 arena', 'united kingdom')],\n",
       " [],\n",
       " [('kalidas jayaram', 'parvathy'), ('malavika jayaram', 'parvathy')],\n",
       " [('kalidas jayaram', 'parvathy'), ('malavika jayaram', 'parvathy')],\n",
       " [],\n",
       " [],\n",
       " [('amrutham gambaya', 'balachandra menon'),\n",
       "  ('amrutham gaan', 'balachandra menon')],\n",
       " [('amrutham gamaya', 'parvathy'),\n",
       "  ('amrutham guha', 'parvathy'),\n",
       "  ('vivahithare ithile', 'parvathy'),\n",
       "  ('malayalam', 'parvathy')],\n",
       " [('amrutham gamaya', '1986'),\n",
       "  ('amrutham guha', '1988'),\n",
       "  ('vivahithare ithile', '1986'),\n",
       "  ('ponmuttayidunna tharavu', '1988')],\n",
       " [('amrutham gamaya', 'malayalam'),\n",
       "  ('amrutham gujarat', 'malayalam'),\n",
       "  ('vivahithare ithile', 'malayalam'),\n",
       "  ('amrutham gambaya', 'malayalam'),\n",
       "  ('ponmuttayidunna tharavu', 'malayalam')],\n",
       " [],\n",
       " [('north rhine-westphalia', 'germany'),\n",
       "  ('bergisches land', 'germany'),\n",
       "  ('wuppertal', 'germany'),\n",
       "  ('rhineland', 'germany')],\n",
       " [('bergisches land', 'rhine-westphalia'),\n",
       "  ('rhine-westphalia', 'germany'),\n",
       "  ('rhine-westphalia', 'north rhine-westphalia'),\n",
       "  ('wuppertal', 'rhine-westphalia')],\n",
       " [('germany', 'north rhine-westphalia'), ('germany', 'rhine-westphalia')],\n",
       " [],\n",
       " [('barry town', 'england'),\n",
       "  ('tottenham hotspur', 'england'),\n",
       "  ('tottenham hotspur', 'english'),\n",
       "  ('tottenham hotspur', 'british')],\n",
       " [('renard davies', 'england'),\n",
       "  ('kevin muscat', 'england'),\n",
       "  ('gareth naven', 'england'),\n",
       "  ('gavin postecoglou', 'england'),\n",
       "  ('ross aloisi', 'england')],\n",
       " [('ren davies', 'barry town'),\n",
       "  ('ren davies', 'england'),\n",
       "  ('kevin muscat', 'melbourne'),\n",
       "  ('ross aloisi', 'melbourne'),\n",
       "  ('gareth naven', 'melbourne')],\n",
       " [('arren davies', '13 august 1978')],\n",
       " [],\n",
       " [('alko kuli khan khattak', 'pakistani'), ('pervez musharraf', 'pakistani')],\n",
       " [('pakistan', 'benazir bhutto')],\n",
       " [('udu', 'pakistan'),\n",
       "  ('national defence university', 'pakistan'),\n",
       "  ('pakistani', 'pakistan')],\n",
       " [],\n",
       " [('peder anker', 'norway'),\n",
       "  ('peder anker', 'norwegian'),\n",
       "  ('norwegian', 'norway'),\n",
       "  ('norwegian', 'norwegian')],\n",
       " [('riksakten', 'norway'),\n",
       "  ('norwegian constitution', 'norway'),\n",
       "  ('norwegian constitution', 'norwegian')],\n",
       " [('norwegian parliament', 'norway')],\n",
       " [('riksakten', '1814')],\n",
       " [('peder anker', 'norwegian parliament')],\n",
       " [('kim jong-un', 'american')],\n",
       " [],\n",
       " [],\n",
       " [('the guardian', 'american')],\n",
       " [('american', 'kim jong-un')],\n",
       " [],\n",
       " [],\n",
       " [('ernesto pérez balladares', 'september 28, 2009')],\n",
       " [('carlos duque', 'may 12, 1936')],\n",
       " [('ernesto pérez balladares', 'u.s.'),\n",
       "  ('martín torrijos', 'u.s.'),\n",
       "  ('ernesto pérez balladares', 'democratic'),\n",
       "  ('ricardo martinelli', 'u.s.'),\n",
       "  ('martín torrijos', 'democratic')],\n",
       " [('ernesto pérez balladares', 'panama'),\n",
       "  ('ernesto pérez balladares', 'democratic revolutionary party')],\n",
       " [('national assembly', 'panama')],\n",
       " [('panama', 'the united states'),\n",
       "  ('national assembly', 'the united states'),\n",
       "  ('national assembly', 'argentina')],\n",
       " [('panama', 'arnulfo arias'),\n",
       "  ('panama', 'martín torrijos'),\n",
       "  ('panama', 'ernesto pérez balladares')],\n",
       " [('u.s.', 'national assembly')],\n",
       " [('ernesto pérez balladares', 'panama')],\n",
       " [('ganggang thüne', '8 october 1949')],\n",
       " [('kristian thüne', 'germany')],\n",
       " [('karlgang thüne', '1972 summer olympics')],\n",
       " [('wolfgang thüne', 'east germany'),\n",
       "  ('eberhard gienger', 'tus 04 leverkusen')],\n",
       " [('germany', 'bern')],\n",
       " [('joseph octave mousseau', 'berthier')],\n",
       " [('joseph octave mousseau', 'louis mousseau'),\n",
       "  ('joseph octave mousseau', 'joseph octave mousseau')],\n",
       " [('joseph octave mousseau', 'april 25, 1844')],\n",
       " [('joseph octave mousseau', 'december 13, 1898')],\n",
       " [('louis mousseau', 'joseph octave mousseau')],\n",
       " [('house of commons of canada', 'quebec')],\n",
       " [('quebec', 'canada'), ('house of commons of canada', 'quebec')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('kwazulu', 'south africa'),\n",
       "  ('bophuthatswana', 'south africa'),\n",
       "  ('venda', 'south africa'),\n",
       "  ('kavangoland', 'south africa'),\n",
       "  ('transkei', 'south africa')],\n",
       " [('kwazulu', 'south africa'),\n",
       "  ('kavangoland', 'south africa'),\n",
       "  ('transkei', 'south africa')],\n",
       " [],\n",
       " [('zambia', 'kenneth kaunda'), ('zambia', 'kenneth kordon')],\n",
       " [('frederick jacob titus chiluba', 'zambia'),\n",
       "  ('kenneth kaunda', 'zambia'),\n",
       "  ('kenneth kaunda', 'zambian'),\n",
       "  ('kenneth kordon', 'zambian')],\n",
       " [('frederick jacob titus chiluba', 'mmd'), ('kenneth kaunda', 'mmd')],\n",
       " [('frederick jacob titus chiluba', 'april 30, 1943')],\n",
       " [('frederick jacob titus chiluba', 'june 18, 2011')],\n",
       " [('zambia', 'zambian'),\n",
       "  ('labour party', 'zambian'),\n",
       "  ('national party', 'zambian'),\n",
       "  ('postal workers', 'zambian')],\n",
       " [],\n",
       " [('kundry', 'venjan')],\n",
       " [('bruno walter', 'sweden')],\n",
       " [('vienna musikverein', 'sweden')],\n",
       " [],\n",
       " [('met', 'sweden')],\n",
       " [('charleston county', 'south carolina'),\n",
       "  ('south carolina', 'united states'),\n",
       "  ('charleston', 'south carolina')],\n",
       " [('charleston county', 'united states'),\n",
       "  ('south carolina', 'united states'),\n",
       "  ('charleston', 'united states'),\n",
       "  ('north carolina', 'united states')],\n",
       " [('south carolina', 'charleston county'),\n",
       "  ('united states', 'south carolina')],\n",
       " [],\n",
       " [('joseph daniel gates', 'chicago white sox'),\n",
       "  ('joseph daniel gates', 'kansas city royals'),\n",
       "  ('don kessinger', 'kansas city royals')],\n",
       " [('joseph daniel gates', 'october 3, 1954')],\n",
       " [('joseph daniel gates', 'march 28, 2010')],\n",
       " [('chicago white sox', 'major league baseball'),\n",
       "  ('kansas city royals', 'major league baseball'),\n",
       "  ('eduardo rodriguez', 'major league baseball'),\n",
       "  ('don kessinger', 'major league baseball'),\n",
       "  ('greg pryor', 'major league baseball')],\n",
       " [],\n",
       " [('springfield', 'united states'),\n",
       "  ('illinois', 'united states'),\n",
       "  ('university of illinois at springfield', 'united states'),\n",
       "  ('wuis', 'united states'),\n",
       "  ('university of illinois system', 'united states'),\n",
       "  ('quincy', 'united states')],\n",
       " [('illinois', 'united states'),\n",
       "  ('university of illinois at springfield', 'illinois'),\n",
       "  ('university of illinois system', 'illinois'),\n",
       "  ('wuis', 'illinois')],\n",
       " [('illinois', 'illinois'), ('united states', 'illinois')],\n",
       " [('wuis', 'january 3, 1975')],\n",
       " [],\n",
       " [('wuis', 'university of illinois at springfield'),\n",
       "  ('wuis', 'university of illinois system')],\n",
       " [],\n",
       " [('cumbria', 'united kingdom'),\n",
       "  ('holme abbey', 'united kingdom'),\n",
       "  ('holme st. cuthbert', 'united kingdom'),\n",
       "  ('aldoth', 'united kingdom'),\n",
       "  ('three - quarters of a mile', 'united kingdom'),\n",
       "  ('tarns', 'united kingdom')],\n",
       " [('cumbria', 'united kingdom'),\n",
       "  ('holme abbey', 'cumbria'),\n",
       "  ('aldoth', 'cumbria')],\n",
       " [],\n",
       " [('mike hope', 'republican')],\n",
       " [],\n",
       " [('mike hope', 'american'),\n",
       "  ('jay inslee', 'american'),\n",
       "  ('doug roulstone', 'american')],\n",
       " [('washington house of representatives', 'american'),\n",
       "  ('mill creek city council', 'american'),\n",
       "  ('american', 'american'),\n",
       "  ('i-405 tolls', 'american'),\n",
       "  ('mill creek', 'american')],\n",
       " [('american', 'jay inslee')],\n",
       " [('washington house of representatives', 'washington')],\n",
       " [],\n",
       " [('u.s.', 'arizona'),\n",
       "  ('mexico', 'arizona'),\n",
       "  ('mexico', 'sonora'),\n",
       "  ('mexico', 'san bernardino national wildlife refuge')],\n",
       " [('arizona', 'u.s.'),\n",
       "  ('sonora', 'arizona'),\n",
       "  ('san bernardino national wildlife refuge', 'arizona'),\n",
       "  ('sonora', 'new mexico'),\n",
       "  ('arizona', 'mexico')],\n",
       " [('arizona', 'u.s.'),\n",
       "  ('arizona', 'mexico'),\n",
       "  ('sonora', 'u.s.'),\n",
       "  ('san bernardino national wildlife refuge', 'u.s.'),\n",
       "  ('san bernardino national wildlife refuge', 'mexico'),\n",
       "  ('sonora', 'mexico')],\n",
       " [],\n",
       " [('us airways group inc.', 'tempe')],\n",
       " [('us airways group inc.', 'us airways group'),\n",
       "  ('us airways', 'us airways group'),\n",
       "  ('piedmont airlines', 'us airways group'),\n",
       "  ('america west airlines', 'us airways group'),\n",
       "  ('us airways express', 'us airways group'),\n",
       "  ('us airways group international route network', 'us airways group')],\n",
       " [('us airways group inc.', 'us airways'),\n",
       "  ('us airways group inc.', 'america west airlines'),\n",
       "  ('us airways', 'us airways'),\n",
       "  ('america west airlines', 'us airways'),\n",
       "  ('american airlines', 'us airways')],\n",
       " [('us airways group inc.', 'us airways group inc.'),\n",
       "  ('us airways group', 'us airways group inc.'),\n",
       "  ('america west airlines', 'us airways group inc.'),\n",
       "  ('america west airlines', 'us airways'),\n",
       "  ('american airlines', 'us airways group inc.')],\n",
       " [('us airways group inc.', 'us'),\n",
       "  ('us airways group', 'us'),\n",
       "  ('us airways', 'us'),\n",
       "  ('america west airlines', 'us'),\n",
       "  ('us airways express', 'us'),\n",
       "  ('american airlines', 'us')],\n",
       " [('us airways group inc.', '2005')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('us airways group inc.', 'us'),\n",
       "  ('us airways', 'us'),\n",
       "  ('piedmont airlines', 'us'),\n",
       "  ('america west holdings corporation', 'us'),\n",
       "  ('us airways group', 'us'),\n",
       "  ('america west airlines', 'us')],\n",
       " [('ayumi hamasaki', 'japanese')],\n",
       " [('ayumi hamasaki', 'avex trax')],\n",
       " [('surreal', 'ayumi hamasaki'),\n",
       "  ('duty', 'ayumi hamasaki'),\n",
       "  ('dream of confusion', 'ayumi hamasaki')],\n",
       " [('surreal', 'ayumi hamasaki')],\n",
       " [('surreal', '2000'), ('duty', '2000')],\n",
       " [('surreal', 'max matsuura'),\n",
       "  ('dream', 'max matsuura'),\n",
       "  ('duty', 'max matsuura')],\n",
       " [('surreal', 'duty')],\n",
       " [('recording industry association of japan', 'japan')],\n",
       " [],\n",
       " [('england', 'europe'),\n",
       "  ('wye valley', 'europe'),\n",
       "  ('wales', 'europe'),\n",
       "  ('e.g. wyre forest', 'europe')],\n",
       " [('england', 'wye valley'), ('uk', 'england')],\n",
       " [('england', 'uk'), ('wye valley', 'england')],\n",
       " [('wye valley', 'england'), ('english', 'uk'), ('wye', 'england')],\n",
       " [('e.g. wyre forest', 'oecophorinae'),\n",
       "  ('wye valley', 'oecophorinae'),\n",
       "  ('hideer moth', 'oecophorinae')],\n",
       " [],\n",
       " [('temagami', 'ontario'),\n",
       "  ('ontario', 'canada'),\n",
       "  ('white bear lake', 'temagami'),\n",
       "  ('white bear forest', 'ontario'),\n",
       "  ('ottawa river', 'ontario')],\n",
       " [('temagami', 'canada'),\n",
       "  ('ontario', 'canada'),\n",
       "  ('white bear lake', 'canada'),\n",
       "  ('ottawa river', 'canada'),\n",
       "  ('lake temagami', 'canada')],\n",
       " [('canada', 'ontario')],\n",
       " [('ottawa river', 'lake ontario')],\n",
       " [('ontario', 'canada')],\n",
       " [('asean', '1997')],\n",
       " [('mahathir bin mohamad', 'republic of korea'),\n",
       "  ('dr. mahathir bin mohamad', 'japan'),\n",
       "  ('mahathir bin mohamad', 'china'),\n",
       "  ('mahathir bin mohamad', 'japan')],\n",
       " [('republic of korea', 'asean'), ('japan', 'asean')],\n",
       " [('east asia', 'china')],\n",
       " [],\n",
       " [],\n",
       " [('chile free trade agreement', 'australia'),\n",
       "  ('australia – chile free trade agreement', 'australia'),\n",
       "  ('australian', 'australia'),\n",
       "  ('chile', 'australia')],\n",
       " [('simon crean', 'australia'),\n",
       "  ('australia', 'australia'),\n",
       "  ('australian', 'australia'),\n",
       "  ('australian', 'australian')],\n",
       " [('australia', 'australian parliament')],\n",
       " [('australian parliament', 'australia')],\n",
       " [],\n",
       " [('susan j. pharr', 'march 16, 1944')],\n",
       " [('harvard university', 'japan'),\n",
       "  ('harvard university', 'japanese'),\n",
       "  ('american', 'japan')],\n",
       " [],\n",
       " [('connecticut', 'unitedicut'),\n",
       "  (\"saint paul's church\", 'connecticut'),\n",
       "  ('yale college', 'connecticut'),\n",
       "  ('yale college', 'new haven'),\n",
       "  ('yale baptist', 'connecticut')],\n",
       " [('chauncey bunce brewster', 'windham')],\n",
       " [('joseph brewster', 'yale college'), ('benjamin brewster', 'yale college')],\n",
       " [('chauncey bunce brewster', 'september 5, 1848')],\n",
       " [('chauncey bunce brewster', 'april 9, 1941')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('arnold school', 'english'),\n",
       "  ('fusilier museum', 'english'),\n",
       "  ('lancashire fusiliers', 'english'),\n",
       "  ('memorial hall', 'english'),\n",
       "  ('1st battalion', 'english')],\n",
       " [('john schofield vc', 'victoria cross'),\n",
       "  ('british army', 'victoria cross'),\n",
       "  ('lancashire fusiliers', 'victoria cross'),\n",
       "  ('2/5th battalion', 'victoria cross')],\n",
       " [('john schofield vc', '4 march 1892')],\n",
       " [('john schofield vc', '9 april 1918')],\n",
       " [('england', 'commonwealth')],\n",
       " [('john schofield vc', 'british army'),\n",
       "  ('john schofield vc', 'lancashire fusiliers')],\n",
       " [('john schofield vc', 'first world war'),\n",
       "  ('british army', 'first world war'),\n",
       "  ('lancashire fusiliers', 'first world war'),\n",
       "  ('1st battalion', 'first world war')],\n",
       " [],\n",
       " [('sara black', '1978')],\n",
       " [('sara black', 'university of chicago')],\n",
       " [('sara black', 'university of chicago')],\n",
       " [('sara black', 'american')],\n",
       " [('antioch college', 'american'),\n",
       "  ('university of chicago', 'american'),\n",
       "  ('evergreen state college', 'american'),\n",
       "  ('evergreen state college', 'chicago'),\n",
       "  ('university of wisconsin – eau claire', 'chicago'),\n",
       "  ('museum of contemporary art', 'chicago')],\n",
       " [],\n",
       " [('antioch college', 'chicago')],\n",
       " [],\n",
       " [('overseas press club of america', '1939')],\n",
       " [('new york city', 'american')],\n",
       " [('the overseas press club of america', 'new york city')],\n",
       " [('new york city', 'the united states'),\n",
       "  ('new york city', 'american'),\n",
       "  ('the wire service', 'the united states'),\n",
       "  ('the overseas press club of america', 'the united states'),\n",
       "  ('the wire service', 'american'),\n",
       "  ('the foreign correspondent', 'the united states')],\n",
       " [],\n",
       " [('university of colorado law school', 'u.s.'), ('ei', 'u.s.')],\n",
       " [('ei', 'university of colorado law school')],\n",
       " [('poland', 'lech wałęsa'), ('poland', 'tadeusz mazowiecki')],\n",
       " [('pruszków', 'poland'),\n",
       "  ('poland', 'canada'),\n",
       "  ('poland', 'canadian'),\n",
       "  ('x', 'poland')],\n",
       " [('lech wałęsa', 'poland'),\n",
       "  ('lech wałęsa', 'polish'),\n",
       "  ('lech wałęsa', 'canadian')],\n",
       " [('poland', 'lech wałęsa')],\n",
       " [('lech wałęsa', 'x'), ('lech wałęsa', 'polish')],\n",
       " [],\n",
       " [('arkisław \" stan \" tymiński', 'january 27, 1948')],\n",
       " [('arkisław \" stan \" tymiński', 'pruszków')],\n",
       " [],\n",
       " [],\n",
       " [('university of uyo', 'akwa ibom state'), ('university of uyo', 'nigeria')],\n",
       " [('university of uyo', 'nigeria')],\n",
       " [('university of uyo', 'october 1, 1991')],\n",
       " [('university of uyo', 'akwa ibom state')],\n",
       " [('nigeria', 'akwa ibom state')],\n",
       " [('university of uyo', 'akwa ibom state')],\n",
       " [('enefiok essien', 'nigeria')],\n",
       " [],\n",
       " [('warmian-masurian voivodeship', 'poland'),\n",
       "  ('szczytno', 'poland'),\n",
       "  ('gmina wielbark', 'poland')],\n",
       " [('poland', 'szczytno county'), ('poland', 'warmian-masurian voivodeship')],\n",
       " [('szczytno county', 'warmian-masurian voivodeship'),\n",
       "  ('szczytno', 'warmian-masurian voivodeship'),\n",
       "  ('gmina wielbark', 'szczytno county')],\n",
       " [('poland', 'poles'), ('gmina wielbark', 'poles'), ('szczytno', 'poles')],\n",
       " [],\n",
       " [('szczytno', 'warmian-masurian voivodeship')],\n",
       " [('szczytno', 'warmian-masurian voivodeship')],\n",
       " [],\n",
       " [('renato martino', 'vatican city'),\n",
       "  ('john xxiii', 'vatican city'),\n",
       "  ('paul vi', 'vatican city'),\n",
       "  ('john paul ii', 'vatican city'),\n",
       "  ('paul ii', 'vatican city')],\n",
       " [('vatican', 'benedict xvi'),\n",
       "  ('vatican', 'renato martino'),\n",
       "  ('vatican', 'renato raffaele cardinal martino')],\n",
       " [('john xxiii', 'vatican city'), ('john paul ii', 'vatican city')],\n",
       " [],\n",
       " [('ilipp brammer', 'munich')],\n",
       " [('ilipp brammer', '28 august 1969')],\n",
       " [('ilipp brammer', '28 august 1969')],\n",
       " [('ilipp brammer', 'german')],\n",
       " [('munich', 'german')],\n",
       " [('lindenstraße', 'jan günzel'),\n",
       "  ('jason priestley', 'jan günzel'),\n",
       "  ('jason priestley', 'aamir khan')],\n",
       " [],\n",
       " [],\n",
       " [('lec billing', 'the united states'),\n",
       "  ('canada', 'the united states'),\n",
       "  ('telus', 'the united states'),\n",
       "  ('bell', 'the united states'),\n",
       "  ('lec', 'the united states')],\n",
       " [],\n",
       " [('toyota aurelia', 'toyota'), ('toyota aurion', 'toyota')],\n",
       " [],\n",
       " [('toyota', 'the united states'),\n",
       "  ('kentucky', 'the united states'),\n",
       "  ('american', 'the united states')],\n",
       " [],\n",
       " [('the united states', 'kentucky')],\n",
       " [('dahod district', 'india'),\n",
       "  ('gujarat', 'india'),\n",
       "  ('dudhimati river', 'india'),\n",
       "  ('madhya pradesh', 'india'),\n",
       "  ('rajasthan', 'india')],\n",
       " [('india', 'gujarat'), ('india', 'dudhimati river')],\n",
       " [('india', 'narendra modi')],\n",
       " [('dahod district', 'gujarat'),\n",
       "  ('gujarat', 'india'),\n",
       "  ('dudhimati river', 'gujarat'),\n",
       "  ('dudhimati river', 'indian'),\n",
       "  ('madhya pradesh', 'india'),\n",
       "  ('rajasthan', 'india')],\n",
       " [('dahod', 'dahod')],\n",
       " [('dahod', '1618')],\n",
       " [('jahangir', 'india'),\n",
       "  ('jahangir', 'indian'),\n",
       "  ('narendra modi', 'india'),\n",
       "  ('narendra modi', 'indian')],\n",
       " [],\n",
       " [('kazimierz szosland', 'february 21, 1891')],\n",
       " [('kazimierz szosland', 'april 20, 1944')],\n",
       " [('kazimierz szosland', 'polish army')],\n",
       " [('kazimierz szosland', 'world war ii'),\n",
       "  ('ukraine', 'world war ii'),\n",
       "  ('soviet russia', 'world war ii'),\n",
       "  ('polish army', 'world war ii'),\n",
       "  ('pol', 'world war ii')],\n",
       " [('kazimierz szosland', 'grzymaczew')],\n",
       " [('kazimierz szosland', 'poland')],\n",
       " [('grzymaczew', 'poland'),\n",
       "  ('kalisz', 'poland'),\n",
       "  ('poland army', 'poland'),\n",
       "  ('wars with ukraine', 'poland'),\n",
       "  ('world war ii', 'poland'),\n",
       "  ('poland', 'poland')],\n",
       " [('world war ii', '1928 summer olympics')],\n",
       " [('kgalema motlanthe', 'african national congress')],\n",
       " [('kgalema motlanthe', 'south african')],\n",
       " [('national executive committee of the african national congress',\n",
       "   'kgalema motlanthe')],\n",
       " [('national executive committee of the african national congress',\n",
       "   'south african'),\n",
       "  ('south african constitution', 'south african')],\n",
       " [],\n",
       " [('makoto kamiya', 'japan')],\n",
       " [('capcom studios', 'japan')],\n",
       " [('leon s. kennedy', 'live-action'),\n",
       "  ('claire redfield', 'live-action'),\n",
       "  ('ron s. kennedy', 'live-action')],\n",
       " [('live-action', 'japan')],\n",
       " [('resident evil : degeneration', 'makoto kamiya')],\n",
       " [('live-action', 'capcom')],\n",
       " [('live-action', 'october 11, 2008')],\n",
       " [('live-action', 'capcom')],\n",
       " [('live-action', 'capcom')],\n",
       " [],\n",
       " [('live-action', 'capcom')],\n",
       " [],\n",
       " [('moscow', 'russian'),\n",
       "  ('smolensk', 'russian'),\n",
       "  ('zakhary petrovich lyapunov', 'russian'),\n",
       "  ('zakhary lyapunov', 'russian')],\n",
       " [('zakhary petrovich lyapunov', 'zakhary petrovich lyapunov')],\n",
       " [('zakhary petrovich lyapunov', 'zakhary petrovich lyapunov')],\n",
       " [('moscow', 'russian'), ('smolensk', 'russian')],\n",
       " [('russia', 'moscow')],\n",
       " [('zakhary petrovich lyapunov', 'russia'),\n",
       "  ('zakhary petrovich lyapunov', 'russian'),\n",
       "  ('sigismund iii vasa', 'russia'),\n",
       "  ('wladislaus', 'russia'),\n",
       "  ('sigismund iii vasa', 'russian')],\n",
       " [('zakhary petrovich lyapunov', 'russian')],\n",
       " [('zakhary petrovich lyapunov', 'prokopy lyapunov'),\n",
       "  ('zakhary lyapunov', 'prokopy lyapunov')],\n",
       " [('miami', 'the united states'),\n",
       "  ('florida', 'the united states'),\n",
       "  ('miami heart institute', 'miami'),\n",
       "  ('barry university', 'miami')],\n",
       " [('barry university school of podiatric medicine', '1985')],\n",
       " [('miami', 'the united states'),\n",
       "  ('florida', 'the united states'),\n",
       "  ('miami heart institute', 'the united states'),\n",
       "  ('barry university', 'the united states')],\n",
       " [('albert armstrong', 'the united states')],\n",
       " [('the united states', 'florida')],\n",
       " [],\n",
       " [('nikita vladimirovich bogstva', 'may 22, 1913')],\n",
       " [('nikita vladimirovich bogstva', 'saint-petersburg')],\n",
       " [('nikita vladimirovich bogstva', 'russian empire')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('moscow', 'russian empire'),\n",
       "  ('saint-petersburg', 'russian empire'),\n",
       "  ('moscow', 'soviet')],\n",
       " [('moscow', 'russian empire'), ('saint-petersburg', 'russian empire')],\n",
       " [('russia', 'moscow')],\n",
       " [('two fighter', '1943'),\n",
       "  ('the great patriotic war', '1943'),\n",
       "  ('order of the red banner of labour', '1971'),\n",
       "  ('order of the red star', '1946')],\n",
       " [('two fighter', 'dva boitsa'),\n",
       "  ('order of the red banner of labour', 'dva boitsa'),\n",
       "  ('order of the red star', 'dva boitsa')],\n",
       " [],\n",
       " [],\n",
       " [('los angeles county museum of art', 'los angeles'),\n",
       "  ('museum of contemporary art', 'los angeles county'),\n",
       "  ('the hamm museum', 'los angeles county'),\n",
       "  ('the museum of contemporary art', 'los angeles'),\n",
       "  ('the southon simon museum', 'los angeles county')],\n",
       " [('los angeles county museum of art', 'san francisco'),\n",
       "  ('museum of contemporary art', 'san francisco')],\n",
       " [('los angeles county museum of art', 'san francisco')],\n",
       " [],\n",
       " [('estêvão gomes', 'portuguese')],\n",
       " [('estêvão gomes', '1538')],\n",
       " [],\n",
       " [('estêvão gomes', 'portuguese')],\n",
       " [('portuguese', 'spanish'),\n",
       "  ('nova scotia', 'spanish'),\n",
       "  ('castile', 'spanish')],\n",
       " [],\n",
       " [],\n",
       " [('estevan gómez', 'portuguese')],\n",
       " [],\n",
       " [],\n",
       " [('u.s. health care reform : progress to date and next steps',\n",
       "   'the united states')],\n",
       " [('united states health care reform : progress to date and next steps',\n",
       "   'the united states'),\n",
       "  ('affordable care act', 'the united states'),\n",
       "  ('american medical association', 'the united states'),\n",
       "  ('altmetric', 'the united states')],\n",
       " [('the united states', 'barack obama')],\n",
       " [('barack obama', 'the united states')],\n",
       " [('united states health care reform : progress to date and next steps',\n",
       "   'the united states')],\n",
       " [('the united states',\n",
       "   'u.s. health care reform : progress to date and next steps')],\n",
       " [('oberliga rheinland - pfalz/maar', 'germany'),\n",
       "  ('oberliga südwest', 'germany')],\n",
       " [('oberliga rheinland - pfalz/maar', 'rhineland-palatinate'),\n",
       "  ('oberliga südwest', 'rhineland-palatinate'),\n",
       "  ('oberliga südwest', 'saarland'),\n",
       "  ('oberliga südwest', 'germany')],\n",
       " [('germany', 'rhineland-palatinate'),\n",
       "  ('germany', 'saarland'),\n",
       "  ('german', 'rhineland-palatinate')],\n",
       " [('oberliga rheinland - pfalz/ saar', '2008')],\n",
       " [],\n",
       " [('hamid karzai', 'afghanistan'), ('gulbuddin hekmatyar', 'afghanistan')],\n",
       " [('mcclatchy news service', 'afghanistan'), ('tolo', 'afghanistan')],\n",
       " [],\n",
       " [('second world war', 'first world war')],\n",
       " [],\n",
       " [('lebrun', '1917')],\n",
       " [(\"l'école des officiers de la gendarmerie nationale\", '1901')],\n",
       " [('avery fisher', 'u.s.')],\n",
       " [('avery fisher', 'avery fisher'),\n",
       "  ('charlie albright', 'avery fisher'),\n",
       "  ('joshua bell', 'avery fisher'),\n",
       "  ('demarre mcgill', 'avery fisher'),\n",
       "  ('anthony mcgill', 'avery fisher'),\n",
       "  ('heidi lehwalder', 'avery fisher')],\n",
       " [('avery fisher', '2004')],\n",
       " [],\n",
       " [('pathanamthitta district', 'kerala'),\n",
       "  ('kerala', 'india'),\n",
       "  ('pathanamthitta', 'kerala'),\n",
       "  ('kumplampoika', 'ranni'),\n",
       "  ('benglavkadavu', 'pathanamthitta district'),\n",
       "  ('ranni', 'pathanamthitta district')],\n",
       " [('pathanamthitta district', 'india'),\n",
       "  ('kerala', 'india'),\n",
       "  ('pathanamthitta', 'india'),\n",
       "  ('kumplampoika', 'india'),\n",
       "  ('benglavkadavu', 'india'),\n",
       "  ('ranni', 'india')],\n",
       " [('india', 'pathanamthitta district'), ('india', 'kerala')],\n",
       " [],\n",
       " [('upper reaches', 'philippines'),\n",
       "  ('nueva ecija', 'philippines'),\n",
       "  ('pantabangan dam', 'philippines'),\n",
       "  ('pantabangan', 'philippines'),\n",
       "  ('sierra madre', 'philippines')],\n",
       " [('nueva ecija', 'upper reaches of the pampanga river'),\n",
       "  ('philippines', 'nueva ecija'),\n",
       "  ('philippines', 'upper reaches of the caraballo'),\n",
       "  ('nueva vizcaya', 'nueva vizcaya'),\n",
       "  ('nueva vizcaya', 'pantabangan dam')],\n",
       " [('pantabangan river', 'nueva ecija'),\n",
       "  ('pantabangan dam', 'nueva vizcaya'),\n",
       "  ('pantabangan', 'nueva ecija'),\n",
       "  ('pantabangan', 'nueva vizcaya')],\n",
       " [('pantabangan river', 'carranglan river')],\n",
       " [],\n",
       " [('estuary', 'english channel')],\n",
       " [('england', 'devon')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('edmund hlawka', 'november 5, 1916')],\n",
       " [('edmund hlawka', 'february 19, 2009')],\n",
       " [('edmund hlawka', 'austrian')],\n",
       " [('edmund hlawka', 'university of vienna')],\n",
       " [('edmund hlawka', 'bruck an der mur')],\n",
       " [('edmund hlawka', 'vienna university of technology')],\n",
       " [('edmund hlawka', 'bruck an der mur')],\n",
       " [('bruck an der mur', 'styria')],\n",
       " [('bruck an der mur', 'austrian')],\n",
       " [('austria', 'bruck an der mur')],\n",
       " [('university of vienna', 'vienna')],\n",
       " [],\n",
       " [],\n",
       " [('the killers', 'charles bukowski'),\n",
       "  ('south of no north', 'charles bukowski')],\n",
       " [('the killers', 'charles bukowski')],\n",
       " [('the killers', '1973')],\n",
       " [('south of no north', 'john martin')],\n",
       " [],\n",
       " [('gypsy rose lee', 'american')],\n",
       " [],\n",
       " [('lady of burlesque', '1941')],\n",
       " [('lady of burlesque', 'gypsy rose lee'),\n",
       "  ('the g - string murders', 'gypsy rose lee')],\n",
       " [('g-string murders', 'american')],\n",
       " [],\n",
       " [('appenzell innerrhoden', 'switzerland'),\n",
       "  ('appenzell', 'canton of appenzell innerrhoden'),\n",
       "  ('schwende', 'switzerland'),\n",
       "  ('rüte', 'switzerland'),\n",
       "  ('appenzell', 'appenzell innerrhoden')],\n",
       " [('appenzell', 'switzerland'),\n",
       "  ('schwende', 'switzerland'),\n",
       "  ('rüte', 'switzerland'),\n",
       "  ('abbacella', 'switzerland')],\n",
       " [('switzerland', 'appenzell innerrhoden')],\n",
       " [],\n",
       " [('american airlines group inc.', 'fort worth')],\n",
       " [('american airlines group inc.', 'fort worth')],\n",
       " [('american airlines group inc.', 'december 9, 2013')],\n",
       " [('american airlines group inc.', 'us airways group'),\n",
       "  ('us airways group', 'us airways group')],\n",
       " [('american airlines group inc.', 'american'),\n",
       "  ('us airways group', 'american'),\n",
       "  ('american airlines group', 'american')],\n",
       " [('texas', 'american'),\n",
       "  ('fort worth', 'texas'),\n",
       "  ('us airways group', 'texas'),\n",
       "  ('us airways group', 'american')],\n",
       " [('american airlines group inc.', 'us airways group')],\n",
       " [('american', 'texas')],\n",
       " [('american airlines group inc.', 'december 9, 2013')],\n",
       " [('federal aviation administration', 'american')],\n",
       " [],\n",
       " [('fox', 'the united states')],\n",
       " [('burns verkaufen der kraftwerk', 'the united states')],\n",
       " [('fox', 'the united states')],\n",
       " [('burns verkaufen der kraftwerk', 'simpsons'),\n",
       "  ('burns verkaufkter', 'simpsons'),\n",
       "  ('burns verkaufkter', 'the simpsons')],\n",
       " [('burns verkaufen der kraftwerk', 'fox'), ('burns verkaufisher', 'fox')],\n",
       " [('burns verkaufen der kraftwerk', 'homer')],\n",
       " [('burns verkaufen der kraftwerk', 'jon vitti')],\n",
       " [('burns verkaufen der kraftwerk', 'mark kirkland')],\n",
       " [('homer', 'simpsons'),\n",
       "  ('burns verkaufen der kraftwerk', 'simpsons'),\n",
       "  ('burns verkaufisher', 'simpsons'),\n",
       "  ('burns verkauft das kraftwerk', 'simpsons'),\n",
       "  ('burns verkauft das kraftwerk', 'the simpsons')],\n",
       " [],\n",
       " [('ukraine', 'west ukraine'),\n",
       "  ('ukrainian', 'west ukraine'),\n",
       "  ('polish', 'west ukraine'),\n",
       "  ('polish', 'buchach'),\n",
       "  ('buchast', 'chernivtsi'),\n",
       "  ('buchast', 'drohobych'),\n",
       "  ('buchast', 'podolia')],\n",
       " [('west ukraine', 'ukraine'),\n",
       "  ('west ukraine', 'soviet union'),\n",
       "  ('pokuttia', 'soviet union'),\n",
       "  ('podolia', 'soviet union'),\n",
       "  ('chernivtsi', 'soviet union')],\n",
       " [('west ukraine', 'ukraine'),\n",
       "  ('west ukraine', 'buchach'),\n",
       "  ('west ukraine', 'chernivtsi'),\n",
       "  ('west ukraine', 'drohobych'),\n",
       "  ('west ukraine', 'podolia')],\n",
       " [('eastern ukraine', 'west ukraine'), ('west ukraine', 'west ukraine')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('blank page', 'christina aguilera'),\n",
       "  ('bionic and burlesque', 'christina aguilera'),\n",
       "  ('beautiful', 'christina aguilera'),\n",
       "  ('bicycle', 'christina aguilera'),\n",
       "  ('burlesque', 'christina aguilera')],\n",
       " [('blank page', '2012'),\n",
       "  ('bionic and burlesque', '2010'),\n",
       "  ('lotus', '2012'),\n",
       "  ('beautiful', '2002')],\n",
       " [('blank page', 'lotus')],\n",
       " [('bionic', 'burlesque'),\n",
       "  ('burlesque', 'bionic'),\n",
       "  ('lotus', 'bionic'),\n",
       "  ('lotus', 'bicycle')],\n",
       " [('bionic', 'burlesque'), ('burlesque', 'bionic'), ('lotus', 'bionic')],\n",
       " [],\n",
       " [('lan bo', 'april 16, 1965')],\n",
       " [('lan bo', 'harbin')],\n",
       " [('luan bo', 'chinese'),\n",
       "  ('yao bin', 'chinese'),\n",
       "  ('sui wenjing', 'chinese'),\n",
       "  ('han cong', 'chinese')],\n",
       " [('luan bo', 'world figure skating championships'),\n",
       "  ('yao bin', 'world figure skating championships'),\n",
       "  ('sui wenjing', 'world figure skating championships'),\n",
       "  ('han cong', 'world figure skating championships'),\n",
       "  ('zhu qiuying', 'world figure skating championships')],\n",
       " [('harbin', 'heilongjiang')],\n",
       " [('harbin', 'china'),\n",
       "  ('harbin', 'chinese'),\n",
       "  ('west germany', 'china'),\n",
       "  ('dortmund', 'china'),\n",
       "  ('2010 world figure skating championships', 'china'),\n",
       "  ('1984 winter olympics', 'china')],\n",
       " [('china', 'harbin')],\n",
       " [],\n",
       " [('1984 winter olympics', 'dortmund')],\n",
       " [('world figure skating championships', 'world')],\n",
       " [],\n",
       " [('newsnight', 'bbc')],\n",
       " [('kirsty wark', 'bbc'), ('emily maitlis', 'bbc')],\n",
       " [('bbc', 'bbc')],\n",
       " [('bbc television', 'bbc')],\n",
       " [('bbc television', 'bbc')],\n",
       " [('bbc', '11 july 2011')],\n",
       " [('newsnight', '1980')],\n",
       " [('leone marucci', 'march 28, 1973')],\n",
       " [('leone marucci', 'youngstown')],\n",
       " [('leone marucci', 'american')],\n",
       " [('the power of few', 'leone marucci')],\n",
       " [('the power of few', 'steelyard pictures')],\n",
       " [('the power of few', '2012')],\n",
       " [('the power of few', 'christopher walken'),\n",
       "  ('the power of many', 'jesse bradford'),\n",
       "  ('the power of few', 'jesse bradford')],\n",
       " [('the power of few', 'leone marucci')],\n",
       " [('youngstown', 'american'),\n",
       "  ('ohio', 'american'),\n",
       "  ('steelyard pictures', 'american'),\n",
       "  ('the power of few', 'american')],\n",
       " [('american', 'ohio')],\n",
       " [('the power of few', 'leone marucci')],\n",
       " [('the power of few', 'the power of many')],\n",
       " [('east kalimantan', 'east kalimantan')],\n",
       " [('east kalimantan', 'east kalimantan')],\n",
       " [('east kalimantan', 'indonesian'),\n",
       "  ('mahakam river', 'indonesian'),\n",
       "  ('pelabuhan indonesia', 'indonesian')],\n",
       " [('east kalimantan', 'east kalimantan'), ('indonesian', 'east kalimantan')],\n",
       " [('east kalimantan', 'east kalimantan')],\n",
       " [],\n",
       " [('slums of kingston', 'jamaican')],\n",
       " [('the kingston', '1954')],\n",
       " [],\n",
       " [('the kingston', 'jamaican roger mais')],\n",
       " [('jamaican roger mais', 'jamaican')],\n",
       " [],\n",
       " [('procter & gambler', 'procter & gambler')],\n",
       " [],\n",
       " [],\n",
       " [('νικόλαος αλιάγας', 'paris')],\n",
       " [('νικόλαος αλιάγας', 'greece'), ('νικόλαος αλιάκας', 'greece')],\n",
       " [('νικόλαος αλιάγας', 'greek')],\n",
       " [('νικόλαος αλιάγας', '13 may 1969')],\n",
       " [('star academy', 'greece')],\n",
       " [('paris', 'greece'), ('french', 'greece')],\n",
       " [('star academy', 'french')],\n",
       " [('paris', 'french')],\n",
       " [],\n",
       " [('playstation 2', 'playstation 2'),\n",
       "  ('microsoft windows', 'playstation 2'),\n",
       "  ('xbox live arcade', 'playstation 2'),\n",
       "  ('future good & evil 2', 'playstation 2'),\n",
       "  ('future good & evil 2', 'playstation 3'),\n",
       "  ('future good & evil 2', 'playstation 4')],\n",
       " [('beyond good & evil', 'ubisoft pictures')],\n",
       " [('beyond good & evil', 'ubisoft'),\n",
       "  ('the rayman series', 'ubisoft'),\n",
       "  ('future good & evil', 'ubisoft'),\n",
       "  ('future good & evil 2', 'ubisoft')],\n",
       " [('microsoft windows', 'microsoft'), ('microsoft windows', 'xbox')],\n",
       " [('beyond good & evil', '2003'), ('the rayman series', '2003')],\n",
       " [('jade', 'michel ancel')],\n",
       " [('beyond good & evil', 'michel ancel')],\n",
       " [],\n",
       " [('nicky ladanowski', 'uk')],\n",
       " [('coronation street', 'bbc'), ('loveable characters', 'bbc')],\n",
       " [('coronation street', 'uk'), ('my family', 'uk')],\n",
       " [('los angeles', 'uk'), ('bbc', 'uk')],\n",
       " [],\n",
       " [('ljiljana raičević', '29 june 1947')],\n",
       " [('ljiljana raičević', 'serbia'), ('ljiljana raičević', 'montenegro')],\n",
       " [('montenegro', 'parliament of montenegro')],\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"output\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for output, label in zip(pairs, dataset['pair']):\n",
    "    result['output'].append(output)\n",
    "    result['label'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the result dictionary\n",
    "# import pickle\n",
    "# with open(\"DocRED/GPT_without_ner/result/epoch_5_result.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['portland golf club', '1914']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['label'][48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('portland golf club', 'summer of 1914')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['output'][48]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"DocRED/GPT_w_ner/result/epoch_5_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length: 12275, 12275\n",
      "instance:\n",
      "('1', '2', 'P17')\n",
      "('1', '2', 'P17')\n"
     ]
    }
   ],
   "source": [
    "print(f'the length: {len(result[\"output\"])}, {len(result[\"label\"])}')\n",
    "print(f'instance:\\n{result[\"output\"][0]}\\n{result[\"label\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source and target, relation\n",
    "st_tp = 0\n",
    "st_fp = 0\n",
    "st_fn = 0\n",
    "st_tn = 0\n",
    "\n",
    "r_tp = 0\n",
    "r_fp = 0\n",
    "r_fn = 0\n",
    "r_tn = 0\n",
    "\n",
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    pair = False\n",
    "    relation = False\n",
    "    if output[0] == label[0] and output[1] == label[1]:\n",
    "        st_tp += 1\n",
    "        pair = True\n",
    "    else:\n",
    "        st_fn += 1\n",
    "        st_fp += 1\n",
    "    \n",
    "    if output[2] == label[2]:\n",
    "        r_tp += 1\n",
    "        relation = True\n",
    "    else:\n",
    "        r_fn += 1\n",
    "        r_fp += 1\n",
    "\n",
    "    if pair and relation:\n",
    "        tuple_tp += 1\n",
    "    else:\n",
    "        tuple_fn += 1\n",
    "        tuple_fp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source and target precision: 0.8511608961303462, recall: 0.8511608961303462, f1: 0.8511608961303462\n",
      "relation precision: 0.7165784114052953, recall: 0.7165784114052953, f1: 0.7165784114052953\n",
      "tuple precision: 0.6974338085539715, recall: 0.6974338085539715, f1: 0.6974338085539715\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for source and target\n",
    "st_precision = st_tp / (st_tp + st_fp)\n",
    "st_recall = st_tp / (st_tp + st_fn)\n",
    "st_f1 = 2 * st_precision * st_recall / (st_precision + st_recall)\n",
    "print(f\"source and target precision: {st_precision}, recall: {st_recall}, f1: {st_f1}\")\n",
    "\n",
    "# for relation\n",
    "r_precision = r_tp / (r_tp + r_fp)\n",
    "r_recall = r_tp / (r_tp + r_fn)\n",
    "r_f1 = 2 * r_precision * r_recall / (r_precision + r_recall)\n",
    "print(f\"relation precision: {r_precision}, recall: {r_recall}, f1: {r_f1}\")\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"DocRED/GPT_without_ner/result/epoch_5_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length: 6254, 6254\n",
      "instance:\n",
      "[('texas', 'american')]\n",
      "[['texas', 'american'], ['yates high school', 'houston']]\n"
     ]
    }
   ],
   "source": [
    "print(f'the length: {len(result[\"output\"])}, {len(result[\"label\"])}')\n",
    "print(f'instance:\\n{result[\"output\"][30]}\\n{result[\"label\"][30]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    for pair in output:\n",
    "        true_tuple = False\n",
    "        for label_pair in label:\n",
    "            if pair[0] == label_pair[0] and pair[1] == label_pair[1]:\n",
    "                tuple_tp += 1\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fp += 1\n",
    "    \n",
    "    for label_pair in label:\n",
    "        true_tuple = False\n",
    "        for pair in output:\n",
    "            if pair[0] == label_pair[0] and pair[1] == label_pair[1]:\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple precision: 0.26895140664961636, recall: 0.19856495468277946, f1: 0.22845970019552467\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple precision: 0.3231713554987212, recall: 0.24026467903863705, f1: 0.2756183745583039\n"
     ]
    }
   ],
   "source": [
    "# loosen the condition for the tp\n",
    "\n",
    "\n",
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    for pair in output:\n",
    "        true_tuple = False\n",
    "        for label_pair in label:\n",
    "            if (pair[0] in label_pair[0]) and (pair[1] in label_pair[1]):\n",
    "                tuple_tp += 1\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fp += 1\n",
    "    \n",
    "    for label_pair in label:\n",
    "        true_tuple = False\n",
    "        for pair in output:\n",
    "            if (pair[0] in label_pair[0]) and (pair[1] in label_pair[1]):\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fn += 1\n",
    "\n",
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioRED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
