{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_tokens = {'additional_special_tokens': ['[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 124439808 || all params: 124439808 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open('DocRED/data/train_annotated.json') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {\n",
    "    'id': [],\n",
    "    'text': [],\n",
    "    'head': [],\n",
    "    'tail': [],\n",
    "    'head_first': [],\n",
    "    'relation': [],\n",
    "    'head_start_pos' : [],\n",
    "    'tail_start_pos' : []\n",
    "}\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    sents = \"\"\n",
    "    for sent in train_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # relation_dict['text'].append(sents)\n",
    "\n",
    "    for relation_pair in train_set[i]['labels']:\n",
    "        relation_dict['id'].append(i)\n",
    "        relation_dict['text'].append(sents)\n",
    "        head = []\n",
    "        head_ = []\n",
    "        head_start_pos = []\n",
    "        head.append([[item['name'].lower()] for item in train_set[i]['vertexSet'][relation_pair['h']]])\n",
    "        for j, item in enumerate(head[0]):\n",
    "            if item not in head_:\n",
    "                head_.append(item)\n",
    "                head_start_pos.append(train_set[i]['vertexSet'][relation_pair['h']][j]['pos'][0])\n",
    "\n",
    "        relation_dict['head'].append(head_)\n",
    "        relation_dict['head_start_pos'].append(head_start_pos)\n",
    "\n",
    "        tail = []\n",
    "        tail_ = []\n",
    "        tail_start_pos = []\n",
    "        tail.append([[item['name'].lower()] for item in train_set[i]['vertexSet'][relation_pair['t']]])\n",
    "        for j, item in enumerate(tail[0]):\n",
    "            if item not in tail_:\n",
    "                tail_.append(item)\n",
    "                tail_start_pos.append(train_set[i]['vertexSet'][relation_pair['t']][j]['pos'][0])\n",
    "        relation_dict['tail'].append(tail_)\n",
    "        relation_dict['tail_start_pos'].append(tail_start_pos)\n",
    "\n",
    "        \n",
    "        if train_set[i]['vertexSet'][relation_pair['h']][0]['pos'][0] < train_set[i]['vertexSet'][relation_pair['t']][0]['pos'][0]:\n",
    "            relation_dict['head_first'].append(1)\n",
    "        else:\n",
    "            relation_dict['head_first'].append(0)\n",
    "        \n",
    "        relation_dict['relation'].append(relation_pair['r'])\n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/DocRED_baseline_metadata/relation_dict.json', 'w') as f:\n",
    "    json.dump(relation_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a8bb72fd144517a7b731c4ceafc43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "relation_dict = {\n",
    "    # 'id': [],\n",
    "    'text': [],\n",
    "    'pair': [],\n",
    "    # 'head_first': [],\n",
    "    'relation': [],\n",
    "    # 'head_start_pos' : [],\n",
    "    # 'tail_start_pos' : []\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(train_set))):\n",
    "    sents = \"\"\n",
    "    for sent in train_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # relation_dict['text'].append(sents)\n",
    "    relation_lines = {k['r']: [] for k in train_set[i]['labels']}\n",
    "    # print(relation_lines)\n",
    "    for relation_pair in train_set[i]['labels']:\n",
    "        # print(relation_pair)\n",
    "        # relation_dict['text'].append(sents)\n",
    "        heads = [item['name'].lower() for item in train_set[i]['vertexSet'][relation_pair['h']]]\n",
    "\n",
    "        max_length, max_index = 0, 0\n",
    "        for k, item in enumerate(heads):\n",
    "            # head = the longest string in the example['head'], also record the index of the longest string\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = k\n",
    "            \n",
    "        head = heads[max_index]\n",
    "\n",
    "        # print(heads)\n",
    "        # print(head)\n",
    "\n",
    "\n",
    "        tails = [item['name'].lower() for item in train_set[i]['vertexSet'][relation_pair['t']]]\n",
    "        max_length, max_index = 0, 0\n",
    "        for k, item in enumerate(tails):\n",
    "            # tail = the longest string in the example['tail'], also record the index of the longest string\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = k\n",
    "\n",
    "        tail = tails[max_index]\n",
    "\n",
    "        # print(tails)\n",
    "        # print(tail)\n",
    "\n",
    "        relation_lines[relation_pair['r']].append((head, tail))\n",
    "        \n",
    "    # random choosing a relation in the rel_info that not be included in the relation_lines.keys()\n",
    "\n",
    "    none_relation = random.choice(list(rel_info.keys()))\n",
    "    while none_relation in relation_lines.keys():\n",
    "        none_relation = random.choice(list(rel_info.keys()))\n",
    "    relation_lines[none_relation] = [(\"none\", \"none\")]\n",
    "\n",
    "    for relation, pair in relation_lines.items():\n",
    "        relation_dict['text'].append(sents)\n",
    "        relation_dict['pair'].append(pair)\n",
    "        relation_dict['relation'].append(rel_info[relation])\n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/DocRED_baseline_metadata/relation_dict_ner.json', 'w') as f:\n",
    "    json.dump(relation_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "if ner:\n",
    "    with open('DocRED/data/DocRED_baseline_metadata/relation_dict.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'head': relation_dict['head'],\n",
    "            'tail': relation_dict['tail'],\n",
    "            'head_first': relation_dict['head_first'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    with open('DocRED/data/DocRED_baseline_metadata/relation_dict_ner.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'pair': relation_dict['pair'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'pair', 'relation'],\n",
       "    num_rows: 19431\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_without_ner(example, tokenizer, padding=True):\n",
    "\n",
    "    padding=True\n",
    "    texts = example['text']\n",
    "\n",
    "    # special_tokens = [50259, 50260, 50261, 50262, 50263, 50264]\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    text_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    output_lines = []\n",
    "\n",
    "    for i in range(len(example['pair'])):\n",
    "        if example['pair'][i][0][0] != \"none\":\n",
    "            output_line = f\"for relation {example['relation'][i]} , \" + f\"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            for pair in example['pair'][i]:\n",
    "                output_line = output_line  + f\"the source is {pair[0]} and the target is {pair[1]} ; \"\n",
    "            \n",
    "            output_line = output_line[:-2] + \". \" + tokenizer.eos_token\n",
    "            output_lines.append(output_line)\n",
    "\n",
    "        else:\n",
    "            output_line = f\"for relation {example['relation'][i]} , \" + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] the source is none . \" + tokenizer.eos_token\n",
    "            output_lines.append(output_line)\n",
    "\n",
    "    # print(output_lines)\n",
    "\n",
    "\n",
    "    output_ids = tokenizer(output_lines, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    count = 0\n",
    "    for i, ids in enumerate(output_ids):\n",
    "        if len(text_ids[i]) + len(ids) > 1024:\n",
    "            text_ids[i] = text_ids[:1024 - len(ids)]\n",
    "            count += 1\n",
    "        text_ids[i] = text_ids[i] + output_ids[i]\n",
    "        assert len(text_ids[i]) <= 1024\n",
    "        attention_mask.append([1] * len(text_ids[i]) + [0] * (1024 - len(text_ids[i])))\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    if padding:\n",
    "        for i, ids in enumerate(output_ids):\n",
    "            output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "            text_ids[i] = text_ids[i] + [tokenizer.pad_token_id] * (1024 - len(text_ids[i]))\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_ner(example, tokenizer, padding=True):\n",
    "    texts = example['text']\n",
    "\n",
    "    # special_tokens = [50259, 50260, 50261, 50262, 50263, 50264]\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    text_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    for i in range(len(example['head'])):\n",
    "        head = \"\"\n",
    "        for item in example['head'][i]:\n",
    "            head += item[0] + \" ; \"\n",
    "        head = head[:-2]\n",
    "        head += \". \"\n",
    "\n",
    "        tail = \"\"\n",
    "        for item in example['tail'][i]:\n",
    "            tail += item[0] + \" ; \"\n",
    "        tail = tail[:-2]\n",
    "        tail += \". \"\n",
    "        \n",
    "        if example['head_first'][i] == 1:\n",
    "            output_line = \" entity 1 : \" + head + \"entity 2 : \" + tail + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            output_line = output_line + f\"the relation between source entity 1 and target entity 2 is {rel_info[example['relation'][i]]} . \" + tokenizer.eos_token\n",
    "\n",
    "        else:\n",
    "            output_line = \" entity 1 : \" + tail + \"entity 2 : \" + head + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            output_line = output_line + f\"the relation between source entity 2 and target entity 1 is {rel_info[example['relation'][i]]} . \" + tokenizer.eos_token\n",
    "        \n",
    "        output_texts.append(output_line)\n",
    "\n",
    "\n",
    "    output_ids = tokenizer(output_texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    count = 0\n",
    "    for i, ids in enumerate(output_ids):\n",
    "        if len(text_ids[i]) + len(ids) > 1024:\n",
    "            text_ids[i] = text_ids[:1024 - len(ids)]\n",
    "            count += 1\n",
    "        text_ids[i] = text_ids[i] + output_ids[i]\n",
    "        assert len(text_ids[i]) <= 1024\n",
    "        attention_mask.append([1] * len(text_ids[i]) + [0] * (1024 - len(text_ids[i])))\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    if padding:\n",
    "        for i, ids in enumerate(output_ids):\n",
    "            output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "            text_ids[i] = text_ids[i] + [tokenizer.pad_token_id] * (1024 - len(text_ids[i]))\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d53cf304fbb4cf1bc5f915572f61d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19431 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_dataset = dataset.map(lambda example: pro_processing_ner(example, tokenizer), batched=True, remove_columns=['text', 'head', 'tail', 'head_first', 'relation'])\n",
    "\n",
    "tokenized_dataset = dataset.map(lambda example: pro_processing_without_ner(example, tokenizer), batched=True, remove_columns=['text', 'pair', 'relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3d067f751847729301fa6d062e3373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/19431 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the datasets type tokenized_dataset\n",
    "\n",
    "tokenized_dataset.save_to_disk('DocRED/data/DocRED_baseline_metadata/tokenized_dataset_without_ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenized_dataset\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = load_from_disk('DocRED/data/DocRED_baseline_metadata/tokenized_dataset_without_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/BioRED/wandb/run-20230707_222615-u0fmms5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v' target=\"_blank\">GPT2-DocRED-without-ner-5epochs</a></strong> to <a href='https://wandb.ai/tian1995/GPT2-normal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/GPT2-normal' target=\"_blank\">https://wandb.ai/tian1995/GPT2-normal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v' target=\"_blank\">https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd87ca81240>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"GPT2-normal\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"GPT2-DocRED-without-ner-5epochs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, \n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=1000, \n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-4, \n",
    "        # fp16=True,\n",
    "        logging_steps=100, \n",
    "        report_to=\"wandb\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir='DocRED/GPT_without_ner'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcfa0271efc4d498a323011c30fe91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 28.43, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6175, 'learning_rate': 4e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2075, 'learning_rate': 6e-05, 'epoch': 0.06}\n",
      "{'loss': 3.0989, 'learning_rate': 8e-05, 'epoch': 0.08}\n",
      "{'loss': 3.0115, 'learning_rate': 0.0001, 'epoch': 0.1}\n",
      "{'loss': 2.8918, 'learning_rate': 0.00012, 'epoch': 0.12}\n",
      "{'loss': 2.8831, 'learning_rate': 0.00014, 'epoch': 0.14}\n",
      "{'loss': 2.8157, 'learning_rate': 0.00016, 'epoch': 0.16}\n",
      "{'loss': 2.73, 'learning_rate': 0.00018, 'epoch': 0.19}\n",
      "{'loss': 2.7119, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 2.6979, 'learning_rate': 0.00019914126234435382, 'epoch': 0.23}\n",
      "{'loss': 2.6357, 'learning_rate': 0.0001982825246887076, 'epoch': 0.25}\n",
      "{'loss': 2.6032, 'learning_rate': 0.00019742378703306143, 'epoch': 0.27}\n",
      "{'loss': 2.5512, 'learning_rate': 0.0001965650493774152, 'epoch': 0.29}\n",
      "{'loss': 2.4882, 'learning_rate': 0.000195706311721769, 'epoch': 0.31}\n",
      "{'loss': 2.4568, 'learning_rate': 0.00019484757406612282, 'epoch': 0.33}\n",
      "{'loss': 2.3863, 'learning_rate': 0.0001939888364104766, 'epoch': 0.35}\n",
      "{'loss': 2.4046, 'learning_rate': 0.00019313009875483042, 'epoch': 0.37}\n",
      "{'loss': 2.3109, 'learning_rate': 0.0001922713610991842, 'epoch': 0.39}\n",
      "{'loss': 2.2869, 'learning_rate': 0.000191412623443538, 'epoch': 0.41}\n",
      "{'loss': 2.3034, 'learning_rate': 0.0001905538857878918, 'epoch': 0.43}\n",
      "{'loss': 2.1946, 'learning_rate': 0.0001896951481322456, 'epoch': 0.45}\n",
      "{'loss': 2.2203, 'learning_rate': 0.0001888364104765994, 'epoch': 0.47}\n",
      "{'loss': 2.1452, 'learning_rate': 0.0001879776728209532, 'epoch': 0.49}\n",
      "{'loss': 2.0762, 'learning_rate': 0.00018711893516530702, 'epoch': 0.51}\n",
      "{'loss': 2.0585, 'learning_rate': 0.00018626019750966083, 'epoch': 0.54}\n",
      "{'loss': 2.0818, 'learning_rate': 0.0001854014598540146, 'epoch': 0.56}\n",
      "{'loss': 1.9827, 'learning_rate': 0.0001845427221983684, 'epoch': 0.58}\n",
      "{'loss': 1.9438, 'learning_rate': 0.0001836839845427222, 'epoch': 0.6}\n",
      "{'loss': 1.9437, 'learning_rate': 0.000182825246887076, 'epoch': 0.62}\n",
      "{'loss': 1.9876, 'learning_rate': 0.00018196650923142982, 'epoch': 0.64}\n",
      "{'loss': 1.9105, 'learning_rate': 0.0001811077715757836, 'epoch': 0.66}\n",
      "{'loss': 1.789, 'learning_rate': 0.0001802490339201374, 'epoch': 0.68}\n",
      "{'loss': 1.874, 'learning_rate': 0.00017939029626449119, 'epoch': 0.7}\n",
      "{'loss': 1.77, 'learning_rate': 0.000178531558608845, 'epoch': 0.72}\n",
      "{'loss': 1.7492, 'learning_rate': 0.00017767282095319882, 'epoch': 0.74}\n",
      "{'loss': 1.6937, 'learning_rate': 0.0001768140832975526, 'epoch': 0.76}\n",
      "{'loss': 1.6315, 'learning_rate': 0.00017595534564190642, 'epoch': 0.78}\n",
      "{'loss': 1.6714, 'learning_rate': 0.0001750966079862602, 'epoch': 0.8}\n",
      "{'loss': 1.639, 'learning_rate': 0.000174237870330614, 'epoch': 0.82}\n",
      "{'loss': 1.5652, 'learning_rate': 0.0001733791326749678, 'epoch': 0.84}\n",
      "{'loss': 1.603, 'learning_rate': 0.0001725203950193216, 'epoch': 0.86}\n",
      "{'loss': 1.5197, 'learning_rate': 0.0001716616573636754, 'epoch': 0.89}\n",
      "{'loss': 1.5292, 'learning_rate': 0.0001708029197080292, 'epoch': 0.91}\n",
      "{'loss': 1.467, 'learning_rate': 0.00016994418205238301, 'epoch': 0.93}\n",
      "{'loss': 1.4443, 'learning_rate': 0.0001690854443967368, 'epoch': 0.95}\n",
      "{'loss': 1.442, 'learning_rate': 0.0001682267067410906, 'epoch': 0.97}\n",
      "{'loss': 1.4727, 'learning_rate': 0.0001673679690854444, 'epoch': 0.99}\n",
      "{'loss': 1.337, 'learning_rate': 0.0001665092314297982, 'epoch': 1.01}\n",
      "{'loss': 1.2009, 'learning_rate': 0.000165650493774152, 'epoch': 1.03}\n",
      "{'loss': 1.1859, 'learning_rate': 0.00016479175611850582, 'epoch': 1.05}\n",
      "{'loss': 1.1621, 'learning_rate': 0.0001639330184628596, 'epoch': 1.07}\n",
      "{'loss': 1.1745, 'learning_rate': 0.0001630742808072134, 'epoch': 1.09}\n",
      "{'loss': 1.1124, 'learning_rate': 0.00016221554315156718, 'epoch': 1.11}\n",
      "{'loss': 1.1129, 'learning_rate': 0.000161356805495921, 'epoch': 1.13}\n",
      "{'loss': 1.1435, 'learning_rate': 0.00016049806784027481, 'epoch': 1.15}\n",
      "{'loss': 1.0897, 'learning_rate': 0.0001596393301846286, 'epoch': 1.17}\n",
      "{'loss': 1.0557, 'learning_rate': 0.00015878059252898242, 'epoch': 1.19}\n",
      "{'loss': 1.0264, 'learning_rate': 0.0001579218548733362, 'epoch': 1.21}\n",
      "{'loss': 1.093, 'learning_rate': 0.00015706311721769, 'epoch': 1.24}\n",
      "{'loss': 1.0324, 'learning_rate': 0.0001562043795620438, 'epoch': 1.26}\n",
      "{'loss': 1.0288, 'learning_rate': 0.0001553456419063976, 'epoch': 1.28}\n",
      "{'loss': 1.0433, 'learning_rate': 0.0001544869042507514, 'epoch': 1.3}\n",
      "{'loss': 0.9878, 'learning_rate': 0.0001536281665951052, 'epoch': 1.32}\n",
      "{'loss': 1.0081, 'learning_rate': 0.000152769428939459, 'epoch': 1.34}\n",
      "{'loss': 0.9349, 'learning_rate': 0.0001519106912838128, 'epoch': 1.36}\n",
      "{'loss': 0.9252, 'learning_rate': 0.0001510519536281666, 'epoch': 1.38}\n",
      "{'loss': 0.933, 'learning_rate': 0.0001501932159725204, 'epoch': 1.4}\n",
      "{'loss': 0.8933, 'learning_rate': 0.0001493344783168742, 'epoch': 1.42}\n",
      "{'loss': 0.8764, 'learning_rate': 0.000148475740661228, 'epoch': 1.44}\n",
      "{'loss': 0.8524, 'learning_rate': 0.00014761700300558182, 'epoch': 1.46}\n",
      "{'loss': 0.8842, 'learning_rate': 0.0001467582653499356, 'epoch': 1.48}\n",
      "{'loss': 0.8748, 'learning_rate': 0.0001458995276942894, 'epoch': 1.5}\n",
      "{'loss': 0.8963, 'learning_rate': 0.00014504079003864318, 'epoch': 1.52}\n",
      "{'loss': 0.8453, 'learning_rate': 0.000144182052382997, 'epoch': 1.54}\n",
      "{'loss': 0.8147, 'learning_rate': 0.0001433233147273508, 'epoch': 1.56}\n",
      "{'loss': 0.7874, 'learning_rate': 0.0001424645770717046, 'epoch': 1.59}\n",
      "{'loss': 0.8046, 'learning_rate': 0.00014160583941605841, 'epoch': 1.61}\n",
      "{'loss': 0.7834, 'learning_rate': 0.0001407471017604122, 'epoch': 1.63}\n",
      "{'loss': 0.7781, 'learning_rate': 0.000139888364104766, 'epoch': 1.65}\n",
      "{'loss': 0.7468, 'learning_rate': 0.0001390296264491198, 'epoch': 1.67}\n",
      "{'loss': 0.7705, 'learning_rate': 0.0001381708887934736, 'epoch': 1.69}\n",
      "{'loss': 0.7008, 'learning_rate': 0.0001373121511378274, 'epoch': 1.71}\n",
      "{'loss': 0.7005, 'learning_rate': 0.0001364534134821812, 'epoch': 1.73}\n",
      "{'loss': 0.7104, 'learning_rate': 0.000135594675826535, 'epoch': 1.75}\n",
      "{'loss': 0.7232, 'learning_rate': 0.0001347359381708888, 'epoch': 1.77}\n",
      "{'loss': 0.6478, 'learning_rate': 0.00013387720051524259, 'epoch': 1.79}\n",
      "{'loss': 0.696, 'learning_rate': 0.0001330184628595964, 'epoch': 1.81}\n",
      "{'loss': 0.72, 'learning_rate': 0.0001321597252039502, 'epoch': 1.83}\n",
      "{'loss': 0.6838, 'learning_rate': 0.000131300987548304, 'epoch': 1.85}\n",
      "{'loss': 0.6136, 'learning_rate': 0.00013044224989265782, 'epoch': 1.87}\n",
      "{'loss': 0.687, 'learning_rate': 0.0001295835122370116, 'epoch': 1.89}\n",
      "{'loss': 0.6562, 'learning_rate': 0.0001287247745813654, 'epoch': 1.91}\n",
      "{'loss': 0.6013, 'learning_rate': 0.00012786603692571918, 'epoch': 1.93}\n",
      "{'loss': 0.6566, 'learning_rate': 0.000127007299270073, 'epoch': 1.96}\n",
      "{'loss': 0.6016, 'learning_rate': 0.0001261485616144268, 'epoch': 1.98}\n",
      "{'loss': 0.6257, 'learning_rate': 0.0001252898239587806, 'epoch': 2.0}\n",
      "{'loss': 0.4766, 'learning_rate': 0.0001244310863031344, 'epoch': 2.02}\n",
      "{'loss': 0.5081, 'learning_rate': 0.0001235723486474882, 'epoch': 2.04}\n",
      "{'loss': 0.4996, 'learning_rate': 0.000122713610991842, 'epoch': 2.06}\n",
      "{'loss': 0.5075, 'learning_rate': 0.0001218548733361958, 'epoch': 2.08}\n",
      "{'loss': 0.5102, 'learning_rate': 0.00012099613568054959, 'epoch': 2.1}\n",
      "{'loss': 0.4718, 'learning_rate': 0.0001201373980249034, 'epoch': 2.12}\n",
      "{'loss': 0.4578, 'learning_rate': 0.0001192786603692572, 'epoch': 2.14}\n",
      "{'loss': 0.4695, 'learning_rate': 0.000118419922713611, 'epoch': 2.16}\n",
      "{'loss': 0.5269, 'learning_rate': 0.00011756118505796481, 'epoch': 2.18}\n",
      "{'loss': 0.4688, 'learning_rate': 0.0001167024474023186, 'epoch': 2.2}\n",
      "{'loss': 0.4584, 'learning_rate': 0.0001158437097466724, 'epoch': 2.22}\n",
      "{'loss': 0.4356, 'learning_rate': 0.00011498497209102619, 'epoch': 2.24}\n",
      "{'loss': 0.4369, 'learning_rate': 0.00011412623443538, 'epoch': 2.26}\n",
      "{'loss': 0.4029, 'learning_rate': 0.0001132674967797338, 'epoch': 2.28}\n",
      "{'loss': 0.432, 'learning_rate': 0.00011240875912408759, 'epoch': 2.31}\n",
      "{'loss': 0.4199, 'learning_rate': 0.0001115500214684414, 'epoch': 2.33}\n",
      "{'loss': 0.409, 'learning_rate': 0.00011069128381279519, 'epoch': 2.35}\n",
      "{'loss': 0.4134, 'learning_rate': 0.000109832546157149, 'epoch': 2.37}\n",
      "{'loss': 0.4672, 'learning_rate': 0.00010897380850150281, 'epoch': 2.39}\n",
      "{'loss': 0.4084, 'learning_rate': 0.0001081150708458566, 'epoch': 2.41}\n",
      "{'loss': 0.4143, 'learning_rate': 0.0001072563331902104, 'epoch': 2.43}\n",
      "{'loss': 0.4191, 'learning_rate': 0.00010639759553456419, 'epoch': 2.45}\n",
      "{'loss': 0.3953, 'learning_rate': 0.000105538857878918, 'epoch': 2.47}\n",
      "{'loss': 0.3813, 'learning_rate': 0.0001046801202232718, 'epoch': 2.49}\n",
      "{'loss': 0.3917, 'learning_rate': 0.00010382138256762559, 'epoch': 2.51}\n",
      "{'loss': 0.3844, 'learning_rate': 0.0001029626449119794, 'epoch': 2.53}\n",
      "{'loss': 0.3772, 'learning_rate': 0.00010210390725633319, 'epoch': 2.55}\n",
      "{'loss': 0.3758, 'learning_rate': 0.000101245169600687, 'epoch': 2.57}\n",
      "{'loss': 0.3656, 'learning_rate': 0.00010038643194504081, 'epoch': 2.59}\n",
      "{'loss': 0.3642, 'learning_rate': 9.95276942893946e-05, 'epoch': 2.61}\n",
      "{'loss': 0.3857, 'learning_rate': 9.86689566337484e-05, 'epoch': 2.63}\n",
      "{'loss': 0.3549, 'learning_rate': 9.78102189781022e-05, 'epoch': 2.66}\n",
      "{'loss': 0.3138, 'learning_rate': 9.6951481322456e-05, 'epoch': 2.68}\n",
      "{'loss': 0.3474, 'learning_rate': 9.609274366680979e-05, 'epoch': 2.7}\n",
      "{'loss': 0.3485, 'learning_rate': 9.523400601116359e-05, 'epoch': 2.72}\n",
      "{'loss': 0.3496, 'learning_rate': 9.43752683555174e-05, 'epoch': 2.74}\n",
      "{'loss': 0.339, 'learning_rate': 9.351653069987119e-05, 'epoch': 2.76}\n",
      "{'loss': 0.325, 'learning_rate': 9.265779304422499e-05, 'epoch': 2.78}\n",
      "{'loss': 0.3136, 'learning_rate': 9.17990553885788e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3321, 'learning_rate': 9.09403177329326e-05, 'epoch': 2.82}\n",
      "{'loss': 0.3223, 'learning_rate': 9.00815800772864e-05, 'epoch': 2.84}\n",
      "{'loss': 0.3242, 'learning_rate': 8.92228424216402e-05, 'epoch': 2.86}\n",
      "{'loss': 0.3106, 'learning_rate': 8.8364104765994e-05, 'epoch': 2.88}\n",
      "{'loss': 0.3147, 'learning_rate': 8.750536711034779e-05, 'epoch': 2.9}\n",
      "{'loss': 0.3067, 'learning_rate': 8.664662945470159e-05, 'epoch': 2.92}\n",
      "{'loss': 0.3057, 'learning_rate': 8.57878917990554e-05, 'epoch': 2.94}\n",
      "{'loss': 0.298, 'learning_rate': 8.492915414340919e-05, 'epoch': 2.96}\n",
      "{'loss': 0.305, 'learning_rate': 8.407041648776299e-05, 'epoch': 2.98}\n",
      "{'loss': 0.2773, 'learning_rate': 8.32116788321168e-05, 'epoch': 3.01}\n",
      "{'loss': 0.2597, 'learning_rate': 8.23529411764706e-05, 'epoch': 3.03}\n",
      "{'loss': 0.2292, 'learning_rate': 8.14942035208244e-05, 'epoch': 3.05}\n",
      "{'loss': 0.2348, 'learning_rate': 8.06354658651782e-05, 'epoch': 3.07}\n",
      "{'loss': 0.2332, 'learning_rate': 7.9776728209532e-05, 'epoch': 3.09}\n",
      "{'loss': 0.2494, 'learning_rate': 7.891799055388579e-05, 'epoch': 3.11}\n",
      "{'loss': 0.2556, 'learning_rate': 7.805925289823959e-05, 'epoch': 3.13}\n",
      "{'loss': 0.2205, 'learning_rate': 7.72005152425934e-05, 'epoch': 3.15}\n",
      "{'loss': 0.2437, 'learning_rate': 7.634177758694719e-05, 'epoch': 3.17}\n",
      "{'loss': 0.2461, 'learning_rate': 7.548303993130099e-05, 'epoch': 3.19}\n",
      "{'loss': 0.2576, 'learning_rate': 7.462430227565479e-05, 'epoch': 3.21}\n",
      "{'loss': 0.2209, 'learning_rate': 7.37655646200086e-05, 'epoch': 3.23}\n",
      "{'loss': 0.2183, 'learning_rate': 7.29068269643624e-05, 'epoch': 3.25}\n",
      "{'loss': 0.2239, 'learning_rate': 7.20480893087162e-05, 'epoch': 3.27}\n",
      "{'loss': 0.2461, 'learning_rate': 7.118935165307e-05, 'epoch': 3.29}\n",
      "{'loss': 0.2189, 'learning_rate': 7.033061399742379e-05, 'epoch': 3.31}\n",
      "{'loss': 0.2118, 'learning_rate': 6.947187634177759e-05, 'epoch': 3.33}\n",
      "{'loss': 0.2278, 'learning_rate': 6.86131386861314e-05, 'epoch': 3.36}\n",
      "{'loss': 0.2245, 'learning_rate': 6.775440103048519e-05, 'epoch': 3.38}\n",
      "{'loss': 0.2215, 'learning_rate': 6.689566337483899e-05, 'epoch': 3.4}\n",
      "{'loss': 0.2209, 'learning_rate': 6.603692571919279e-05, 'epoch': 3.42}\n",
      "{'loss': 0.2299, 'learning_rate': 6.517818806354658e-05, 'epoch': 3.44}\n",
      "{'loss': 0.1983, 'learning_rate': 6.43194504079004e-05, 'epoch': 3.46}\n",
      "{'loss': 0.2237, 'learning_rate': 6.34607127522542e-05, 'epoch': 3.48}\n",
      "{'loss': 0.2213, 'learning_rate': 6.260197509660798e-05, 'epoch': 3.5}\n",
      "{'loss': 0.2073, 'learning_rate': 6.174323744096178e-05, 'epoch': 3.52}\n",
      "{'loss': 0.2056, 'learning_rate': 6.0884499785315586e-05, 'epoch': 3.54}\n",
      "{'loss': 0.2072, 'learning_rate': 6.0025762129669394e-05, 'epoch': 3.56}\n",
      "{'loss': 0.2149, 'learning_rate': 5.916702447402319e-05, 'epoch': 3.58}\n",
      "{'loss': 0.2113, 'learning_rate': 5.830828681837699e-05, 'epoch': 3.6}\n",
      "{'loss': 0.1917, 'learning_rate': 5.7449549162730784e-05, 'epoch': 3.62}\n",
      "{'loss': 0.1998, 'learning_rate': 5.6590811507084585e-05, 'epoch': 3.64}\n",
      "{'loss': 0.1947, 'learning_rate': 5.5732073851438393e-05, 'epoch': 3.66}\n",
      "{'loss': 0.1984, 'learning_rate': 5.487333619579219e-05, 'epoch': 3.68}\n",
      "{'loss': 0.1846, 'learning_rate': 5.401459854014599e-05, 'epoch': 3.71}\n",
      "{'loss': 0.1873, 'learning_rate': 5.3155860884499784e-05, 'epoch': 3.73}\n",
      "{'loss': 0.1838, 'learning_rate': 5.2297123228853585e-05, 'epoch': 3.75}\n",
      "{'loss': 0.182, 'learning_rate': 5.143838557320739e-05, 'epoch': 3.77}\n",
      "{'loss': 0.1889, 'learning_rate': 5.057964791756119e-05, 'epoch': 3.79}\n",
      "{'loss': 0.1823, 'learning_rate': 4.972091026191499e-05, 'epoch': 3.81}\n",
      "{'loss': 0.1844, 'learning_rate': 4.886217260626878e-05, 'epoch': 3.83}\n",
      "{'loss': 0.1895, 'learning_rate': 4.800343495062259e-05, 'epoch': 3.85}\n",
      "{'loss': 0.1855, 'learning_rate': 4.7144697294976386e-05, 'epoch': 3.87}\n",
      "{'loss': 0.1756, 'learning_rate': 4.628595963933019e-05, 'epoch': 3.89}\n",
      "{'loss': 0.1719, 'learning_rate': 4.542722198368399e-05, 'epoch': 3.91}\n",
      "{'loss': 0.1832, 'learning_rate': 4.456848432803778e-05, 'epoch': 3.93}\n",
      "{'loss': 0.1722, 'learning_rate': 4.370974667239159e-05, 'epoch': 3.95}\n",
      "{'loss': 0.1721, 'learning_rate': 4.2851009016745385e-05, 'epoch': 3.97}\n",
      "{'loss': 0.1885, 'learning_rate': 4.199227136109919e-05, 'epoch': 3.99}\n",
      "{'loss': 0.1441, 'learning_rate': 4.113353370545299e-05, 'epoch': 4.01}\n",
      "{'loss': 0.1541, 'learning_rate': 4.027479604980678e-05, 'epoch': 4.03}\n",
      "{'loss': 0.1615, 'learning_rate': 3.941605839416059e-05, 'epoch': 4.06}\n",
      "{'loss': 0.1554, 'learning_rate': 3.8557320738514385e-05, 'epoch': 4.08}\n",
      "{'loss': 0.1645, 'learning_rate': 3.7698583082868186e-05, 'epoch': 4.1}\n",
      "{'loss': 0.1383, 'learning_rate': 3.683984542722199e-05, 'epoch': 4.12}\n",
      "{'loss': 0.1402, 'learning_rate': 3.598110777157578e-05, 'epoch': 4.14}\n",
      "{'loss': 0.1456, 'learning_rate': 3.512237011592959e-05, 'epoch': 4.16}\n",
      "{'loss': 0.1438, 'learning_rate': 3.4263632460283385e-05, 'epoch': 4.18}\n",
      "{'loss': 0.141, 'learning_rate': 3.340489480463718e-05, 'epoch': 4.2}\n",
      "{'loss': 0.1299, 'learning_rate': 3.254615714899099e-05, 'epoch': 4.22}\n",
      "{'loss': 0.134, 'learning_rate': 3.168741949334478e-05, 'epoch': 4.24}\n",
      "{'loss': 0.1376, 'learning_rate': 3.082868183769859e-05, 'epoch': 4.26}\n",
      "{'loss': 0.137, 'learning_rate': 2.9969944182052384e-05, 'epoch': 4.28}\n",
      "{'loss': 0.1375, 'learning_rate': 2.9111206526406182e-05, 'epoch': 4.3}\n",
      "{'loss': 0.142, 'learning_rate': 2.8252468870759983e-05, 'epoch': 4.32}\n",
      "{'loss': 0.1369, 'learning_rate': 2.739373121511378e-05, 'epoch': 4.34}\n",
      "{'loss': 0.1524, 'learning_rate': 2.6534993559467586e-05, 'epoch': 4.36}\n",
      "{'loss': 0.1414, 'learning_rate': 2.5676255903821384e-05, 'epoch': 4.38}\n",
      "{'loss': 0.1497, 'learning_rate': 2.4817518248175185e-05, 'epoch': 4.41}\n",
      "{'loss': 0.1198, 'learning_rate': 2.3958780592528983e-05, 'epoch': 4.43}\n",
      "{'loss': 0.1273, 'learning_rate': 2.3100042936882784e-05, 'epoch': 4.45}\n",
      "{'loss': 0.1351, 'learning_rate': 2.2241305281236582e-05, 'epoch': 4.47}\n",
      "{'loss': 0.1295, 'learning_rate': 2.1382567625590383e-05, 'epoch': 4.49}\n",
      "{'loss': 0.1239, 'learning_rate': 2.0523829969944185e-05, 'epoch': 4.51}\n",
      "{'loss': 0.1351, 'learning_rate': 1.9665092314297983e-05, 'epoch': 4.53}\n",
      "{'loss': 0.13, 'learning_rate': 1.8806354658651784e-05, 'epoch': 4.55}\n",
      "{'loss': 0.1251, 'learning_rate': 1.7947617003005582e-05, 'epoch': 4.57}\n",
      "{'loss': 0.1466, 'learning_rate': 1.7088879347359383e-05, 'epoch': 4.59}\n",
      "{'loss': 0.1343, 'learning_rate': 1.623014169171318e-05, 'epoch': 4.61}\n",
      "{'loss': 0.1308, 'learning_rate': 1.5371404036066982e-05, 'epoch': 4.63}\n",
      "{'loss': 0.1239, 'learning_rate': 1.4512666380420784e-05, 'epoch': 4.65}\n",
      "{'loss': 0.1453, 'learning_rate': 1.3653928724774581e-05, 'epoch': 4.67}\n",
      "{'loss': 0.1383, 'learning_rate': 1.2795191069128381e-05, 'epoch': 4.69}\n",
      "{'loss': 0.1209, 'learning_rate': 1.1936453413482182e-05, 'epoch': 4.71}\n",
      "{'loss': 0.1259, 'learning_rate': 1.1077715757835982e-05, 'epoch': 4.73}\n",
      "{'loss': 0.1471, 'learning_rate': 1.0218978102189781e-05, 'epoch': 4.76}\n",
      "{'loss': 0.1263, 'learning_rate': 9.360240446543583e-06, 'epoch': 4.78}\n",
      "{'loss': 0.1161, 'learning_rate': 8.50150279089738e-06, 'epoch': 4.8}\n",
      "{'loss': 0.1238, 'learning_rate': 7.642765135251182e-06, 'epoch': 4.82}\n",
      "{'loss': 0.1287, 'learning_rate': 6.784027479604981e-06, 'epoch': 4.84}\n",
      "{'loss': 0.1313, 'learning_rate': 5.925289823958781e-06, 'epoch': 4.86}\n",
      "{'loss': 0.1307, 'learning_rate': 5.066552168312581e-06, 'epoch': 4.88}\n",
      "{'loss': 0.1369, 'learning_rate': 4.20781451266638e-06, 'epoch': 4.9}\n",
      "{'loss': 0.1224, 'learning_rate': 3.3490768570201806e-06, 'epoch': 4.92}\n",
      "{'loss': 0.1283, 'learning_rate': 2.49033920137398e-06, 'epoch': 4.94}\n",
      "{'loss': 0.116, 'learning_rate': 1.6316015457277802e-06, 'epoch': 4.96}\n",
      "{'loss': 0.1358, 'learning_rate': 7.728638900815801e-07, 'epoch': 4.98}\n",
      "{'train_runtime': 20677.7711, 'train_samples_per_second': 4.699, 'train_steps_per_second': 1.175, 'train_loss': 0.8649780624047284, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24290, training_loss=0.8649780624047284, metrics={'train_runtime': 20677.7711, 'train_samples_per_second': 4.699, 'train_steps_per_second': 1.175, 'train_loss': 0.8649780624047284, 'epoch': 5.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▇███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>24290</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1358</td></tr><tr><td>train/total_flos</td><td>5.077165473792e+16</td></tr><tr><td>train/train_loss</td><td>0.86498</td></tr><tr><td>train/train_runtime</td><td>20677.7711</td></tr><tr><td>train/train_samples_per_second</td><td>4.699</td></tr><tr><td>train/train_steps_per_second</td><td>1.175</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GPT2-DocRED-without-ner-5epochs</strong> at: <a href='https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v' target=\"_blank\">https://wandb.ai/tian1995/GPT2-normal/runs/u0fmms5v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230707_222615-u0fmms5v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "trainer.save_model(\"DocRED/GPT_without_ner/model\")\n",
    "\n",
    "# save the tokenizer\n",
    "# tokenizer.save_pretrained(\"DocRED/GPT_w_ner/tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"DocRED/GPT_w_ner/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"DocRED/GPT_w_ner/tokenizer\")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label : uk supreme motorsports   uk supreme motors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file from DocRED/data/test.json and DocRED/data/rel_info.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open('DocRED/data/dev.json') as f:\n",
    "    test_set = json.load(f)\n",
    "\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b41da423a6d4577aa7a8f1a843f87b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with ner\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "relation_dict = {\n",
    "    'id': [],\n",
    "    'text': [],\n",
    "    'head': [],\n",
    "    'tail': [],\n",
    "    'head_first': [],\n",
    "    'relation': [],\n",
    "    'head_start_pos' : [],\n",
    "    'tail_start_pos' : []\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(test_set))):\n",
    "    sents = \"\"\n",
    "    for sent in test_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # relation_dict['text'].append(sents)\n",
    "\n",
    "    for relation_pair in test_set[i]['labels']:\n",
    "        relation_dict['id'].append(i)\n",
    "        relation_dict['text'].append(sents)\n",
    "        head = []\n",
    "        head_ = []\n",
    "        head_start_pos = []\n",
    "        head.append([[item['name'].lower()] for item in test_set[i]['vertexSet'][relation_pair['h']]])\n",
    "        for j, item in enumerate(head[0]):\n",
    "            if item not in head_:\n",
    "                head_.append(item)\n",
    "                head_start_pos.append(test_set[i]['vertexSet'][relation_pair['h']][j]['pos'][0])\n",
    "\n",
    "        relation_dict['head'].append(head_)\n",
    "        relation_dict['head_start_pos'].append(head_start_pos)\n",
    "\n",
    "        tail = []\n",
    "        tail_ = []\n",
    "        tail_start_pos = []\n",
    "        tail.append([[item['name'].lower()] for item in test_set[i]['vertexSet'][relation_pair['t']]])\n",
    "        for j, item in enumerate(tail[0]):\n",
    "            if item not in tail_:\n",
    "                tail_.append(item)\n",
    "                tail_start_pos.append(test_set[i]['vertexSet'][relation_pair['t']][j]['pos'][0])\n",
    "        relation_dict['tail'].append(tail_)\n",
    "        relation_dict['tail_start_pos'].append(tail_start_pos)\n",
    "\n",
    "        \n",
    "        if test_set[i]['vertexSet'][relation_pair['h']][0]['pos'][0] < test_set[i]['vertexSet'][relation_pair['t']][0]['pos'][0]:\n",
    "            relation_dict['head_first'].append(1)\n",
    "        else:\n",
    "            relation_dict['head_first'].append(0)\n",
    "        \n",
    "        relation_dict['relation'].append(relation_pair['r'])\n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/DocRED_baseline_metadata/dev_relation_dict.json', 'w') as f:\n",
    "    json.dump(relation_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257f2305217e4f708a6c6a29cf057ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# without ner\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "relation_dict = {\n",
    "    # 'id': [],\n",
    "    'text': [],\n",
    "    'pair': [],\n",
    "    # 'head_first': [],\n",
    "    'relation': [],\n",
    "    # 'head_start_pos' : [],\n",
    "    # 'tail_start_pos' : []\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(test_set))):\n",
    "    sents = \"\"\n",
    "    for sent in test_set[i]['sents']:\n",
    "        # flatten the sent list\n",
    "        a = \" \".join(sent)\n",
    "        sents += a.lower() + \" \"\n",
    "    # relation_dict['text'].append(sents)\n",
    "    relation_lines = {k['r']: [] for k in test_set[i]['labels']}\n",
    "    # print(relation_lines)\n",
    "    for relation_pair in test_set[i]['labels']:\n",
    "        # print(relation_pair)\n",
    "        # relation_dict['text'].append(sents)\n",
    "        heads = [item['name'].lower() for item in test_set[i]['vertexSet'][relation_pair['h']]]\n",
    "\n",
    "        max_length, max_index = 0, 0\n",
    "        for k, item in enumerate(heads):\n",
    "            # head = the longest string in the example['head'], also record the index of the longest string\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = k\n",
    "            \n",
    "        head = heads[max_index]\n",
    "\n",
    "        # print(heads)\n",
    "        # print(head)\n",
    "\n",
    "\n",
    "        tails = [item['name'].lower() for item in test_set[i]['vertexSet'][relation_pair['t']]]\n",
    "        max_length, max_index = 0, 0\n",
    "        for k, item in enumerate(tails):\n",
    "            # tail = the longest string in the example['tail'], also record the index of the longest string\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = k\n",
    "\n",
    "        tail = tails[max_index]\n",
    "\n",
    "        # print(tails)\n",
    "        # print(tail)\n",
    "\n",
    "        relation_lines[relation_pair['r']].append((head, tail))\n",
    "        \n",
    "    # random choosing a relation in the rel_info that not be included in the relation_lines.keys()\n",
    "\n",
    "    none_relation = random.choice(list(rel_info.keys()))\n",
    "    while none_relation in relation_lines.keys():\n",
    "        none_relation = random.choice(list(rel_info.keys()))\n",
    "    relation_lines[none_relation] = [(\"none\", \"none\")]\n",
    "\n",
    "    for relation, pair in relation_lines.items():\n",
    "        relation_dict['text'].append(sents)\n",
    "        relation_dict['pair'].append(pair)\n",
    "        relation_dict['relation'].append(rel_info[relation])\n",
    "    # break\n",
    "\n",
    "\n",
    "# save the relation_dict to a json file\n",
    "\n",
    "with open('DocRED/data/DocRED_baseline_metadata/dev_relation_dict_without_ner.json', 'w') as f:\n",
    "    json.dump(relation_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "if ner:\n",
    "    with open('DocRED/data/DocRED_baseline_metadata/dev_relation_dict.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'head': relation_dict['head'],\n",
    "            'tail': relation_dict['tail'],\n",
    "            'head_first': relation_dict['head_first'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    with open('DocRED/data/DocRED_baseline_metadata/dev_relation_dict_without_ner.json') as f:\n",
    "        relation_dict = json.load(f)\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            'text': relation_dict['text'],\n",
    "            'pair': relation_dict['pair'],\n",
    "            'relation': relation_dict['relation']\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'washington place ( william washington house ) is one of the first homes built by freed slaves after the emancipation proclamation of 1863 in hampshire county , west virginia , united states . washington place was built by william and annie washington in north romney between 1863 and 1874 on land given to annie by her former owner , susan blue parsons of wappocomo plantation . william washington later acquired other properties on the hills north of romney along west virginia route 28 and became the first african - american land developer in the state of west virginia . one of his subdivisions is the \" blacks hill \" neighborhood of romney , adjacent to the washington place homestead . washington place was bought and restored by ralph w. haines , a local attorney and historic preservationist . ',\n",
       " 'head': [['emancipation proclamation']],\n",
       " 'tail': [['united states']],\n",
       " 'head_first': 1,\n",
       " 'relation': 'P17'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_without_ner_infer(example, tokenizer):\n",
    "\n",
    "    padding=True\n",
    "    texts = example['text']\n",
    "\n",
    "    # special_tokens = [50259, 50260, 50261, 50262, 50263, 50264]\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    text_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    output_lines = []\n",
    "\n",
    "    for i in range(len(example['pair'])):\n",
    "        # if example['pair'][i][0][0] != \"none\":\n",
    "        output_line = f\"for relation {example['relation'][i]} , [learn1] [learn2] [learn3][learn4] [learn5] [learn6] \"\n",
    "        # for pair in example['pair'][i]:\n",
    "        #     output_line = output_line  + f\"the source is {pair[0]} and the target is {pair[1]} ; \"\n",
    "        \n",
    "        # output_line = output_line[:-2] + \". \" + tokenizer.eos_token\n",
    "        output_lines.append(output_line)\n",
    "        if i == 5:\n",
    "            print(output_line)\n",
    "\n",
    "        # else:\n",
    "        #     output_line = f\"for relation {example['relation'][i]} , \" + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] the source is none . \" + tokenizer.eos_token\n",
    "        #     output_lines.append(output_line)\n",
    "\n",
    "    # print(output_lines)\n",
    "\n",
    "\n",
    "    output_ids = tokenizer(output_lines, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    attention_mask = []\n",
    "\n",
    "    count = 0\n",
    "    for i, ids in enumerate(output_ids):\n",
    "        if len(text_ids[i]) + len(ids) > 1024:\n",
    "            text_ids[i] = text_ids[:1024 - len(ids)]\n",
    "            count += 1\n",
    "        text_ids[i] = text_ids[i] + output_ids[i]\n",
    "        assert len(text_ids[i]) <= 1024\n",
    "        # attention_mask.append([1] * len(text_ids[i]) + [0] * (1024 - len(text_ids[i])))\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    # if padding:\n",
    "    #     for i, ids in enumerate(output_ids):\n",
    "    #         output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "    #         text_ids[i] = text_ids[i] + [tokenizer.pad_token_id] * (1024 - len(text_ids[i]))\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_ids,\n",
    "        # 'attention_mask': attention_mask,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_processing_ner_infer(example, tokenizer):\n",
    "    texts = example['text']\n",
    "\n",
    "    # special_tokens = [50259, 50260, 50261, 50262, 50263, 50264]\n",
    "\n",
    "    output_texts = []\n",
    "\n",
    "    text_ids = tokenizer(texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    for i in range(len(example['head'])):\n",
    "        head = \"\"\n",
    "        for item in example['head'][i]:\n",
    "            head += item[0] + \" ; \"\n",
    "        head = head[:-2]\n",
    "        head += \". \"\n",
    "\n",
    "        tail = \"\"\n",
    "        for item in example['tail'][i]:\n",
    "            tail += item[0] + \" ; \"\n",
    "        tail = tail[:-2]\n",
    "        tail += \". \"\n",
    "        \n",
    "        if example['head_first'][i] == 1:\n",
    "            output_line = \"entity 1 : \" + head + \"entity 2 : \" + tail + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            # output_line = output_line + f\"the relation between source entity 1 and target entity 2 is {rel_info[example['relation'][i]]} . \" + tokenizer.eos_token\n",
    "\n",
    "        else:\n",
    "            output_line = \" entity 1 : \" + tail + \"entity 2 : \" + head + \"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \"\n",
    "            # output_line = output_line + f\"the relation between source entity 2 and target entity 1 is {rel_info[example['relation'][i]]} . \" + tokenizer.eos_token\n",
    "        \n",
    "        output_texts.append(output_line)\n",
    "\n",
    "\n",
    "    output_ids = tokenizer(output_texts, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # input_ids = []\n",
    "    # attention_mask = []\n",
    "\n",
    "    count = 0\n",
    "    for i, ids in enumerate(output_ids):\n",
    "        if len(text_ids[i]) + len(ids) > 1024:\n",
    "            text_ids[i] = text_ids[:1024 - len(ids)]\n",
    "            count += 1\n",
    "        text_ids[i] = text_ids[i] + output_ids[i]\n",
    "        assert len(text_ids[i]) <= 1024\n",
    "        # attention_mask.append([1] * len(text_ids[i]) + [0] * (1024 - len(text_ids[i])))\n",
    "    if count != 0:\n",
    "        print(f\"truncated {count} examples\")\n",
    "\n",
    "    # if padding:\n",
    "    #     for i, ids in enumerate(output_ids):\n",
    "    #         output_ids[i] = ids + [tokenizer.pad_token_id] * (1024 - len(ids))\n",
    "    #         text_ids[i] = text_ids[i] + [tokenizer.pad_token_id] * (1024 - len(text_ids[i]))\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_ids,\n",
    "        # 'attention_mask': attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'head', 'tail', 'head_first', 'relation'],\n",
       "    num_rows: 12275\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388dfd9df45a45088a27cd3e95c1aaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_dataset = dataset.map(lambda example: pro_processing_without_ner_infer(example, tokenizer), batched=True, remove_columns=['text', 'pair', 'relation'])\n",
    "tokenized_dataset = dataset.map(lambda example: pro_processing_ner_infer(example, tokenizer), batched=True, remove_columns=['text', 'head', 'tail', 'head_first', 'relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a838cf6086a4b489e8d2c78a5dfd92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk('DocRED/data/DocRED_baseline_metadata/dev_tokenized_dataset_w_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12275"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = load_from_disk('DocRED/data/DocRED_baseline_metadata/dev_tokenized_dataset_w_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"skai tv is a greek free - to - air television network based in piraeus. it is part of the skai group, one of the largest media groups in the country. it was relaunched in its present form on 1st of april 2006 in the athens metropolitan area, and gradually spread its coverage nationwide. besides digital terrestrial transmission, it is available on the subscription - based encrypted services of nova and cosmote tv. skai tv is also a member of digea, a consortium of private television networks introducing digital terrestrial transmission in greece. at launch, skai tv opted for dubbing all foreign language content into greek, instead of using subtitles. this is very uncommon in greece for anything except documentaries ( using voiceover dubbing ) and children's programmes ( using lip - synced dubbing ), so after intense criticism the station switched to using subtitles for almost all foreign shows. entity 1 : piraeus. entity 2 : greece. [learn1] [learn2] [learn3] [learn4] [learn5] [learn6]\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset[0]['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e017f9a6fbee4a48987c3e235d56f422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is mother. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is member of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is award received. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is author. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is publication date. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country of origin. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is present in work. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is member of political party. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is publisher. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is religion. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is sibling. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is conflict. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is director. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is instance of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is performer. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is continent. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of death. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is follows. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is narrative location. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is place of birth. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is author. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is author. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is has part. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is author. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is participant of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is part of. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country of origin. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is member of sports team. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is sibling. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is member of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is member of. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is father. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is cast member. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is follows. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is applies to jurisdiction. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is present in work. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is ethnic group. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is performer. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is award received. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is present in work. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is record label. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is religion. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is composer. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is contains administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is part of. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is cast member. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country of citizenship. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is producer. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is performer. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in or next to body of water. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of death. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is follows. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is religion. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is has part. <|endoftext|>\n",
      "the relation between source entity 2 and target entity 1 is located in the administrative territorial entity. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is country. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is inception. <|endoftext|>\n",
      "the relation between source entity 1 and target entity 2 is date of birth. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "model.eval()\n",
    "outputs = []\n",
    "model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(tokenized_dataset))):\n",
    "    # for i in range(1):\n",
    "        output = model.generate(input_ids=tokenized_dataset[\"input_ids\"][i].unsqueeze(0).to(\"cuda\"), max_new_tokens=50, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "        output_text = tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0]\n",
    "        try:\n",
    "            outputs.append(output_text.split(\"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6]\")[1].strip())\n",
    "        except:\n",
    "            outputs.append(output_text.split(\"[learn1][learn2][learn3][learn4][learn5][learn6]\")[1].strip())\n",
    "        if i % 100 == 0:\n",
    "            print(outputs[-1])\n",
    "\n",
    "    # print(tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"skai tv is a greek free - to - air television network based in piraeus . it is part of the skai group , one of the largest media groups in the country . it was relaunched in its present form on 1st of april 2006 in the athens metropolitan area , and gradually spread its coverage nationwide . besides digital terrestrial transmission , it is available on the subscription - based encrypted services of nova and cosmote tv . skai tv is also a member of digea , a consortium of private television networks introducing digital terrestrial transmission in greece . at launch , skai tv opted for dubbing all foreign language content into greek , instead of using subtitles . this is very uncommon in greece for anything except documentaries ( using voiceover dubbing ) and children 's programmes ( using lip - synced dubbing ) , so after intense criticism the station switched to using subtitles for almost all foreign shows . \",\n",
       " 'head': [['piraeus']],\n",
       " 'tail': [['greece']],\n",
       " 'head_first': 1,\n",
       " 'relation': 'P17'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the rel_info\n",
    "\n",
    "with open('DocRED/data/rel_info.json') as f:\n",
    "    rel_info = json.load(f)\n",
    "\n",
    "rel2id = {v: k for k, v in rel_info.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 12275\n"
     ]
    }
   ],
   "source": [
    "# post processing for the outputs w ner\n",
    "# (source, target, relation)\n",
    "# (2, 1, relation)\n",
    "pairs = []\n",
    "count = 0\n",
    "for output in outputs:\n",
    "    # if the output doesn't end with \"<|endoftext|>\", find the lastest \";\" of the output and only take the previous part\n",
    "    try:\n",
    "        source = output.split(\"between source \")[1].strip()\n",
    "        source = source.split(\" and target \")[0].strip()\n",
    "        source = source.split(\"entity\")[1].strip()\n",
    "\n",
    "        target = output.split(\" and target \")[1].strip()\n",
    "        target = target.split(\" is \")[0].strip()\n",
    "        target = target.split(\"entity\")[1].strip()\n",
    "\n",
    "        relation = output.split(\" is \")[-1].strip()\n",
    "        relation = relation.split(\". <|endoftext|>\")[0].strip()\n",
    "\n",
    "        try:\n",
    "            relation = rel2id[relation]\n",
    "        except:\n",
    "            count += 1\n",
    "            pass\n",
    "\n",
    "        pairs.append((source, target, relation))\n",
    "    except:\n",
    "        pairs.append((\"none\", \"none\", \"none\"))\n",
    "\n",
    "print(f\"{count} / {len(outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"skai tv is a greek free - to - air television network based in piraeus . it is part of the skai group , one of the largest media groups in the country . it was relaunched in its present form on 1st of april 2006 in the athens metropolitan area , and gradually spread its coverage nationwide . besides digital terrestrial transmission , it is available on the subscription - based encrypted services of nova and cosmote tv . skai tv is also a member of digea , a consortium of private television networks introducing digital terrestrial transmission in greece . at launch , skai tv opted for dubbing all foreign language content into greek , instead of using subtitles . this is very uncommon in greece for anything except documentaries ( using voiceover dubbing ) and children 's programmes ( using lip - synced dubbing ) , so after intense criticism the station switched to using subtitles for almost all foreign shows . \",\n",
       " 'head': [['piraeus']],\n",
       " 'tail': [['greece']],\n",
       " 'head_first': 1,\n",
       " 'relation': 'P17'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"output\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for i, output in enumerate(pairs):\n",
    "    result['output'].append(output)\n",
    "    if dataset[i]['head_first'] == 1:\n",
    "        result['label'].append(('1', '2', dataset[i]['relation']))\n",
    "    else:\n",
    "        result['label'].append(('2', '1', dataset[i]['relation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result dictionary\n",
    "import pickle\n",
    "with open(\"DocRED/GPT_w_ner/result/epoch_5_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing for the outputs without ner\n",
    "pairs = []\n",
    "for output in outputs:\n",
    "    pair = []\n",
    "    # if the output doesn't end with \"<|endoftext|>\", find the lastest \";\" of the output and only take the previous part\n",
    "    if output.endswith(\"<|endoftext|>\"):\n",
    "        string = output\n",
    "\n",
    "        for line in string.split(\";\"):\n",
    "            try:\n",
    "                source = line.split(\"the source is\")[1].strip()\n",
    "                source = source.split(\"and the target is\")[0].strip()\n",
    "                if source.startswith(\"none\"):\n",
    "                    source = \"none\"\n",
    "                    target = \"none\"\n",
    "                    continue\n",
    "                target = line.split(\"the target is\")[1].strip()\n",
    "                if target.endswith(\". <|endoftext|>\"):\n",
    "                    target = target.split(\". <|endoftext|>\")[0].strip()\n",
    "                    \n",
    "                if (source, target) not in pair:\n",
    "                    pair.append((source, target))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    else:\n",
    "        string = output.split(\";\")[:-1]\n",
    "        string = [line.strip() for line in string]\n",
    "        for line in string:\n",
    "            try:\n",
    "                source = line.split(\"the source is\")[1].strip()\n",
    "                source = source.split(\"and the target is\")[0].strip()\n",
    "\n",
    "                target = line.split(\"the target is\")[1].strip()\n",
    "                if (source, target) not in pair:\n",
    "                    pair.append((source, target))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    pairs.append(pair)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the source is hampshire county and the target is united states ; the source is west virginia and the target is united states ; the source is virginia route 28 and the target is united states ; the source is william washington house and the target is united states ; the source is north romney and the target is united states ; the source is william washington house and the target is united states ; the source is west virginia route 28 and the target is united states ; the'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"output\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for output, label in zip(pairs, dataset['pair']):\n",
    "    result['output'].append(output)\n",
    "    result['label'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result dictionary\n",
    "import pickle\n",
    "with open(\"DocRED/GPT_without_ner/result/epoch_5_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"DocRED/GPT_w_ner/result/epoch_5_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length: 12275, 12275\n",
      "instance:\n",
      "('1', '2', 'P17')\n",
      "('1', '2', 'P17')\n"
     ]
    }
   ],
   "source": [
    "print(f'the length: {len(result[\"output\"])}, {len(result[\"label\"])}')\n",
    "print(f'instance:\\n{result[\"output\"][0]}\\n{result[\"label\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source and target, relation\n",
    "st_tp = 0\n",
    "st_fp = 0\n",
    "st_fn = 0\n",
    "st_tn = 0\n",
    "\n",
    "r_tp = 0\n",
    "r_fp = 0\n",
    "r_fn = 0\n",
    "r_tn = 0\n",
    "\n",
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    pair = False\n",
    "    relation = False\n",
    "    if output[0] == label[0] and output[1] == label[1]:\n",
    "        st_tp += 1\n",
    "        pair = True\n",
    "    else:\n",
    "        st_fn += 1\n",
    "        st_fp += 1\n",
    "    \n",
    "    if output[2] == label[2]:\n",
    "        r_tp += 1\n",
    "        relation = True\n",
    "    else:\n",
    "        r_fn += 1\n",
    "        r_fp += 1\n",
    "\n",
    "    if pair and relation:\n",
    "        tuple_tp += 1\n",
    "    else:\n",
    "        tuple_fn += 1\n",
    "        tuple_fp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source and target precision: 0.8511608961303462, recall: 0.8511608961303462, f1: 0.8511608961303462\n",
      "relation precision: 0.7165784114052953, recall: 0.7165784114052953, f1: 0.7165784114052953\n",
      "tuple precision: 0.6974338085539715, recall: 0.6974338085539715, f1: 0.6974338085539715\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for source and target\n",
    "st_precision = st_tp / (st_tp + st_fp)\n",
    "st_recall = st_tp / (st_tp + st_fn)\n",
    "st_f1 = 2 * st_precision * st_recall / (st_precision + st_recall)\n",
    "print(f\"source and target precision: {st_precision}, recall: {st_recall}, f1: {st_f1}\")\n",
    "\n",
    "# for relation\n",
    "r_precision = r_tp / (r_tp + r_fp)\n",
    "r_recall = r_tp / (r_tp + r_fn)\n",
    "r_f1 = 2 * r_precision * r_recall / (r_precision + r_recall)\n",
    "print(f\"relation precision: {r_precision}, recall: {r_recall}, f1: {r_f1}\")\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"DocRED/GPT_without_ner/result/epoch_5_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length: 6254, 6254\n",
      "instance:\n",
      "[('piraeus', 'greece'), ('athens metropolitan area', 'greece'), ('athens', 'greece')]\n",
      "[['piraeus', 'greece'], ['skai group', 'greece'], ['athens', 'greece'], ['skai tv', 'greece']]\n"
     ]
    }
   ],
   "source": [
    "print(f'the length: {len(result[\"output\"])}, {len(result[\"label\"])}')\n",
    "print(f'instance:\\n{result[\"output\"][0]}\\n{result[\"label\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    for pair in output:\n",
    "        true_tuple = False\n",
    "        for label_pair in label:\n",
    "            if pair[0] == label_pair[0] and pair[1] == label_pair[1]:\n",
    "                tuple_tp += 1\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fp += 1\n",
    "    \n",
    "    for label_pair in label:\n",
    "        true_tuple = False\n",
    "        for pair in output:\n",
    "            if pair[0] == label_pair[0] and pair[1] == label_pair[1]:\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple precision: 0.27586206896551724, recall: 0.172809667673716, f1: 0.2125011609547692\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple precision: 0.329153605015674, recall: 0.20828564888990617, f1: 0.2551282650343442\n"
     ]
    }
   ],
   "source": [
    "# loosen the condition for the tp\n",
    "\n",
    "\n",
    "tuple_tp = 0\n",
    "tuple_fp = 0  \n",
    "tuple_fn = 0\n",
    "tuple_tn = 0\n",
    "\n",
    "for output, label in zip(result['output'], result['label']):\n",
    "    for pair in output:\n",
    "        true_tuple = False\n",
    "        for label_pair in label:\n",
    "            if (pair[0] in label_pair[0]) and (pair[1] in label_pair[1]):\n",
    "                tuple_tp += 1\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fp += 1\n",
    "    \n",
    "    for label_pair in label:\n",
    "        true_tuple = False\n",
    "        for pair in output:\n",
    "            if (pair[0] in label_pair[0]) and (pair[1] in label_pair[1]):\n",
    "                true_tuple = True\n",
    "                break\n",
    "        if not true_tuple:\n",
    "            tuple_fn += 1\n",
    "\n",
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "print(f\"tuple precision: {tuple_precision}, recall: {tuple_recall}, f1: {tuple_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioRED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
