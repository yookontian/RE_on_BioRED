{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "spacy_model = 'en_core_sci_md'\n",
    "nlp = spacy.load(spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =  \"data/BioRED/Train.PubTator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        self.text_instances = []\n",
    "\n",
    "class PubtatorDocument(Document):\n",
    "    \n",
    "    def __init__(self, id):\n",
    "        \n",
    "        super().__init__(id)\n",
    "        self.relation_pairs = None\n",
    "        self.nary_relations = None\n",
    "        self.variant_gene_pairs = {}\n",
    "\n",
    "class TextInstance:\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.annotations = []\n",
    "        self.offset = 0\n",
    "                        \n",
    "        self.tokenized_text = ''\n",
    "        self.pos_tags = []\n",
    "        self.head = []\n",
    "        self.head_indexes = []\n",
    "        self.stems = []\n",
    "\n",
    "class AnnotationInfo:\n",
    "    \n",
    "    def __init__(self, position, length, text, ne_type):\n",
    "        self.position = position\n",
    "        self.length = length\n",
    "        self.text = text\n",
    "        self.ne_type = ne_type\n",
    "        self.ids = set()\n",
    "        self.corresponding_gene_id = ''\n",
    "        self.orig_ne_type = '' # sometime ne_type is normalized eg \"Variant\" => 'Gene'\n",
    "        self.corresponding_variant_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_id_spliter_str = r'\\,'  \n",
    "normalized_type_dict = {'SequenceVariant':'GeneOrGeneProduct'}\n",
    "has_novelty = True\n",
    "src_tgt_pairs = set(\n",
    "        [('ChemicalEntity', 'ChemicalEntity'),\n",
    "            ('ChemicalEntity', 'DiseaseOrPhenotypicFeature'),\n",
    "            ('ChemicalEntity', 'GeneOrGeneProduct'),\n",
    "            ('DiseaseOrPhenotypicFeature', 'GeneOrGeneProduct'),\n",
    "            ('GeneOrGeneProduct', 'GeneOrGeneProduct')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_annotations_2_text_instances(text_instances, annotations):\n",
    "    offset = 0\n",
    "    for text_instance in text_instances:\n",
    "        text_instance.offset = offset\n",
    "        offset += len(text_instance.text) + 1\n",
    "        \n",
    "    for annotation in annotations:\n",
    "        can_be_mapped_to_text_instance = False\n",
    "                \n",
    "        for i, text_instance in enumerate(text_instances):\n",
    "            if text_instance.offset <= annotation.position and annotation.position + annotation.length <= text_instance.offset + len(text_instance.text):\n",
    "                \n",
    "                annotation.position = annotation.position - text_instance.offset\n",
    "                text_instance.annotations.append(annotation)\n",
    "                can_be_mapped_to_text_instance = True\n",
    "                break\n",
    "        if not can_be_mapped_to_text_instance:\n",
    "            print(annotation.text)\n",
    "            print(annotation.position)\n",
    "            print(annotation.length)\n",
    "            print(annotation, 'cannot be mapped to original text')\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the info from the PubTator file\n",
    "# all_documents = load_pubtator_into_documents(in_pubtator_file, \n",
    "                                                #  normalized_type_dict = normalized_type_dict,\n",
    "                                                #  re_id_spliter_str = re_id_spliter_str)\n",
    "\n",
    "def make_doc():\n",
    "    c = 0\n",
    "    count = 20000\n",
    "    # pmid_2_index_2_groupID_dict is a dict to classify the pmids into groups\n",
    "    pmid_2_index_2_groupID_dict = None\n",
    "\n",
    "    documents = []\n",
    "    with open(filename, 'r', encoding='utf8') as pub_reader:\n",
    "        pmid = ''\n",
    "            \n",
    "        document = None\n",
    "        \n",
    "        annotations = []\n",
    "        text_instances = []\n",
    "        relation_pairs = {}\n",
    "        index2normalized_id = {}\n",
    "        id2index = {}\n",
    "\n",
    "        \n",
    "        for line in pub_reader:\n",
    "            c += 1\n",
    "            line = line.rstrip()\n",
    "\n",
    "            if line == '':\n",
    "                # print(\"line == ''\")\n",
    "\n",
    "                document = PubtatorDocument(pmid)\n",
    "                add_annotations_2_text_instances(text_instances, annotations)\n",
    "                document.text_instances = text_instances\n",
    "                document.relation_pairs = relation_pairs\n",
    "                documents.append(document)\n",
    "\n",
    "                annotations = []\n",
    "                text_instances = []\n",
    "                relation_pairs = {}\n",
    "                index2normalized_id = {}\n",
    "                id2index = {}\n",
    "                continue\n",
    "\n",
    "            tks = line.split('|')\n",
    "            if len(tks) > 1 and (tks[1] == 't' or tks[1] == 'a'):\n",
    "                #2234245\t250\t270\taudiovisual toxicity\tDisease\tD014786|D006311\n",
    "                pmid = tks[0]\n",
    "                x = TextInstance(tks[2])\n",
    "                text_instances.append(x)\n",
    "\n",
    "            else:\n",
    "                _tks = line.split('\\t')\n",
    "                if len(_tks) == 6:\n",
    "                    start = int(_tks[1])\n",
    "                    end = int(_tks[2])\n",
    "                    index = _tks[1] + '|' + _tks[2]\n",
    "                    text = _tks[3]\n",
    "                    ne_type = _tks[4]\n",
    "                    ne_type = re.sub('\\s*\\(.*?\\)\\s*$', '', ne_type)\n",
    "                    orig_ne_type = ne_type\n",
    "                    if ne_type in normalized_type_dict:\n",
    "                        ne_type = normalized_type_dict[ne_type]\n",
    "                    \n",
    "                    _anno = AnnotationInfo(start, end-start, text, ne_type)\n",
    "                    \n",
    "                    #2234245\t250\t270\taudiovisual toxicity\tDisease\tD014786|D006311\n",
    "                    ids = [x.strip('*') for x in re.split(re_id_spliter_str, _tks[5])]\n",
    "                    \n",
    "                    # if annotation has groupID then update its id\n",
    "                    if orig_ne_type == 'SequenceVariant':\n",
    "                        if pmid_2_index_2_groupID_dict != None and index in pmid_2_index_2_groupID_dict[pmid]:\n",
    "                            print(\"pmid_2_index_2_groupID_dict != None\")\n",
    "\n",
    "                            index2normalized_id[index] = pmid_2_index_2_groupID_dict[pmid][index][0] # pmid_2_tmvarID_2_groupID_dict[pmid][_id] => (var_id, gene_id)\n",
    "                            _anno.corresponding_gene_id = pmid_2_index_2_groupID_dict[pmid][index][1]\n",
    "                    for i, _id in enumerate(ids):\n",
    "                        if pmid_2_index_2_groupID_dict != None and index in pmid_2_index_2_groupID_dict[pmid]:\n",
    "                            id2index[ids[i]] = index\n",
    "                            ids[i] = pmid_2_index_2_groupID_dict[pmid][index][0] # pmid_2_tmvarID_2_groupID_dict[pmid][_id] => (var_id, gene_id)\n",
    "                            _anno.corresponding_gene_id = pmid_2_index_2_groupID_dict[pmid][index][1]\n",
    "                        else:\n",
    "                            #ids[i] = re.sub('\\s*\\(.*?\\)\\s*$', '', _id)\n",
    "                            ids[i] = _id\n",
    "\n",
    "                    _anno.orig_ne_type = orig_ne_type\n",
    "                    _anno.ids = set(ids)\n",
    "                    annotations.append(_anno)\n",
    "\n",
    "                # ==5 is the line of relation\n",
    "                elif len(_tks) == 4 or len(_tks) == 5:\n",
    "                    id1 = _tks[2]\n",
    "                    id2 = _tks[3]\n",
    "\n",
    "                    if pmid_2_index_2_groupID_dict != None and (id1 in id2index) and (id2index[id1] in index2normalized_id):\n",
    "                        id1 = index2normalized_id[id2index[id1]] # pmid_2_tmvarID_2_groupID_dict[pmid][_id] => (var_id, gene_id)\n",
    "                    if pmid_2_index_2_groupID_dict != None and (id2 in id2index) and (id2index[id2] in index2normalized_id):\n",
    "                        id2 = index2normalized_id[id2index[id2]] # pmid_2_tmvarID_2_groupID_dict[pmid][_id] => (var_id, gene_id)\n",
    "                    rel_type = _tks[1]\n",
    "                    if len(_tks) == 5:\n",
    "                        rel_type += '|' + _tks[-1]\n",
    "                    relation_pairs[(id1, id2)] = rel_type\n",
    "\n",
    "            if count == c:\n",
    "                break\n",
    "\n",
    "        if len(text_instances) != 0:\n",
    "            document = PubtatorDocument(pmid)\n",
    "            add_annotations_2_text_instances(text_instances, annotations)\n",
    "            document.text_instances = text_instances\n",
    "            document.relation_pairs = relation_pairs\n",
    "            documents.append(document)\n",
    "\n",
    "\n",
    "    print(len(documents))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _spacy_split_sentence(text, nlp):\n",
    "    offset = 0\n",
    "    offsets = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    do_not_split = False\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for sent in doc.sents:\n",
    "        # look for a sentence ending with a lower char followed by a dot, an upper char followed by a space,\n",
    "        # and then a greater char, a word 'del' followed by a dot, and a word 'viz' followed by a dot\n",
    "\n",
    "        # if finid, do not split the sentence, otherwise, split the sentence\n",
    "        if re.search(r'\\b[a-z]\\.$|[A-Z] ?\\>$|[^a-z]del\\.$| viz\\.$', sent.text):\n",
    "            if not do_not_split:\n",
    "                start = offset\n",
    "            end = offset + len(sent.text)\n",
    "            offset = end\n",
    "            for c in text[end:]:\n",
    "                if c == ' ':\n",
    "                    offset += 1\n",
    "                else:\n",
    "                    break\n",
    "            do_not_split = True\n",
    "        else:\n",
    "            if do_not_split:                \n",
    "                do_not_split = False\n",
    "                end = offset + len(sent.text)\n",
    "                offset = end\n",
    "                for c in text[end:]:\n",
    "                    if c == ' ':\n",
    "                        offset += 1\n",
    "                    else:\n",
    "                        break\n",
    "                offsets.append((start, end))\n",
    "            else:\n",
    "                start = offset\n",
    "                end = offset + len(sent.text)\n",
    "                offsets.append((start, end))\n",
    "                \n",
    "                offset = end\n",
    "                for c in text[end:]:\n",
    "                    if c == ' ':\n",
    "                        offset += 1\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "    if do_not_split:\n",
    "        offsets.append((start, end))\n",
    "    # there must be a space at the end, no space at the beginning\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def split_sentence(document, nlp):\n",
    "    new_text_instances = []\n",
    "    for text_instance in document.text_instances:\n",
    "        # split into sentences\n",
    "        offsets = [o for o in _spacy_split_sentence(text_instance.text, nlp)]\n",
    "        #offsets = [o for o in _nltk_split_sentence(text_instance.text)]\n",
    "        _tmp_text_instances = []\n",
    "        for start, end in offsets:\n",
    "            new_text_instance = TextInstance(text_instance.text[start:end])\n",
    "            new_text_instance.offset = start\n",
    "            _tmp_text_instances.append(new_text_instance)\n",
    "        for annotation in text_instance.annotations:\n",
    "            is_entity_splited = True\n",
    "            for _tmp_text_instance in _tmp_text_instances:\n",
    "                if _tmp_text_instance.offset <= annotation.position and \\\n",
    "                    (annotation.position + annotation.length) - _tmp_text_instance.offset <= len(_tmp_text_instance.text):\n",
    "                    annotation.position = annotation.position - _tmp_text_instance.offset\n",
    "                    _tmp_text_instance.annotations.append(annotation)\n",
    "                    is_entity_splited = False\n",
    "                    break\n",
    "            if is_entity_splited:\n",
    "                print(annotation.position, annotation.length, annotation.text)\n",
    "                print (' splited by Spacy\\' sentence spliter is failed to be loaded into TextInstance\\n')\n",
    "                for _tmp_text_instance in _tmp_text_instances:\n",
    "                    print (_tmp_text_instance.offset, len(_tmp_text_instance.text), _tmp_text_instance.text)\n",
    "        new_text_instances.extend(_tmp_text_instances)\n",
    "    \n",
    "    document.text_instances = new_text_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "documents = make_doc()\n",
    "\n",
    "\n",
    "# tokenize_documents_by_spacy(all_documents, spacy_model)\n",
    "for document in documents:\n",
    "    # split into sentences, with the entities, respectively\n",
    "    split_sentence(document, nlp)\n",
    "\n",
    "    # tokenize_document_by_spacy(document, nlp)\n",
    "    for text_instance in document.text_instances: \n",
    "        # remove multi spaces\n",
    "        doc = nlp(re.sub(r'\\s+', ' ', text_instance.text))\n",
    "                \n",
    "        tokens = []\n",
    "        for i, token in enumerate(doc):\n",
    "            # token -> Spacy.tokens.token.Token\n",
    "            tokens.append(token.text)\n",
    "            # Coarse-grained part-of-speech from the Universal POS tag set. -> str\n",
    "            text_instance.pos_tags.append(token.pos_)\n",
    "            # Syntactic dependency relation. -> str\n",
    "            text_instance.head.append(token.dep_)\n",
    "            # The syntactic parent, or “governor”, of this token. -> Token -> int\n",
    "            text_instance.head_indexes.append(token.head.i)\n",
    "            # Base form of the token, with no inflectional suffixes.\n",
    "            text_instance.stems.append(token.lemma_)\n",
    "\n",
    "        # punctuation with spaces before and after\n",
    "        text_instance.tokenized_text = ' '.join(tokens)\n",
    "\n",
    "\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======>len(all_documents) 400\n"
     ]
    }
   ],
   "source": [
    "all_documents = documents\n",
    "print('=======>len(all_documents)', len(all_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_all_id_pairs_by_specified(document,\n",
    "                                        src_tgt_pairs,\n",
    "                                        only_pair_in_same_sent):\n",
    "    all_pairs = set()\n",
    "    \n",
    "    if only_pair_in_same_sent:\n",
    "        for text_instance in document.text_instances:\n",
    "                    \n",
    "            all_id_infos_list = list()\n",
    "            _all_id_infos_set = set()\n",
    "            \n",
    "            text_instance.annotations = sorted(text_instance.annotations, key=lambda x: x.position, reverse=False)\n",
    "            for annotation in text_instance.annotations:\n",
    "                for id in annotation.ids:\n",
    "                    if (id, annotation.ne_type) not in _all_id_infos_set:\n",
    "                        all_id_infos_list.append((id, annotation.ne_type))\n",
    "                        _all_id_infos_set.add((id, annotation.ne_type))\n",
    "            \n",
    "            #print('====>len(all_id_infos_list)', len(all_id_infos_list))\n",
    "            for i in range(0, len(all_id_infos_list) - 1):\n",
    "                id1_info = all_id_infos_list[i]\n",
    "                for j in range(i + 1, len(all_id_infos_list)):\n",
    "                    id2_info = all_id_infos_list[j]\n",
    "                    #print(id1_info[0], id2_info[1], id1_info[1], id2_info[1])\n",
    "                    for src_ne_type, tgt_ne_type in src_tgt_pairs:\n",
    "                        if id1_info[1] == src_ne_type and id2_info[1] == tgt_ne_type:\n",
    "                            all_pairs.add((id1_info[0], id2_info[0], id1_info[1], id2_info[1]))\n",
    "                            break\n",
    "                            #print('OK')\n",
    "                        elif id2_info[1] == src_ne_type and id1_info[1] == tgt_ne_type:\n",
    "                            all_pairs.add((id2_info[0], id1_info[0], id2_info[1], id1_info[1]))\n",
    "                            break\n",
    "                        #print('OK')\n",
    "    else:    \n",
    "        all_id_infos_list = list()\n",
    "        _all_id_infos_set = set()\n",
    "        \n",
    "        for text_instance in document.text_instances:\n",
    "            text_instance.annotations = sorted(text_instance.annotations, key=lambda x: x.position, reverse=False)\n",
    "            for annotation in text_instance.annotations:\n",
    "                for id in annotation.ids:\n",
    "                    if (id, annotation.ne_type) not in _all_id_infos_set:\n",
    "                        all_id_infos_list.append((id, annotation.ne_type))\n",
    "                        _all_id_infos_set.add((id, annotation.ne_type))\n",
    "        \n",
    "        #print('====>len(all_id_infos_list)', len(all_id_infos_list))\n",
    "        for i in range(0, len(all_id_infos_list) - 1):\n",
    "            id1_info = all_id_infos_list[i]\n",
    "            for j in range(i + 1, len(all_id_infos_list)):\n",
    "                id2_info = all_id_infos_list[j]\n",
    "                #print(id1_info[0], id2_info[1], id1_info[1], id2_info[1])\n",
    "                for src_ne_type, tgt_ne_type in src_tgt_pairs:\n",
    "                    if id1_info[1] == src_ne_type and id2_info[1] == tgt_ne_type:\n",
    "                        all_pairs.add((id1_info[0], id2_info[0], id1_info[1], id2_info[1]))\n",
    "                        break\n",
    "                        #print('OK')\n",
    "                    elif id2_info[1] == src_ne_type and id1_info[1] == tgt_ne_type:\n",
    "                        all_pairs.add((id2_info[0], id1_info[0], id2_info[1], id1_info[1]))\n",
    "                        break\n",
    "                        #print('OK')\n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_instance_2_iob2(text_instance, id1, id2, do_mask_other_nes):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    for token in text_instance.tokenized_text.split(' '):\n",
    "        tokens.append(token)\n",
    "        labels.append('O')\n",
    "        \n",
    "    annotation_indexes_wo_count_space = []\n",
    "    for annotation in text_instance.annotations:\n",
    "        start = len(text_instance.text[:annotation.position].replace(' ', ''))\n",
    "        end = start + len(annotation.text.replace(' ', ''))\n",
    "        annotation_indexes_wo_count_space.append((start, end))\n",
    "    \n",
    "    for (start, end), annotation in zip(annotation_indexes_wo_count_space, text_instance.annotations):\n",
    "        offset = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            if offset == start:\n",
    "                if id1 in annotation.ids:\n",
    "                    labels[i] = \"B-\" + annotation.ne_type + 'Src'\n",
    "                elif id2 in annotation.ids:\n",
    "                    labels[i] = \"B-\" + annotation.ne_type + 'Tgt'\n",
    "                elif do_mask_other_nes:\n",
    "                    labels[i] = \"B-\" + annotation.ne_type\n",
    "            elif start < offset and offset < end:\n",
    "                if id1 in annotation.ids:\n",
    "                    labels[i] = \"I-\" + annotation.ne_type + 'Src'\n",
    "                elif id2 in annotation.ids:\n",
    "                    labels[i] = \"I-\" + annotation.ne_type + 'Tgt'\n",
    "                elif do_mask_other_nes:\n",
    "                    labels[i] = \"I-\" + annotation.ne_type\n",
    "            elif offset < start and start < offset + len(token): #ex: renin-@angiotensin$\n",
    "                if id1 in annotation.ids:\n",
    "                    labels[i] = \"B-\" + annotation.ne_type + 'Src'\n",
    "                elif id2 in annotation.ids:\n",
    "                    labels[i] = \"B-\" + annotation.ne_type + 'Tgt'\n",
    "                elif do_mask_other_nes:\n",
    "                    labels[i] = \"B-\" + annotation.ne_type                \n",
    "                    \n",
    "            offset += len(token)\n",
    "        \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_neighbor_indices_and_add_end_tag(tagged_sent,\n",
    "                                           ne_positions,\n",
    "                                           ne_list, \n",
    "                                           neighbor_indices,\n",
    "                                           has_end_tag):\n",
    "                    \n",
    "    new_tagged_sent = tagged_sent.split(' ')\n",
    "    \n",
    "    # if parsing sentence fail => len(neighbor_indices) == 0\n",
    "    if len(neighbor_indices) > 0:\n",
    "        # update indices by using ne_positions if indices > ne_positions then shift NE's length\n",
    "        for _neighbor_indices in neighbor_indices:\n",
    "            \n",
    "            for i, _indice in enumerate(_neighbor_indices):\n",
    "                \n",
    "                if not has_end_tag:\n",
    "                    _shift_num = 0\n",
    "                else:\n",
    "                    # we consider \"end tag\" as part of ne text\n",
    "                    _shift_num = 1\n",
    "                for j, shift_point_indice in enumerate(ne_positions):\n",
    "                    if _indice > shift_point_indice:\n",
    "                        _shift_num += len(ne_list[j].split(' '))\n",
    "                _neighbor_indices[i] += _shift_num\n",
    "            \n",
    "    ne_positions.reverse()\n",
    "    ne_list.reverse()\n",
    "        \n",
    "    for ne_position, ne_text in zip(ne_positions, ne_list):\n",
    "        \n",
    "        if len(neighbor_indices) > 0:\n",
    "            ne_tag_neighbor_indices = neighbor_indices[ne_position]\n",
    "        \n",
    "        # add ne into neighbor and tagged sent\n",
    "        for i, _ne_token in enumerate(ne_text.split(' ')):\n",
    "            \n",
    "            if len(neighbor_indices) > 0:\n",
    "                # ne text point to ne tag\n",
    "                neighbor_indices.insert(ne_position + i, [ne_position])\n",
    "                neighbor_indices[ne_position + i] += ne_tag_neighbor_indices\n",
    "            \n",
    "            # insert ne text\n",
    "            new_tagged_sent.insert(ne_position + 1 + i, _ne_token)\n",
    "        \n",
    "        \n",
    "        if has_end_tag:\n",
    "            # ne text point to ne tag\n",
    "            end_tag_index = ne_position + len(ne_text.split(' ')) + 1\n",
    "            if len(neighbor_indices) > 0:\n",
    "                neighbor_indices.insert(end_tag_index, [ne_position])\n",
    "                neighbor_indices[end_tag_index] += ne_tag_neighbor_indices\n",
    "            new_tagged_sent.insert(end_tag_index, new_tagged_sent[ne_position].replace('@', '@/'))\n",
    "                        \n",
    "    return ' '.join(new_tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_iob2_to_tagged_sent(\n",
    "        tokens, \n",
    "        labels, \n",
    "        in_neighbors_list,\n",
    "        token_offset,\n",
    "        to_mask_src_and_tgt = False,\n",
    "        has_end_tag = False):\n",
    "    \n",
    "        \n",
    "    num_orig_tokens = len(tokens)\n",
    "    \n",
    "    previous_label = 'O'\n",
    "    \n",
    "    orig_token_index_2_new_token_index_mapping = []\n",
    "    \n",
    "    current_idx = -1\n",
    "    \n",
    "    tagged_sent = ''\n",
    "    ne_type = ''\n",
    "    ne_text = ''\n",
    "    ne_list = []\n",
    "    # convert IOB2 to bert format\n",
    "    # NEs are replaced by tags\n",
    "    for i, (token, label) in enumerate(zip(tokens, labels)):\n",
    "        if label == 'O':\n",
    "            if previous_label != 'O':\n",
    "                tagged_sent += '$ ' + token\n",
    "                #print('1 ne_list.append(ne_text)', ne_text)\n",
    "                ne_list.append(ne_text)\n",
    "                ne_text = ''\n",
    "            else:\n",
    "                tagged_sent += ' ' + token    \n",
    "            current_idx += 1\n",
    "                \n",
    "        elif label.startswith('B-'):\n",
    "            if previous_label != 'O':\n",
    "                tagged_sent += '$ @' + label[2:]\n",
    "                #print('2 ne_list.append(ne_text)', ne_text)\n",
    "                ne_list.append(ne_text)\n",
    "                ne_text = token\n",
    "                ne_type = label[2:]\n",
    "            else:\n",
    "                tagged_sent += ' @' + label[2:]\n",
    "                ne_text = token\n",
    "                ne_type = label[2:]\n",
    "            current_idx += 1\n",
    "                \n",
    "        elif label.startswith('I-'):\n",
    "            ne_text += ' ' + token\n",
    "        #print('=================>')\n",
    "        #print(i, token, label)\n",
    "        #print(tagged_sent)\n",
    "        previous_label = label\n",
    "        orig_token_index_2_new_token_index_mapping.append(current_idx)\n",
    "    if ne_text != '':\n",
    "        ne_list.append(ne_text)\n",
    "        ne_text = ''\n",
    "    tagged_sent = tagged_sent.strip()\n",
    "    if previous_label != 'O':\n",
    "        tagged_sent += '$'\n",
    "            \n",
    "    #    \n",
    "    \n",
    "    # update neighbor index\n",
    "    previous_idx = 0\n",
    "    _new_neighbors = [] # \n",
    "    \n",
    "        \n",
    "    _tokens = tagged_sent.split(' ')\n",
    "    \n",
    "    ne_positions = []\n",
    "    for i in range(len(_tokens)):\n",
    "        token = _tokens[i]\n",
    "        if token.startswith('@') and token.endswith('$'):\n",
    "            ne_positions.append(i)\n",
    "            \n",
    "    new_in_neighbors_list = []\n",
    "    if len(in_neighbors_list) != 0:\n",
    "        # update in_neighbors_list to new_in_neighbors_list by orig_token_index_2_new_token_index_mapping\n",
    "        for i in range(num_orig_tokens):\n",
    "            if previous_idx == orig_token_index_2_new_token_index_mapping[i]:\n",
    "                for neighbor_idx in in_neighbors_list[i]:\n",
    "                    _new_neighbors.append(orig_token_index_2_new_token_index_mapping[neighbor_idx])\n",
    "            else:\n",
    "                new_in_neighbors_list.append(list(set(_new_neighbors)))\n",
    "                _new_neighbors = []\n",
    "                for neighbor_idx in in_neighbors_list[i]:\n",
    "                    _new_neighbors.append(orig_token_index_2_new_token_index_mapping[neighbor_idx])\n",
    "            previous_idx = orig_token_index_2_new_token_index_mapping[i]\n",
    "        new_in_neighbors_list.append(list(set(_new_neighbors)))\n",
    "    #\n",
    "    \n",
    "    # insert ne text and update neighbor index again\n",
    "    if to_mask_src_and_tgt == False:\n",
    "        tagged_sent = shift_neighbor_indices_and_add_end_tag(\n",
    "                               tagged_sent,\n",
    "                               ne_positions,\n",
    "                               ne_list,\n",
    "                               new_in_neighbors_list,\n",
    "                               has_end_tag)\n",
    "    #\n",
    "\n",
    "    # add token_offset to neighbor index\n",
    "    new_in_neighbors_list = ['|'.join([str(i + token_offset) for i in set(neighbors)]) for neighbors in new_in_neighbors_list]\n",
    "    \n",
    "    \n",
    "    return tagged_sent.strip(),\\\n",
    "           ' '.join(new_in_neighbors_list),\\\n",
    "           token_offset + len(new_in_neighbors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.54131731351024\n"
     ]
    }
   ],
   "source": [
    "out_bert_file = out_tsv_file = \"data/BioRED/t\" + 'train.tsv'\n",
    "only_pair_in_same_sent = False\n",
    "neg_label = 'None'\n",
    "pos_label = ''\n",
    "do_mask_other_nes = False\n",
    "to_mask_src_and_tgt = False\n",
    "has_end_tag = True\n",
    "# dump_documents_2_bert_format(\n",
    "#     all_documents = all_documents, \n",
    "#     out_bert_file = out_tsv_file, \n",
    "#     src_tgt_pairs = src_tgt_pairs,\n",
    "#     has_novelty = has_novelty)\n",
    "\n",
    "num_seq_lens = []\n",
    "\n",
    "_index = 0\n",
    "\n",
    "with open(out_bert_file, 'w', encoding='utf8') as bert_writer:\n",
    "            \n",
    "    number_unique_YES_instances = 0\n",
    "    for document in all_documents:\n",
    "        pmid = document.id\n",
    "        # if only extract the pairs within one sentence\n",
    "        # (identifier1, identifier2, ne_type1, ne_type2)\n",
    "        all_pairs = enumerate_all_id_pairs_by_specified(document,\n",
    "                                                src_tgt_pairs,\n",
    "                                                only_pair_in_same_sent)\n",
    "        unique_YES_instances = set()\n",
    "\n",
    "        # print('===============>document.relation_pairs', document.relation_pairs)\n",
    "        # print('===============>all_pairs', all_pairs)       \n",
    "        # for pairs have two entities\n",
    "        for relation_pair in all_pairs:\n",
    "    \n",
    "            if not has_novelty:\n",
    "                relation_label = neg_label\n",
    "            else:\n",
    "                relation_label = neg_label + '|None' # rel_type|novelty novelty => ['None', 'No', 'Novel']\n",
    "            \n",
    "            # print('=================>relation_pair', relation_pair)\n",
    "            if not document.relation_pairs:\n",
    "                # print('=================>no relation_pair', document.id)\n",
    "                document.relation_pairs = {}\n",
    "            \n",
    "            if (relation_pair[0], relation_pair[1]) in document.relation_pairs:\n",
    "                relation_label = document.relation_pairs[(relation_pair[0], relation_pair[1])]\n",
    "                if pos_label != '' and (not has_novelty):\n",
    "                    relation_label = pos_label\n",
    "            elif (relation_pair[1], relation_pair[0]) in document.relation_pairs:\n",
    "                relation_label = document.relation_pairs[(relation_pair[1], relation_pair[0])]\n",
    "                if pos_label != '' and (not has_novelty):\n",
    "                    relation_label = pos_label\n",
    "            id1 = relation_pair[0]\n",
    "            id2 = relation_pair[1]    \n",
    "            id1type = relation_pair[2]\n",
    "            id2type = relation_pair[3]\n",
    "            \n",
    "            tagged_sents = []\n",
    "            all_sents_in_neighbors = []\n",
    "            #all_sents_out_neighbors = []\n",
    "            \n",
    "            is_in_same_sent = False\n",
    "            \n",
    "            src_sent_ids = []\n",
    "            tgt_sent_ids = []\n",
    "                            \n",
    "            token_offset = 0\n",
    "\n",
    "            \n",
    "            for sent_id, text_instance in enumerate(document.text_instances):\n",
    "                #  labels: [O, O, O, O, ..., O], length = len(tokens)\n",
    "                tokens, labels = convert_text_instance_2_iob2(text_instance, id1, id2, do_mask_other_nes)\n",
    "                \n",
    "                #print(' '.join(tokens))\n",
    "                \n",
    "                # in_neighbors_list, _ = get_in_neighbors_list(text_instance)\n",
    "                in_neighbors_list = []\n",
    "                in_neighbors_head_list = []\n",
    "                for current_idx, (head, head_idx) in enumerate(zip(\n",
    "                                                    text_instance.head,\n",
    "                                                    text_instance.head_indexes)):\n",
    "                    neighbors = []\n",
    "                    neighbors_head = []\n",
    "                    \n",
    "                    neighbors.append(head_idx)\n",
    "                    neighbors_head.append(head)\n",
    "                    \n",
    "                    in_neighbors_list.append(neighbors)\n",
    "                    in_neighbors_head_list.append(neighbors_head)\n",
    "                #out_neighbors_list, _ = get_out_neighbors_list(text_instance)\n",
    "                \n",
    "                # raise if neighbor is wrong\n",
    "                if len(tokens) != len(in_neighbors_list):\n",
    "                    print('==================>')\n",
    "                    print('len(tokens)', len(tokens))\n",
    "                    print('len(in_neighbors_list)', len(in_neighbors_list))\n",
    "                    print('tokens', tokens)\n",
    "                    print(document.id, sent_id)\n",
    "                    in_neighbors_list = []\n",
    "                #\n",
    "                \n",
    "                # check if Source and Target are in the same sentence\n",
    "                if not is_in_same_sent:\n",
    "                    is_Src_in = False\n",
    "                    is_Tgt_in = False\n",
    "                    for _label in labels:\n",
    "                        if 'Src' in _label:\n",
    "                            is_Src_in = True\n",
    "                            src_sent_ids.append(sent_id)\n",
    "                            break\n",
    "                    for _label in labels:\n",
    "                        if 'Tgt' in _label:\n",
    "                            is_Tgt_in = True\n",
    "                            tgt_sent_ids.append(sent_id)\n",
    "                            break\n",
    "                    if is_Src_in and is_Tgt_in:\n",
    "                        is_in_same_sent = True\n",
    "                #\n",
    "                    \n",
    "                \n",
    "                tagged_sent, in_neighbors_str, token_offset =\\\n",
    "                    convert_iob2_to_tagged_sent(\n",
    "                        tokens,\n",
    "                        labels,\n",
    "                        in_neighbors_list,\n",
    "                        #out_neighbors_list,\n",
    "                        token_offset,\n",
    "                        to_mask_src_and_tgt,\n",
    "                        has_end_tag)\n",
    "                    \n",
    "                only_co_occurrence_sent = False\n",
    "                if only_co_occurrence_sent:\n",
    "                    if is_in_same_sent:\n",
    "                        tagged_sents.append(tagged_sent)\n",
    "                        all_sents_in_neighbors.append(in_neighbors_str)\n",
    "                else:\n",
    "                    tagged_sents.append(tagged_sent)\n",
    "                    all_sents_in_neighbors.append(in_neighbors_str)\n",
    "                #all_sents_out_neighbors.append(out_neighbors_str)\n",
    "                \n",
    "            min_sents_window = 100\n",
    "            for src_sent_id in src_sent_ids:\n",
    "                for tgt_sent_id in tgt_sent_ids:\n",
    "                    _min_sents_window = abs(src_sent_id - tgt_sent_id)\n",
    "                    if _min_sents_window < min_sents_window:\n",
    "                        min_sents_window = _min_sents_window\n",
    "                        \n",
    "            num_seq_lens.append(float(len(tagged_sent.split(' '))))\n",
    "\n",
    "\n",
    "            #print('================>id1', id1)\n",
    "            #print('================>all_sents_in_neighbors', all_sents_in_neighbors)\n",
    "            \n",
    "            out_sent = ' '.join(tagged_sents)\n",
    "            \n",
    "            if id1 == '-1' or id2 == '-1':\n",
    "                continue\n",
    "            if ' '.join(tagged_sents) == '':\n",
    "                continue\n",
    "            has_ne_type = True\n",
    "            if has_ne_type:\n",
    "                instance = document.id + '\\t' +\\\n",
    "                            id1type + '\\t' +\\\n",
    "                            id2type + '\\t' +\\\n",
    "                            id1 + '\\t' +\\\n",
    "                            id2 + '\\t' +\\\n",
    "                            str(is_in_same_sent) + '\\t' +\\\n",
    "                            str(min_sents_window) + '\\t' +\\\n",
    "                            out_sent\n",
    "                            #' '.join(all_sents_in_neighbors)\n",
    "                        #' '.join(all_sents_in_neighbors) + '\\t' +\\\n",
    "                        #' '.join(all_sents_out_neighbors)\n",
    "            else:\n",
    "                instance = document.id + '\\t' +\\\n",
    "                            id1 + '\\t' +\\\n",
    "                            id2 + '\\t' +\\\n",
    "                            str(is_in_same_sent) + '\\t' +\\\n",
    "                            str(min_sents_window) + '\\t' +\\\n",
    "                            out_sent\n",
    "                            #' '.join(all_sents_in_neighbors)\n",
    "                \n",
    "            if relation_label != neg_label:\n",
    "                unique_YES_instances.add(instance)\n",
    "            \n",
    "            is_test_set = False\n",
    "            if is_test_set or (id1 != '-' and id2 != '-'):\n",
    "                if has_novelty:\n",
    "                    relation_label = relation_label.replace('|', '\\t')\n",
    "                else:\n",
    "                    relation_label = relation_label.split('|')[0]\n",
    "                bert_writer.write(instance + '\\t' + \n",
    "                                    relation_label + '\\n')\n",
    "                            \n",
    "        number_unique_YES_instances += len(unique_YES_instances)\n",
    "                \n",
    "        bert_writer.flush()\n",
    "        # break\n",
    "\n",
    "    # average length of tagged sentences\n",
    "    print(sum(num_seq_lens) / len(num_seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24927617',\n",
       " 'ChemicalEntity',\n",
       " 'ChemicalEntity',\n",
       " 'C486464',\n",
       " 'D019821',\n",
       " 'True',\n",
       " '0',\n",
       " \"Rhabdomyolysis in a hepatitis C virus infected patient treated with @ChemicalEntitySrc$ telaprevir @/ChemicalEntitySrc$ and @ChemicalEntityTgt$ simvastatin @/ChemicalEntityTgt$ . A 46-year old man with a chronic hepatitis C virus infection received triple therapy with ribavirin , pegylated interferon and @ChemicalEntitySrc$ telaprevir @/ChemicalEntitySrc$ . The patient also received @ChemicalEntityTgt$ simvastatin @/ChemicalEntityTgt$ . One month after starting the antiviral therapy , the patient was admitted to the hospital because he developed rhabdomyolysis . At admission @ChemicalEntityTgt$ simvastatin @/ChemicalEntityTgt$ and all antiviral drugs were discontinued because toxicity due to a drug-drug interaction was suspected . The creatine kinase peaked at 62,246 IU/L and the patient was treated with intravenous normal saline . The patient 's renal function remained unaffected . Fourteen days after hospitalization , creatine kinase level had returned to 230 IU/L and the patient was discharged . @ChemicalEntitySrc$ Telaprevir @/ChemicalEntitySrc$ was considered the probable causative agent of an interaction with @ChemicalEntityTgt$ simvastatin @/ChemicalEntityTgt$ according to the Drug Interaction Probability Scale . The interaction is due to inhibition of CYP3A4-mediated @ChemicalEntityTgt$ simvastatin @/ChemicalEntityTgt$ clearance . @ChemicalEntityTgt$ Simvastatin @/ChemicalEntityTgt$ plasma concentration increased 30 times in this patient and @ChemicalEntityTgt$ statin @/ChemicalEntityTgt$ induced muscle toxicity is related to the concentration of the @ChemicalEntityTgt$ statin @/ChemicalEntityTgt$ in blood . In conclusion , with this case we illustrate that @ChemicalEntitySrc$ telaprevir @/ChemicalEntitySrc$ as well as @ChemicalEntityTgt$ statins @/ChemicalEntityTgt$ are susceptible to clinical relevant drug-drug interactions .\"]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drug_Interaction\\tNovel'"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/BioRED/ttrain.tsv'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_bert_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_novelty = False\n",
    "\n",
    "# gen_biored_dataset(\n",
    "#     in_data_dir  = in_data_dir,\n",
    "#     out_data_dir = out_data_dir,\n",
    "#     spacy_model  = spacy_model,\n",
    "#     re_id_spliter_str = re_id_spliter_str,\n",
    "#     normalized_type_dict = normalized_type_dict,\n",
    "#     has_novelty          = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hepatocyte nuclear factor-6 : associations between genetic variability and type II diabetes and between genetic variability and estimates of insulin secretion .'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hepatocyte nuclear factor-6 : associations between genetic variability and type II diabetes and between genetic variability and estimates of insulin secretion .',\n",
       " 'The transcription factor hepatocyte nuclear factor (HNF)-6 is an upstream regulator of several genes involved in the pathogenesis of maturity-onset diabetes of the young .',\n",
       " 'We therefore tested the hypothesis that variability in the HNF-6 gene is associated with subsets of Type II ( non-insulin-dependent ) diabetes mellitus and estimates of insulin secretion in @ChemicalEntitySrc$ glucose @/ChemicalEntitySrc$ tolerant subjects .',\n",
       " 'We cloned the coding region as well as the intron-exon boundaries of the HNF-6 gene . W',\n",
       " 'e then examined them on genomic DNA in six MODY probands without mutations in the MODY1 , MODY3 and MODY4 genes and in 54 patients with late-onset Type II diabetes by combined single strand conformational polymorphism-heteroduplex analysis followed by direct sequencing of identified variants .',\n",
       " 'An identified missense variant was examined in association studies and genotype-phenotype studies .',\n",
       " 'We identified two silent and one missense ( @GeneOrGeneProductTgt$ Pro75 Ala @/GeneOrGeneProductTgt$ ) variant . I',\n",
       " 'n an association study the allelic frequency of the @GeneOrGeneProductTgt$ Pro75Ala @/GeneOrGeneProductTgt$ polymorphism was 3.2 % ( 95 % confidence interval , 1.9 - 4.5 ) in 330 patients with Type II diabetes mellitus compared with 4.2 % ( 2.4 - 6.0 ) in 238 age-matched @ChemicalEntitySrc$ glucose @/ChemicalEntitySrc$ tolerant control subjects .',\n",
       " 'Moreover , in studies of 238 middle-aged @ChemicalEntitySrc$ glucose @/ChemicalEntitySrc$ tolerant subjects , of 226 @ChemicalEntitySrc$ glucose @/ChemicalEntitySrc$ tolerant offspring of Type II diabetic patients and of 367 young healthy subjects , the carriers of the polymorphism did not differ from non-carriers in @ChemicalEntitySrc$ glucose @/ChemicalEntitySrc$ induced serum insulin or C-peptide responses .',\n",
       " 'Mutations in the coding region of the HNF-6 gene are not associated with Type II diabetes or with changes in insulin responses to @ChemicalEntitySrc$ glucose @/ChemicalEntitySrc$ among the Caucasians examined .']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 170),\n",
       " (171, 385),\n",
       " (388, 474),\n",
       " (474, 767),\n",
       " (767, 865),\n",
       " (868, 932),\n",
       " (932, 1180),\n",
       " (1180, 1467),\n",
       " (1470, 1640)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spacy_split_sentence(document.text_instances[1].text, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDescription of train data:\\n?\\npmid,\\nentity1 type\\nentity2 type\\n4 x \\nidentifier(in many formats related to many database):\\n    Type | (Normalized component or identifier) | Database\\n    Gene: (19), NCBI Gene\\n    Variant: (p|SUB|S|276|T), dbSNP\\n    Variant: (RS#:2234671), dbSNP\\n    Species:(3175), NCBI Taxonomy\\n    Disease: (D003409), MEDIC (a combination of MESH and OMIM)\\n    CHemical: (D013726), MESH (Chemicals and Drugs Category)\\n    CellLine: (CVCL_1452), Cellosaurus\\nhas_novelty: if the entity showed in abstract is novel\\nneg_label\\n?\\ntext\\nPositive_correlation\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description of train data:\n",
    "pmid\n",
    "id1 type\n",
    "id2 type\n",
    "identifier(in many formats related to many database):\n",
    "    Type | (Normalized component or identifier) | Database\n",
    "    Gene: (19), NCBI Gene\n",
    "    Variant: (p|SUB|S|276|T), dbSNP\n",
    "    Variant: (RS#:2234671), dbSNP\n",
    "    Species:(3175), NCBI Taxonomy\n",
    "    Disease: (D003409), MEDIC (a combination of MESH and OMIM)\n",
    "    CHemical: (D013726), MESH (Chemicals and Drugs Category)\n",
    "    CellLine: (CVCL_1452), Cellosaurus\n",
    "id1\n",
    "id2\n",
    "is_in_same_sent\n",
    "min_sents_window\n",
    "sentence\n",
    "relation_label\n",
    "novelty\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
