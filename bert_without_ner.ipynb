{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model: \n",
    "\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "The input be like:\n",
    "\n",
    "    [cls] [relation] [sep] \"text\"\n",
    "\n",
    "(the ner1 and ner2 tags depends on the first occurence orders)\n",
    "\n",
    "The output be like: (length==512)\n",
    "\n",
    "    [out] [src-b] [src-in] [tgt-b] [tgt-in] [out]\n",
    "\n",
    "or\n",
    "\n",
    "    [out] [out] [out] [out] [out] [out] [out] [out] [out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_special_tokens': ['[Association]', '[Bind]', '[Comparison]', '[Conversion]', '[Cotreatment]', '[Drug_Interaction]', '[Negative_Correlation]', '[Positive_Correlation]']} \n",
      " ['[pad]', '[src-b]', '[src-in]', '[tgt-b]', '[tgt-in]', '[out]']\n"
     ]
    }
   ],
   "source": [
    "# load labels for bert_w_ner\n",
    "additional_tokens, labels, id2label, label2id = get_labels(mode='bert_without_ner')\n",
    "print(additional_tokens, \"\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [UNK]\n",
      "3: [SEP]\n",
      "0: [PAD]\n",
      "2: [CLS]\n",
      "4: [MASK]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "for id in [1, 3, 0, 2, 4]:\n",
    "    print(f\"{id}: {tokenizer.decode(id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 8 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert_without_ner/bert_without_ner_tokenizer/tokenizer_config.json',\n",
       " 'bert_without_ner/bert_without_ner_tokenizer/special_tokens_map.json',\n",
       " 'bert_without_ner/bert_without_ner_tokenizer/vocab.txt',\n",
       " 'bert_without_ner/bert_without_ner_tokenizer/added_tokens.json',\n",
       " 'bert_without_ner/bert_without_ner_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding new tokens to the tokenizer\n",
    "# since I haven't load the model so I will resize the embedding of the model later]\n",
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(\"bert_without_ner/bert_without_ner_tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import make_bert_re_data\n",
    "from data_preprocessing import bert_w_ner_preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and valid file paths\n",
    "train_file_path = 'data/BioRED/processed/train.tsv'\n",
    "valid_file_path = 'data/BioRED/processed/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bert_re data\n",
    "train_data_raw = make_bert_re_data(file_path=train_file_path, lower=True, no_ner_input=True)\n",
    "valid_data_raw = make_bert_re_data(file_path=valid_file_path, lower=True, no_ner_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into Dataset type\n",
    "train_data_raw = Dataset.from_dict(train_data_raw)\n",
    "valid_data_raw = Dataset.from_dict(valid_data_raw)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_data_raw,\n",
    "    \"valid\": valid_data_raw\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pmids', 'input_texts', 'input_relations', 'outputs'],\n",
       "        num_rows: 28723\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['pmids', 'input_texts', 'input_relations', 'outputs'],\n",
       "        num_rows: 7922\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815dceb03b674003af7ae3f5315fb7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68bdc7d91934169a5fd958490e8b50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"inputs\", \"outputs\", \"pmids\"])\n",
    "tokenized_datasets = dataset.map(lambda example: bert_w_ner_preprocess_function(example, tokenizer, mode=\"bert_without_ner\"), batched=True, remove_columns=[\"input_texts\", \"input_relations\", \"outputs\", \"pmids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 28723\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7922\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the number of trainable parameters in the model.\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[pad]', '[src-b]', '[src-in]', '[tgt-b]', '[tgt-in]', '[out]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "print(labels)\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint,\n",
    "                                                        num_labels=len(labels),\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id,\n",
    "                                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding special tokens to the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30530, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108902406 || all params: 108902406 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # only consider non-padding tokens [:4]\n",
    "    true_labels = [[id2label[l.item()] for l in label[:509]] for label in labels]\n",
    "    true_predictions = [\n",
    "    [id2label[p.item()] for (p, l) in zip(prediction[:509], label[:509])]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customized loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "tokenized_datasets\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        # only consider first 4 columns of the batched loss and the batched labels\n",
    "        logits = logits[:, :509, :]\n",
    "        labels = labels[:, :509]\n",
    "        loss = loss_fct(logits.reshape(-1, self.model.config.num_labels), labels.reshape(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 4,  ..., 0, 0, 0],\n",
       "        [5, 5, 5,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/BioRED/wandb/run-20230625_124539-n97rmpuu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/BERT/runs/n97rmpuu' target=\"_blank\">bert_without_ner_epoch_5_loss2</a></strong> to <a href='https://wandb.ai/tian1995/BERT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/BERT' target=\"_blank\">https://wandb.ai/tian1995/BERT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/BERT/runs/n97rmpuu' target=\"_blank\">https://wandb.ai/tian1995/BERT/runs/n97rmpuu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/BERT/runs/n97rmpuu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7010f56b30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"BERT\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"bert_without_ner_epoch_5_loss2\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",\n",
    "    # per_device_train_batch_size=4,\n",
    "    # per_device_eval_batch_size=4,\n",
    "    auto_find_batch_size=True,\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = CustomTrainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"valid\"],\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e96ec7fa4445e4b31981bbb7e668b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0843, 'learning_rate': 1.9443052074631025e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0352, 'learning_rate': 1.8886104149262045e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0304, 'learning_rate': 1.8329156223893068e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0286, 'learning_rate': 1.7772208298524088e-05, 'epoch': 0.56}\n",
      "{'loss': 0.028, 'learning_rate': 1.721526037315511e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0257, 'learning_rate': 1.6658312447786135e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0242, 'learning_rate': 1.6101364522417155e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2d4be90f524fd5955e597cc257e237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [out] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [pad] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026951752603054047, 'eval_precision': 0.6794690507685992, 'eval_recall': 0.3748270048564455, 'eval_f1': 0.4831344058121433, 'eval_accuracy': 0.991852040697389, 'eval_runtime': 172.0931, 'eval_samples_per_second': 46.033, 'eval_steps_per_second': 5.759, 'epoch': 1.0}\n",
      "{'loss': 0.0237, 'learning_rate': 1.5544416597048178e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0232, 'learning_rate': 1.49874686716792e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0228, 'learning_rate': 1.4430520746310221e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0227, 'learning_rate': 1.3873572820941243e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0203, 'learning_rate': 1.3316624895572265e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0207, 'learning_rate': 1.2759676970203288e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0219, 'learning_rate': 1.220272904483431e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5d4b7bde05490bbfd08c7a17f0a009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [out] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [pad] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02589261345565319, 'eval_precision': 0.6360639007732465, 'eval_recall': 0.40776528018922525, 'eval_f1': 0.496948695145512, 'eval_accuracy': 0.9918036811763417, 'eval_runtime': 171.9126, 'eval_samples_per_second': 46.082, 'eval_steps_per_second': 5.765, 'epoch': 2.0}\n",
      "{'loss': 0.019, 'learning_rate': 1.1645781119465331e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0209, 'learning_rate': 1.1088833194096353e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0198, 'learning_rate': 1.0531885268727375e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0194, 'learning_rate': 9.974937343358396e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0198, 'learning_rate': 9.417989417989418e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0191, 'learning_rate': 8.86104149262044e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0202, 'learning_rate': 8.304093567251463e-06, 'epoch': 2.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e7d388fea2451fb7dea76b4753ac45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [out] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [pad] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026785697788000107, 'eval_precision': 0.6227006544953496, 'eval_recall': 0.4548702850959966, 'eval_f1': 0.5257157813613297, 'eval_accuracy': 0.9917235779696838, 'eval_runtime': 172.0715, 'eval_samples_per_second': 46.039, 'eval_steps_per_second': 5.759, 'epoch': 3.0}\n",
      "{'loss': 0.0203, 'learning_rate': 7.747145641882484e-06, 'epoch': 3.06}\n",
      "{'loss': 0.0185, 'learning_rate': 7.190197716513506e-06, 'epoch': 3.2}\n",
      "{'loss': 0.018, 'learning_rate': 6.6332497911445286e-06, 'epoch': 3.34}\n",
      "{'loss': 0.0202, 'learning_rate': 6.07630186577555e-06, 'epoch': 3.48}\n",
      "{'loss': 0.0191, 'learning_rate': 5.519353940406572e-06, 'epoch': 3.62}\n",
      "{'loss': 0.0191, 'learning_rate': 4.962406015037594e-06, 'epoch': 3.76}\n",
      "{'loss': 0.0177, 'learning_rate': 4.405458089668616e-06, 'epoch': 3.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d6f4b5eece489b8237d332cfcd37f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [out] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [pad] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.025697294622659683, 'eval_precision': 0.694310759711241, 'eval_recall': 0.40658262248056165, 'eval_f1': 0.5128465554726802, 'eval_accuracy': 0.9922165970868224, 'eval_runtime': 172.0331, 'eval_samples_per_second': 46.049, 'eval_steps_per_second': 5.761, 'epoch': 4.0}\n",
      "{'loss': 0.0164, 'learning_rate': 3.8485101642996384e-06, 'epoch': 4.04}\n",
      "{'loss': 0.0172, 'learning_rate': 3.29156223893066e-06, 'epoch': 4.18}\n",
      "{'loss': 0.0172, 'learning_rate': 2.734614313561682e-06, 'epoch': 4.32}\n",
      "{'loss': 0.0183, 'learning_rate': 2.177666388192704e-06, 'epoch': 4.46}\n",
      "{'loss': 0.0169, 'learning_rate': 1.620718462823726e-06, 'epoch': 4.59}\n",
      "{'loss': 0.0183, 'learning_rate': 1.063770537454748e-06, 'epoch': 4.73}\n",
      "{'loss': 0.0175, 'learning_rate': 5.0682261208577e-07, 'epoch': 4.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3de145bd3847b186f3e16355435a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [out] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [pad] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.025351131334900856, 'eval_precision': 0.6678882452614572, 'eval_recall': 0.42648650008807026, 'eval_f1': 0.5205626708436991, 'eval_accuracy': 0.9921191340520964, 'eval_runtime': 171.9237, 'eval_samples_per_second': 46.079, 'eval_steps_per_second': 5.764, 'epoch': 5.0}\n",
      "{'train_runtime': 6568.8904, 'train_samples_per_second': 21.863, 'train_steps_per_second': 2.733, 'train_loss': 0.02284046025436856, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17955, training_loss=0.02284046025436856, metrics={'train_runtime': 6568.8904, 'train_samples_per_second': 21.863, 'train_steps_per_second': 2.733, 'train_loss': 0.02284046025436856, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▃▂▁█▇</td></tr><tr><td>eval/f1</td><td>▁▃█▆▇</td></tr><tr><td>eval/loss</td><td>█▃▇▃▁</td></tr><tr><td>eval/precision</td><td>▇▂▁█▅</td></tr><tr><td>eval/recall</td><td>▁▄█▄▆</td></tr><tr><td>eval/runtime</td><td>█▁▇▆▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█▂▃█</td></tr><tr><td>eval/steps_per_second</td><td>▁█▁▃▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.99212</td></tr><tr><td>eval/f1</td><td>0.52056</td></tr><tr><td>eval/loss</td><td>0.02535</td></tr><tr><td>eval/precision</td><td>0.66789</td></tr><tr><td>eval/recall</td><td>0.42649</td></tr><tr><td>eval/runtime</td><td>171.9237</td></tr><tr><td>eval/samples_per_second</td><td>46.079</td></tr><tr><td>eval/steps_per_second</td><td>5.764</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>17955</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0175</td></tr><tr><td>train/total_flos</td><td>3.752749080456192e+16</td></tr><tr><td>train/train_loss</td><td>0.02284</td></tr><tr><td>train/train_runtime</td><td>6568.8904</td></tr><tr><td>train/train_samples_per_second</td><td>21.863</td></tr><tr><td>train/train_steps_per_second</td><td>2.733</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert_without_ner_epoch_5_loss2</strong> at: <a href='https://wandb.ai/tian1995/BERT/runs/n97rmpuu' target=\"_blank\">https://wandb.ai/tian1995/BERT/runs/n97rmpuu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230625_124539-n97rmpuu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "trainer.save_model(\"bert_without_ner/models/bert_without_ner_epoch_5_loss2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from data_preprocessing import make_bert_re_data\n",
    "from data_preprocessing import bert_w_ner_preprocess_function\n",
    "additional_tokens, labels, id2label, label2id = get_labels(mode='bert_without_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and tokenizer\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert_without_ner/models/bert_without_ner_epoch_5_loss2\",\n",
    "                                                        num_labels=len(labels),\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id,\n",
    "                                                        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert_without_ner/bert_without_ner_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e82b095564c4b22b834d1c2dfd84135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load test data and preprocess\n",
    "\n",
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"inputs\", \"outputs\", \"pmids\"])\n",
    "# tokenized_datasets = dataset.map(lambda example: bert_w_ner_preprocess_function(example, tokenizer, mode=\"bert_without_ner\"), batched=True, remove_columns=[\"input_texts\", \"input_relations\", \"outputs\", \"pmids\"])\n",
    "test_file_path = 'data/BioRED/processed/test.tsv'\n",
    "test_data = make_bert_re_data(file_path=test_file_path, lower=True, no_ner_input=True)\n",
    "\n",
    "test_dataset_raw = Dataset.from_dict(test_data)\n",
    "# test_dataset = test_dataset_raw.map(NER_preprocess_function, batched=False)\n",
    "# with bert only:\n",
    "test_dataset = test_dataset_raw.map(lambda example: bert_w_ner_preprocess_function(example, tokenizer, mode=\"bert_without_ner\"), batched=True, remove_columns=[\"input_texts\", \"input_relations\", \"outputs\", \"pmids\"])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics_infer(logits, labels):\n",
    "    predictions = [np.argmax(logit, axis=-1)[0] for logit in logits]\n",
    "\n",
    "    # only consider non-padding tokens [:4]\n",
    "    true_labels = [[id2label[l.item()] for l in label if l != label2id['[pad]']] for label in labels]\n",
    "    true_predictions = [[id2label[p.item()] for (p, l) in zip(prediction, label)]for prediction, label in zip(predictions, true_labels)]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels, suffix=True)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }, all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60720/60720 [14:50<00:00, 68.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_line in tqdm(test_dataset['input_ids']):\n",
    "    # for n in range(1):\n",
    "        torch.cuda.empty_cache()\n",
    "        out = model(input_ids=input_line.unsqueeze(0).to(\"cuda\"))\n",
    "        # print(f\"{n+1} / {len(test_dataset)}\")\n",
    "        output.append(out[0].to(\"cpu\"))\n",
    "        # output.append(torch.argmax(out[0], dim=-1).squeeze(0))\n",
    "        # output[-1].to(\"cpu\")\n",
    "    # print([tag_to_NER_id[i.item()] for i in output[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.load('bert_without_ner/results/bert_without_ner_epoch_5_loss2_raw.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: [out] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: [tgt-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: [tgt-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: [src-b] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: [src-in] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:167: UserWarning: [pad] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'precision': 0.5722459270752521,\n",
       "  'recall': 0.20179878257301143,\n",
       "  'f1': 0.2983769024624564,\n",
       "  'accuracy': 0.9890279785690237},\n",
       " {'[out': {'precision': 0.6857640906449739,\n",
       "   'recall': 0.3190247067091961,\n",
       "   'f1': 0.4354660172680983,\n",
       "   'number': 18497},\n",
       "  '[pad': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0},\n",
       "  '[src': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 6818},\n",
       "  '[tgt': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 3927},\n",
       "  'overall_precision': 0.5722459270752521,\n",
       "  'overall_recall': 0.20179878257301143,\n",
       "  'overall_f1': 0.2983769024624564,\n",
       "  'overall_accuracy': 0.9890279785690237})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_infer(output, test_dataset['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics_infer_doclevel(logits, labels):\n",
    "    predictions = [np.argmax(logit, axis=-1)[0] for logit in logits]\n",
    "\n",
    "\n",
    "    all_doc = 0\n",
    "    correct_src = 0\n",
    "    correct_tgt = 0\n",
    "    all_doc_w_src_tgt = 0\n",
    "    pred_src_doc = 0\n",
    "    pred_tgt_doc = 0\n",
    "    correct_doc = 0\n",
    "    # only consider the src and tgt tags\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        all_doc += 1\n",
    "        temp_dict = {'src': [],\n",
    "                        'tgt': []}\n",
    "        doc_has_relation = False\n",
    "        pred_has = {'src': False,\n",
    "                        'tgt': False}\n",
    "        for pred, lab in zip(prediction, label):\n",
    "            if lab.item() == label2id[\"[pad]\"]:\n",
    "                break\n",
    "            else:\n",
    "                if lab.item() != label2id[\"[out]\"]:\n",
    "                    doc_has_relation = True\n",
    "                if pred.item() != label2id[\"[pad]\"] and pred.item() != label2id[\"[out]\"]:\n",
    "                    pred_has[f'{id2label[pred.item()][1:4]}'] = True\n",
    "\n",
    "                    if pred.item() == lab.item():\n",
    "                        temp_dict[f'{id2label[pred.item()][1:4]}'].append(1)\n",
    "\n",
    "                    else:\n",
    "                        temp_dict[f'{id2label[pred.item()][1:4]}'].append(0)\n",
    "        # print(temp_dict)\n",
    "        if doc_has_relation:\n",
    "            all_doc_w_src_tgt += 1\n",
    "        \n",
    "        if pred_has['src']:\n",
    "            pred_src_doc += 1\n",
    "        if pred_has['tgt']:\n",
    "            pred_tgt_doc += 1\n",
    "\n",
    "        if 0 in temp_dict['src'] or len(temp_dict['src']) == 0:\n",
    "            if 0 in temp_dict['tgt'] or len(temp_dict['tgt']) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                correct_tgt += 1\n",
    "        else:\n",
    "            correct_src += 1\n",
    "            if 0 in temp_dict['tgt'] or len(temp_dict['tgt']) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                correct_tgt += 1\n",
    "                correct_doc == 1\n",
    "                    \n",
    "\n",
    "    return {'src precision': correct_src / pred_src_doc,\n",
    "            'src recall': correct_src / all_doc_w_src_tgt,\n",
    "            'src f1': 2 * correct_src / (pred_src_doc + all_doc_w_src_tgt),\n",
    "            'tgt precision': correct_tgt / pred_tgt_doc,\n",
    "            'tgt recall': correct_tgt / all_doc_w_src_tgt,\n",
    "            'tgt f1': 2 * correct_tgt / (pred_tgt_doc + all_doc_w_src_tgt),\n",
    "            'doc precision': correct_doc / all_doc,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src precision': 0.3069306930693069,\n",
       " 'src recall': 0.026655202063628546,\n",
       " 'src f1': 0.0490506329113924,\n",
       " 'tgt precision': 0.3157894736842105,\n",
       " 'tgt recall': 0.015477214101461736,\n",
       " 'tgt f1': 0.029508196721311476,\n",
       " 'doc precision': 0.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_infer_doclevel(output, test_dataset['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_infer_relation(logits, labels):\n",
    "    predictions = [np.argmax(logit, axis=-1)[0] for logit in logits]\n",
    "\n",
    "\n",
    "    all_text = 0\n",
    "    correct_relation = 0\n",
    "    \n",
    "    precision_correct_relation = 0\n",
    "    # only consider the src and tgt tags\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        all_text += 1\n",
    "        if 1 in label:\n",
    "            if 1 in prediction:\n",
    "                correct_relation += 1\n",
    "        else:\n",
    "            if 1 not in prediction and 2 not in prediction and 3 not in prediction and 4 not in prediction:\n",
    "                correct_relation += 1\n",
    "\n",
    "        if 1 in prediction or 2 in prediction or 3 in prediction or 4 in prediction:\n",
    "            if 1 in label:\n",
    "                precision_correct_relation += 1\n",
    "        else:\n",
    "            if 1 not in label:\n",
    "                precision_correct_relation += 1\n",
    "                    \n",
    "\n",
    "    return {\n",
    "        'relation precision': precision_correct_relation / all_text,\n",
    "        'relation recall': correct_relation / all_text,\n",
    "        'relation f1': 2 * precision_correct_relation * correct_relation / (precision_correct_relation + correct_relation) / all_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relation precision': 0.8694784127776392,\n",
       " 'relation recall': 0.8659845270776142,\n",
       " 'relation f1': 0.8677279529302294}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_infer_relation(output, test_dataset['labels'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioRED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
