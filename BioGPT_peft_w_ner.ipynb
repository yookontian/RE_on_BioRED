{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing#scrollTo=T-gy-LxM0yAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from labels import get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_special_tokens': ['[entity1]', '[entity2]', '[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]', '[None]', '[Association]', '[Bind]', '[Comparison]', '[Conversion]', '[Cotreatment]', '[Drug_Interaction]', '[Negative_Correlation]', '[Positive_Correlation]']} \n",
      " {'additional_special_tokens': ['[entity1]', '[entity2]', '[learn1]', '[learn2]', '[learn3]', '[learn4]', '[learn5]', '[learn6]', '[None]', '[Association]', '[Bind]', '[Comparison]', '[Conversion]', '[Cotreatment]', '[Drug_Interaction]', '[Negative_Correlation]', '[Positive_Correlation]']}\n"
     ]
    }
   ],
   "source": [
    "# load labels for bert_w_ner\n",
    "additional_tokens, _, _, _ = get_labels(mode='GPT_w_ner')\n",
    "print(additional_tokens, \"\\n\", additional_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model in 8-bit quantization configuration\n",
    "# the max length of the input is 1024\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, \n",
    "    # load_in_8bit=True, \n",
    "    device_map={'':torch.cuda.current_device()},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 17 tokens\n"
     ]
    }
   ],
   "source": [
    "# adding new tokens to the tokenizer\n",
    "# since I haven't load the model so I will resize the embedding of the model later]\n",
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "# save the tokenizer\n",
    "# tokenizer.save_pretrained(\"GPT_w_ner/GPT_w_ner_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(42401, 1024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in float32 for stability.\n",
    "\n",
    "We also cast the output of the last layer and embedding layer in float32 for the same reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "model.biogpt.embed_tokens = CastOutputToFloat(model.biogpt.embed_tokens)\n",
    "model.output_projection = CastOutputToFloat(model.output_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/tian/mambaforge/envs/BioRED did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/lib/x86_64-linux-gnu/mesa'), PosixPath('/usr/lib/mesa-diverted/x86_64-linux-gnu'), PosixPath('/usr/lib/x86_64-linux-gnu/gallium-pipe')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib/mesa-diverted/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/mesa:/usr/lib/x86_64-linux-gnu/dri:/usr/lib/x86_64-linux-gnu/gallium-pipe did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('0')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/4416,unix/tian-desktop'), PosixPath('local/tian-desktop')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/home/tian/mambaforge/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/peft/tuners/lora.py:230: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 348353536 || trainable%: 0.45151371737475343\n"
     ]
    }
   ],
   "source": [
    "# more with LoRAconfig: https://huggingface.co/docs/peft/conceptual_guides/lora\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    r=16,\n",
    "    # alpha: LoRA scaling factor.\n",
    "    lora_alpha=32, \n",
    "    # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    fan_in_fan_out=True,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BioGptForCausalLM(\n",
       "      (biogpt): BioGptModel(\n",
       "        (embed_tokens): CastOutputToFloat(\n",
       "          (0): Embedding(42401, 1024)\n",
       "        )\n",
       "        (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x BioGptDecoderLayer(\n",
       "            (self_attn): BioGptAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output_projection): CastOutputToFloat(\n",
       "        (0): Linear(in_features=1024, out_features=42401, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.biogpt.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([16, 1024]) torch.float32\n",
      "base_model.model.biogpt.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([1024, 16]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# for model, print the layer's name if the layer is trainable, and print the precision of the layer\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape, param.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import make_GPT_re_data, GPT_w_ner_preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and valid file paths\n",
    "train_file_path = 'data/BioRED/processed/train.tsv'\n",
    "valid_file_path = 'data/BioRED/processed/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 8 line:\n",
      " [6646, 6758, 6776, 6866, 10222, 11775, 18818, 21689]\n",
      "Dropped 8 line:\n",
      " [941, 2220, 2233, 2261, 5335, 5337, 5378, 5490]\n"
     ]
    }
   ],
   "source": [
    "# make bert_re data\n",
    "train_data_raw = make_GPT_re_data(file_path=train_file_path, lower=True)\n",
    "valid_data_raw = make_GPT_re_data(file_path=valid_file_path, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pmids', 'text', 'entities', 'outputs'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into Dataset type\n",
    "train_data_raw = Dataset.from_dict(train_data_raw)\n",
    "valid_data_raw = Dataset.from_dict(valid_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\"\"\"\n",
    "for the train_dataset:\n",
    "{'[None]': 18720,\n",
    " '[Association]': 2183,\n",
    " '[Bind]': 60,\n",
    " '[Comparison]': 28,\n",
    " '[Conversion]': 3,\n",
    " '[Cotreatment]': 31,\n",
    " '[Drug_Interaction]': 11,\n",
    " '[Negative_Correlation]': 763,\n",
    " '[Positive_Correlation]': 1088}\n",
    "\n",
    "so it is neccessary to balance the dataset, we randomly choose 3000 samples from the [None] class with the seed 42\n",
    "\"\"\"\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# get the index of the [None] class of the datasets type of train_data_raw\n",
    "none_index = [i for i, example in enumerate(train_data_raw) if example['outputs'].split(\" \")[-3] == '[None]']\n",
    "\n",
    "# randomly choose 18720-3000 samples from the [None] class\n",
    "none_index = random.sample(none_index, 18720-3000)\n",
    "keep_indices = [i for i in range(len(train_data_raw)) if i not in none_index]\n",
    "\n",
    "# delete the [None] class samples from the train_data_raw\n",
    "train_data_raw_balanced = train_data_raw.select(keep_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pmids', 'text', 'entities', 'outputs'],\n",
       "    num_rows: 7167\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_data_raw_balanced,\n",
    "    \"valid\": valid_data_raw\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f9c1c957534dc980db14b148dda39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c82ed5f0714b4f8f4b309ca0928cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(lambda example: GPT_w_ner_preprocess_function(example, tokenizer, mode=\"gpt_w_ner\"), batched=True, remove_columns=['pmids', 'text', 'entities', 'outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 7167\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 6650\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/BioRED/wandb/run-20230703_004027-5a0xl1hw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/GPT2/runs/5a0xl1hw' target=\"_blank\">BioGPT_w_ner_epoch_15_balanced_train_data</a></strong> to <a href='https://wandb.ai/tian1995/GPT2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/GPT2' target=\"_blank\">https://wandb.ai/tian1995/GPT2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/GPT2/runs/5a0xl1hw' target=\"_blank\">https://wandb.ai/tian1995/GPT2/runs/5a0xl1hw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/GPT2/runs/5a0xl1hw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbc24b38d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"GPT2\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"BioGPT_w_ner_epoch_15_balanced_train_data\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf73b9529ef4eb2bc1ce73102bf80d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2818, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.01}\n",
      "{'loss': 3.2785, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.02}\n",
      "{'loss': 3.2354, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.03}\n",
      "{'loss': 3.3664, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.04}\n",
      "{'loss': 3.3141, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.04}\n",
      "{'loss': 3.3021, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.05}\n",
      "{'loss': 3.226, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.06}\n",
      "{'loss': 3.2175, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.07}\n",
      "{'loss': 3.2267, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.08}\n",
      "{'loss': 3.2179, 'learning_rate': 1.8e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4256, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2521, 'learning_rate': 2.2e-06, 'epoch': 0.11}\n",
      "{'loss': 3.2805, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.12}\n",
      "{'loss': 3.265, 'learning_rate': 2.6e-06, 'epoch': 0.12}\n",
      "{'loss': 3.2588, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.13}\n",
      "{'loss': 3.2677, 'learning_rate': 3e-06, 'epoch': 0.14}\n",
      "{'loss': 3.1257, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.15}\n",
      "{'loss': 3.2234, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.16}\n",
      "{'loss': 3.2632, 'learning_rate': 3.6e-06, 'epoch': 0.17}\n",
      "{'loss': 3.3167, 'learning_rate': 3.8e-06, 'epoch': 0.18}\n",
      "{'loss': 3.2205, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.19}\n",
      "{'loss': 3.1693, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.2}\n",
      "{'loss': 3.2643, 'learning_rate': 4.4e-06, 'epoch': 0.21}\n",
      "{'loss': 3.2529, 'learning_rate': 4.6e-06, 'epoch': 0.21}\n",
      "{'loss': 3.338, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.22}\n",
      "{'loss': 3.2443, 'learning_rate': 5e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2702, 'learning_rate': 5.2e-06, 'epoch': 0.24}\n",
      "{'loss': 3.21, 'learning_rate': 5.4e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2059, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.26}\n",
      "{'loss': 3.225, 'learning_rate': 5.8e-06, 'epoch': 0.27}\n",
      "{'loss': 3.2774, 'learning_rate': 6e-06, 'epoch': 0.28}\n",
      "{'loss': 3.226, 'learning_rate': 6.2e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2909, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.29}\n",
      "{'loss': 3.2288, 'learning_rate': 6.6e-06, 'epoch': 0.3}\n",
      "{'loss': 3.2429, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 3.2995, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 3.2895, 'learning_rate': 7.2e-06, 'epoch': 0.33}\n",
      "{'loss': 3.2212, 'learning_rate': 7.4e-06, 'epoch': 0.34}\n",
      "{'loss': 3.1899, 'learning_rate': 7.6e-06, 'epoch': 0.35}\n",
      "{'loss': 3.226, 'learning_rate': 7.8e-06, 'epoch': 0.36}\n",
      "{'loss': 3.1933, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.37}\n",
      "{'loss': 3.2734, 'learning_rate': 8.200000000000001e-06, 'epoch': 0.38}\n",
      "{'loss': 3.2852, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.38}\n",
      "{'loss': 3.1279, 'learning_rate': 8.599999999999999e-06, 'epoch': 0.39}\n",
      "{'loss': 3.1967, 'learning_rate': 8.8e-06, 'epoch': 0.4}\n",
      "{'loss': 3.2688, 'learning_rate': 9e-06, 'epoch': 0.41}\n",
      "{'loss': 3.2121, 'learning_rate': 9.2e-06, 'epoch': 0.42}\n",
      "{'loss': 3.2844, 'learning_rate': 9.4e-06, 'epoch': 0.43}\n",
      "{'loss': 3.2747, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.44}\n",
      "{'loss': 3.2033, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.45}\n",
      "{'loss': 3.1368, 'learning_rate': 1e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2634, 'learning_rate': 1.02e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1513, 'learning_rate': 1.04e-05, 'epoch': 0.47}\n",
      "{'loss': 3.1862, 'learning_rate': 1.06e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2133, 'learning_rate': 1.08e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1336, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2333, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2013, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2194, 'learning_rate': 1.16e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2836, 'learning_rate': 1.18e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2565, 'learning_rate': 1.2e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2235, 'learning_rate': 1.22e-05, 'epoch': 0.55}\n",
      "{'loss': 3.1609, 'learning_rate': 1.24e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1906, 'learning_rate': 1.2600000000000001e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2441, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1982, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1636, 'learning_rate': 1.32e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2559, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2186, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1341, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2764, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1498, 'learning_rate': 1.42e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2232, 'learning_rate': 1.44e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1144, 'learning_rate': 1.44e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2145, 'learning_rate': 1.4599999999999999e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2138, 'learning_rate': 1.48e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1503, 'learning_rate': 1.5e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1506, 'learning_rate': 1.52e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1456, 'learning_rate': 1.54e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2368, 'learning_rate': 1.56e-05, 'epoch': 0.71}\n",
      "{'loss': 3.165, 'learning_rate': 1.58e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1801, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.73}\n",
      "{'loss': 3.161, 'learning_rate': 1.62e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1193, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1041, 'learning_rate': 1.66e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1834, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1089, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1487, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1448, 'learning_rate': 1.74e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1191, 'learning_rate': 1.76e-05, 'epoch': 0.8}\n",
      "{'loss': 3.169, 'learning_rate': 1.78e-05, 'epoch': 0.81}\n",
      "{'loss': 3.1867, 'learning_rate': 1.8e-05, 'epoch': 0.82}\n",
      "{'loss': 3.13, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.83}\n",
      "{'loss': 3.08, 'learning_rate': 1.84e-05, 'epoch': 0.84}\n",
      "{'loss': 3.1702, 'learning_rate': 1.86e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1272, 'learning_rate': 1.88e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1708, 'learning_rate': 1.9e-05, 'epoch': 0.87}\n",
      "{'loss': 3.0991, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.88}\n",
      "{'loss': 3.0772, 'learning_rate': 1.94e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1547, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.89}\n",
      "{'loss': 3.0931, 'learning_rate': 1.9800000000000004e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1155, 'learning_rate': 2e-05, 'epoch': 0.91}\n",
      "{'loss': 3.176, 'learning_rate': 2.0200000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 3.1234, 'learning_rate': 2.04e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1155, 'learning_rate': 2.06e-05, 'epoch': 0.94}\n",
      "{'loss': 3.0769, 'learning_rate': 2.08e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1376, 'learning_rate': 2.1e-05, 'epoch': 0.96}\n",
      "{'loss': 3.0881, 'learning_rate': 2.12e-05, 'epoch': 0.96}\n",
      "{'loss': 3.0412, 'learning_rate': 2.1400000000000002e-05, 'epoch': 0.97}\n",
      "{'loss': 3.156, 'learning_rate': 2.16e-05, 'epoch': 0.98}\n",
      "{'loss': 3.0362, 'learning_rate': 2.18e-05, 'epoch': 0.99}\n",
      "{'loss': 3.0909, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0942, 'learning_rate': 2.22e-05, 'epoch': 1.01}\n",
      "{'loss': 2.9814, 'learning_rate': 2.2400000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0062, 'learning_rate': 2.26e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1185, 'learning_rate': 2.2800000000000002e-05, 'epoch': 1.04}\n",
      "{'loss': 2.9384, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0163, 'learning_rate': 2.32e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1125, 'learning_rate': 2.3400000000000003e-05, 'epoch': 1.06}\n",
      "{'loss': 3.0185, 'learning_rate': 2.36e-05, 'epoch': 1.07}\n",
      "{'loss': 3.0681, 'learning_rate': 2.38e-05, 'epoch': 1.08}\n",
      "{'loss': 2.9429, 'learning_rate': 2.4e-05, 'epoch': 1.09}\n",
      "{'loss': 3.0365, 'learning_rate': 2.4200000000000002e-05, 'epoch': 1.1}\n",
      "{'loss': 3.0095, 'learning_rate': 2.44e-05, 'epoch': 1.11}\n",
      "{'loss': 3.0435, 'learning_rate': 2.46e-05, 'epoch': 1.12}\n",
      "{'loss': 2.941, 'learning_rate': 2.48e-05, 'epoch': 1.12}\n",
      "{'loss': 3.0531, 'learning_rate': 2.5e-05, 'epoch': 1.13}\n",
      "{'loss': 2.922, 'learning_rate': 2.5200000000000003e-05, 'epoch': 1.14}\n",
      "{'loss': 2.9733, 'learning_rate': 2.54e-05, 'epoch': 1.15}\n",
      "{'loss': 3.0191, 'learning_rate': 2.5600000000000002e-05, 'epoch': 1.16}\n",
      "{'loss': 2.9754, 'learning_rate': 2.58e-05, 'epoch': 1.17}\n",
      "{'loss': 2.9988, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.18}\n",
      "{'loss': 3.0397, 'learning_rate': 2.6200000000000003e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0216, 'learning_rate': 2.64e-05, 'epoch': 1.2}\n",
      "{'loss': 2.9197, 'learning_rate': 2.6600000000000003e-05, 'epoch': 1.21}\n",
      "{'loss': 3.0028, 'learning_rate': 2.6800000000000004e-05, 'epoch': 1.21}\n",
      "{'loss': 2.9303, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.22}\n",
      "{'loss': 2.8822, 'learning_rate': 2.7200000000000004e-05, 'epoch': 1.23}\n",
      "{'loss': 2.9324, 'learning_rate': 2.7400000000000002e-05, 'epoch': 1.24}\n",
      "{'loss': 2.9079, 'learning_rate': 2.7600000000000003e-05, 'epoch': 1.25}\n",
      "{'loss': 2.8697, 'learning_rate': 2.7800000000000005e-05, 'epoch': 1.26}\n",
      "{'loss': 2.925, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.27}\n",
      "{'loss': 2.8821, 'learning_rate': 2.8199999999999998e-05, 'epoch': 1.28}\n",
      "{'loss': 2.8533, 'learning_rate': 2.84e-05, 'epoch': 1.29}\n",
      "{'loss': 2.9354, 'learning_rate': 2.86e-05, 'epoch': 1.29}\n",
      "{'loss': 2.8881, 'learning_rate': 2.88e-05, 'epoch': 1.3}\n",
      "{'loss': 2.8402, 'learning_rate': 2.9e-05, 'epoch': 1.31}\n",
      "{'loss': 2.8427, 'learning_rate': 2.9199999999999998e-05, 'epoch': 1.32}\n",
      "{'loss': 2.8535, 'learning_rate': 2.94e-05, 'epoch': 1.33}\n",
      "{'loss': 2.8707, 'learning_rate': 2.96e-05, 'epoch': 1.34}\n",
      "{'loss': 2.8879, 'learning_rate': 2.98e-05, 'epoch': 1.35}\n",
      "{'loss': 2.8507, 'learning_rate': 3e-05, 'epoch': 1.36}\n",
      "{'loss': 2.8566, 'learning_rate': 3.02e-05, 'epoch': 1.37}\n",
      "{'loss': 2.8019, 'learning_rate': 3.04e-05, 'epoch': 1.38}\n",
      "{'loss': 2.9232, 'learning_rate': 3.06e-05, 'epoch': 1.38}\n",
      "{'loss': 2.7488, 'learning_rate': 3.08e-05, 'epoch': 1.39}\n",
      "{'loss': 2.7913, 'learning_rate': 3.1e-05, 'epoch': 1.4}\n",
      "{'loss': 2.7865, 'learning_rate': 3.12e-05, 'epoch': 1.41}\n",
      "{'loss': 2.7639, 'learning_rate': 3.1400000000000004e-05, 'epoch': 1.42}\n",
      "{'loss': 2.831, 'learning_rate': 3.16e-05, 'epoch': 1.43}\n",
      "{'loss': 2.8272, 'learning_rate': 3.18e-05, 'epoch': 1.44}\n",
      "{'loss': 2.7883, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.45}\n",
      "{'loss': 2.7586, 'learning_rate': 3.2200000000000003e-05, 'epoch': 1.46}\n",
      "{'loss': 2.7444, 'learning_rate': 3.24e-05, 'epoch': 1.46}\n",
      "{'loss': 2.6719, 'learning_rate': 3.26e-05, 'epoch': 1.47}\n",
      "{'loss': 2.7658, 'learning_rate': 3.2800000000000004e-05, 'epoch': 1.48}\n",
      "{'loss': 2.6702, 'learning_rate': 3.3e-05, 'epoch': 1.49}\n",
      "{'loss': 2.7267, 'learning_rate': 3.32e-05, 'epoch': 1.5}\n",
      "{'loss': 2.7145, 'learning_rate': 3.3400000000000005e-05, 'epoch': 1.51}\n",
      "{'loss': 2.7156, 'learning_rate': 3.3600000000000004e-05, 'epoch': 1.52}\n",
      "{'loss': 2.7369, 'learning_rate': 3.38e-05, 'epoch': 1.53}\n",
      "{'loss': 2.6937, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.54}\n",
      "{'loss': 2.6634, 'learning_rate': 3.4200000000000005e-05, 'epoch': 1.54}\n",
      "{'loss': 2.7341, 'learning_rate': 3.4399999999999996e-05, 'epoch': 1.55}\n",
      "{'loss': 2.6457, 'learning_rate': 3.46e-05, 'epoch': 1.56}\n",
      "{'loss': 2.7021, 'learning_rate': 3.48e-05, 'epoch': 1.57}\n",
      "{'loss': 2.6952, 'learning_rate': 3.5e-05, 'epoch': 1.58}\n",
      "{'loss': 2.7086, 'learning_rate': 3.52e-05, 'epoch': 1.59}\n",
      "{'loss': 2.6036, 'learning_rate': 3.54e-05, 'epoch': 1.6}\n",
      "{'loss': 2.6015, 'learning_rate': 3.56e-05, 'epoch': 1.61}\n",
      "{'loss': 2.6733, 'learning_rate': 3.58e-05, 'epoch': 1.62}\n",
      "{'loss': 2.6202, 'learning_rate': 3.6e-05, 'epoch': 1.62}\n",
      "{'loss': 2.6153, 'learning_rate': 3.62e-05, 'epoch': 1.63}\n",
      "{'loss': 2.6447, 'learning_rate': 3.6400000000000004e-05, 'epoch': 1.64}\n",
      "{'loss': 2.6725, 'learning_rate': 3.66e-05, 'epoch': 1.65}\n",
      "{'loss': 2.5858, 'learning_rate': 3.68e-05, 'epoch': 1.66}\n",
      "{'loss': 2.6645, 'learning_rate': 3.7e-05, 'epoch': 1.67}\n",
      "{'loss': 2.6397, 'learning_rate': 3.72e-05, 'epoch': 1.68}\n",
      "{'loss': 2.5687, 'learning_rate': 3.74e-05, 'epoch': 1.69}\n",
      "{'loss': 2.6336, 'learning_rate': 3.76e-05, 'epoch': 1.7}\n",
      "{'loss': 2.6238, 'learning_rate': 3.7800000000000004e-05, 'epoch': 1.71}\n",
      "{'loss': 2.5614, 'learning_rate': 3.8e-05, 'epoch': 1.71}\n",
      "{'loss': 2.5144, 'learning_rate': 3.82e-05, 'epoch': 1.72}\n",
      "{'loss': 2.6811, 'learning_rate': 3.8400000000000005e-05, 'epoch': 1.73}\n",
      "{'loss': 2.5258, 'learning_rate': 3.86e-05, 'epoch': 1.74}\n",
      "{'loss': 2.5176, 'learning_rate': 3.88e-05, 'epoch': 1.75}\n",
      "{'loss': 2.5676, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.76}\n",
      "{'loss': 2.6275, 'learning_rate': 3.9200000000000004e-05, 'epoch': 1.77}\n",
      "{'loss': 2.556, 'learning_rate': 3.94e-05, 'epoch': 1.78}\n",
      "{'loss': 2.6339, 'learning_rate': 3.960000000000001e-05, 'epoch': 1.79}\n",
      "{'loss': 2.5557, 'learning_rate': 3.9800000000000005e-05, 'epoch': 1.79}\n",
      "{'loss': 2.5504, 'learning_rate': 4e-05, 'epoch': 1.8}\n",
      "{'loss': 2.6456, 'learning_rate': 4.02e-05, 'epoch': 1.81}\n",
      "{'loss': 2.5773, 'learning_rate': 4.0400000000000006e-05, 'epoch': 1.82}\n",
      "{'loss': 2.6139, 'learning_rate': 4.0600000000000004e-05, 'epoch': 1.83}\n",
      "{'loss': 2.5163, 'learning_rate': 4.08e-05, 'epoch': 1.84}\n",
      "{'loss': 2.5461, 'learning_rate': 4.1e-05, 'epoch': 1.85}\n",
      "{'loss': 2.6088, 'learning_rate': 4.12e-05, 'epoch': 1.86}\n",
      "{'loss': 2.5588, 'learning_rate': 4.14e-05, 'epoch': 1.87}\n",
      "{'loss': 2.5407, 'learning_rate': 4.16e-05, 'epoch': 1.88}\n",
      "{'loss': 2.5514, 'learning_rate': 4.18e-05, 'epoch': 1.88}\n",
      "{'loss': 2.522, 'learning_rate': 4.2e-05, 'epoch': 1.89}\n",
      "{'loss': 2.507, 'learning_rate': 4.22e-05, 'epoch': 1.9}\n",
      "{'loss': 2.5812, 'learning_rate': 4.24e-05, 'epoch': 1.91}\n",
      "{'loss': 2.5318, 'learning_rate': 4.26e-05, 'epoch': 1.92}\n",
      "{'loss': 2.5387, 'learning_rate': 4.2800000000000004e-05, 'epoch': 1.93}\n",
      "{'loss': 2.502, 'learning_rate': 4.3e-05, 'epoch': 1.94}\n",
      "{'loss': 2.5186, 'learning_rate': 4.32e-05, 'epoch': 1.95}\n",
      "{'loss': 2.5296, 'learning_rate': 4.3400000000000005e-05, 'epoch': 1.96}\n",
      "{'loss': 2.4315, 'learning_rate': 4.36e-05, 'epoch': 1.96}\n",
      "{'loss': 2.4777, 'learning_rate': 4.38e-05, 'epoch': 1.97}\n",
      "{'loss': 2.447, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.98}\n",
      "{'loss': 2.5693, 'learning_rate': 4.4200000000000004e-05, 'epoch': 1.99}\n",
      "{'loss': 2.5405, 'learning_rate': 4.44e-05, 'epoch': 2.0}\n",
      "{'loss': 2.5539, 'learning_rate': 4.46e-05, 'epoch': 2.01}\n",
      "{'loss': 2.4714, 'learning_rate': 4.4800000000000005e-05, 'epoch': 2.02}\n",
      "{'loss': 2.528, 'learning_rate': 4.5e-05, 'epoch': 2.03}\n",
      "{'loss': 2.4567, 'learning_rate': 4.52e-05, 'epoch': 2.04}\n",
      "{'loss': 2.4855, 'learning_rate': 4.5400000000000006e-05, 'epoch': 2.04}\n",
      "{'loss': 2.5042, 'learning_rate': 4.5600000000000004e-05, 'epoch': 2.05}\n",
      "{'loss': 2.3889, 'learning_rate': 4.58e-05, 'epoch': 2.06}\n",
      "{'loss': 2.5377, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.07}\n",
      "{'loss': 2.4557, 'learning_rate': 4.6200000000000005e-05, 'epoch': 2.08}\n",
      "{'loss': 2.4206, 'learning_rate': 4.64e-05, 'epoch': 2.09}\n",
      "{'loss': 2.4439, 'learning_rate': 4.660000000000001e-05, 'epoch': 2.1}\n",
      "{'loss': 2.498, 'learning_rate': 4.6800000000000006e-05, 'epoch': 2.11}\n",
      "{'loss': 2.44, 'learning_rate': 4.7e-05, 'epoch': 2.12}\n",
      "{'loss': 2.4034, 'learning_rate': 4.72e-05, 'epoch': 2.12}\n",
      "{'loss': 2.4869, 'learning_rate': 4.74e-05, 'epoch': 2.13}\n",
      "{'loss': 2.3928, 'learning_rate': 4.76e-05, 'epoch': 2.14}\n",
      "{'loss': 2.4367, 'learning_rate': 4.78e-05, 'epoch': 2.15}\n",
      "{'loss': 2.406, 'learning_rate': 4.8e-05, 'epoch': 2.16}\n",
      "{'loss': 2.4477, 'learning_rate': 4.82e-05, 'epoch': 2.17}\n",
      "{'loss': 2.4646, 'learning_rate': 4.8400000000000004e-05, 'epoch': 2.18}\n",
      "{'loss': 2.4281, 'learning_rate': 4.86e-05, 'epoch': 2.19}\n",
      "{'loss': 2.4708, 'learning_rate': 4.88e-05, 'epoch': 2.2}\n",
      "{'loss': 2.4159, 'learning_rate': 4.9e-05, 'epoch': 2.21}\n",
      "{'loss': 2.3453, 'learning_rate': 4.92e-05, 'epoch': 2.21}\n",
      "{'loss': 2.4175, 'learning_rate': 4.94e-05, 'epoch': 2.22}\n",
      "{'loss': 2.4263, 'learning_rate': 4.96e-05, 'epoch': 2.23}\n",
      "{'loss': 2.3978, 'learning_rate': 4.9800000000000004e-05, 'epoch': 2.24}\n",
      "{'loss': 2.4366, 'learning_rate': 5e-05, 'epoch': 2.25}\n",
      "{'loss': 2.4341, 'learning_rate': 5.02e-05, 'epoch': 2.26}\n",
      "{'loss': 2.4022, 'learning_rate': 5.0400000000000005e-05, 'epoch': 2.27}\n",
      "{'loss': 2.393, 'learning_rate': 5.0600000000000003e-05, 'epoch': 2.28}\n",
      "{'loss': 2.3615, 'learning_rate': 5.08e-05, 'epoch': 2.29}\n",
      "{'loss': 2.4177, 'learning_rate': 5.1000000000000006e-05, 'epoch': 2.29}\n",
      "{'loss': 2.361, 'learning_rate': 5.1200000000000004e-05, 'epoch': 2.3}\n",
      "{'loss': 2.3495, 'learning_rate': 5.14e-05, 'epoch': 2.31}\n",
      "{'loss': 2.326, 'learning_rate': 5.16e-05, 'epoch': 2.32}\n",
      "{'loss': 2.4107, 'learning_rate': 5.1800000000000005e-05, 'epoch': 2.33}\n",
      "{'loss': 2.2862, 'learning_rate': 5.2000000000000004e-05, 'epoch': 2.34}\n",
      "{'loss': 2.467, 'learning_rate': 5.22e-05, 'epoch': 2.35}\n",
      "{'loss': 2.2941, 'learning_rate': 5.2400000000000007e-05, 'epoch': 2.36}\n",
      "{'loss': 2.3355, 'learning_rate': 5.2600000000000005e-05, 'epoch': 2.37}\n",
      "{'loss': 2.3794, 'learning_rate': 5.28e-05, 'epoch': 2.38}\n",
      "{'loss': 2.3343, 'learning_rate': 5.300000000000001e-05, 'epoch': 2.38}\n",
      "{'loss': 2.3275, 'learning_rate': 5.3200000000000006e-05, 'epoch': 2.39}\n",
      "{'loss': 2.2779, 'learning_rate': 5.3400000000000004e-05, 'epoch': 2.4}\n",
      "{'loss': 2.3297, 'learning_rate': 5.360000000000001e-05, 'epoch': 2.41}\n",
      "{'loss': 2.3227, 'learning_rate': 5.380000000000001e-05, 'epoch': 2.42}\n",
      "{'loss': 2.3291, 'learning_rate': 5.4000000000000005e-05, 'epoch': 2.43}\n",
      "{'loss': 2.2859, 'learning_rate': 5.420000000000001e-05, 'epoch': 2.44}\n",
      "{'loss': 2.3824, 'learning_rate': 5.440000000000001e-05, 'epoch': 2.45}\n",
      "{'loss': 2.3644, 'learning_rate': 5.4600000000000006e-05, 'epoch': 2.46}\n",
      "{'loss': 2.2779, 'learning_rate': 5.4800000000000004e-05, 'epoch': 2.46}\n",
      "{'loss': 2.2244, 'learning_rate': 5.500000000000001e-05, 'epoch': 2.47}\n",
      "{'loss': 2.2267, 'learning_rate': 5.520000000000001e-05, 'epoch': 2.48}\n",
      "{'loss': 2.2831, 'learning_rate': 5.5400000000000005e-05, 'epoch': 2.49}\n",
      "{'loss': 2.3025, 'learning_rate': 5.560000000000001e-05, 'epoch': 2.5}\n",
      "{'loss': 2.3141, 'learning_rate': 5.580000000000001e-05, 'epoch': 2.51}\n",
      "{'loss': 2.222, 'learning_rate': 5.6000000000000006e-05, 'epoch': 2.52}\n",
      "{'loss': 2.268, 'learning_rate': 5.620000000000001e-05, 'epoch': 2.53}\n",
      "{'loss': 2.2664, 'learning_rate': 5.6399999999999995e-05, 'epoch': 2.54}\n",
      "{'loss': 2.2763, 'learning_rate': 5.66e-05, 'epoch': 2.54}\n",
      "{'loss': 2.2873, 'learning_rate': 5.68e-05, 'epoch': 2.55}\n",
      "{'loss': 2.2758, 'learning_rate': 5.6999999999999996e-05, 'epoch': 2.56}\n",
      "{'loss': 2.2977, 'learning_rate': 5.72e-05, 'epoch': 2.57}\n",
      "{'loss': 2.2789, 'learning_rate': 5.74e-05, 'epoch': 2.58}\n",
      "{'loss': 2.2505, 'learning_rate': 5.76e-05, 'epoch': 2.59}\n",
      "{'loss': 2.1798, 'learning_rate': 5.7799999999999995e-05, 'epoch': 2.6}\n",
      "{'loss': 2.285, 'learning_rate': 5.8e-05, 'epoch': 2.61}\n",
      "{'loss': 2.3185, 'learning_rate': 5.82e-05, 'epoch': 2.62}\n",
      "{'loss': 2.2591, 'learning_rate': 5.8399999999999997e-05, 'epoch': 2.62}\n",
      "{'loss': 2.2461, 'learning_rate': 5.86e-05, 'epoch': 2.63}\n",
      "{'loss': 2.1561, 'learning_rate': 5.88e-05, 'epoch': 2.64}\n",
      "{'loss': 2.2652, 'learning_rate': 5.9e-05, 'epoch': 2.65}\n",
      "{'loss': 2.2364, 'learning_rate': 5.92e-05, 'epoch': 2.66}\n",
      "{'loss': 2.2951, 'learning_rate': 5.94e-05, 'epoch': 2.67}\n",
      "{'loss': 2.218, 'learning_rate': 5.96e-05, 'epoch': 2.68}\n",
      "{'loss': 2.249, 'learning_rate': 5.9800000000000003e-05, 'epoch': 2.69}\n",
      "{'loss': 2.2309, 'learning_rate': 6e-05, 'epoch': 2.7}\n",
      "{'loss': 2.2566, 'learning_rate': 6.02e-05, 'epoch': 2.71}\n",
      "{'loss': 2.2053, 'learning_rate': 6.04e-05, 'epoch': 2.71}\n",
      "{'loss': 2.1345, 'learning_rate': 6.06e-05, 'epoch': 2.72}\n",
      "{'loss': 2.2479, 'learning_rate': 6.08e-05, 'epoch': 2.73}\n",
      "{'loss': 2.2716, 'learning_rate': 6.1e-05, 'epoch': 2.74}\n",
      "{'loss': 2.248, 'learning_rate': 6.12e-05, 'epoch': 2.75}\n",
      "{'loss': 2.224, 'learning_rate': 6.14e-05, 'epoch': 2.76}\n",
      "{'loss': 2.2447, 'learning_rate': 6.16e-05, 'epoch': 2.77}\n",
      "{'loss': 2.2367, 'learning_rate': 6.18e-05, 'epoch': 2.78}\n",
      "{'loss': 2.2474, 'learning_rate': 6.2e-05, 'epoch': 2.79}\n",
      "{'loss': 2.2165, 'learning_rate': 6.220000000000001e-05, 'epoch': 2.79}\n",
      "{'loss': 2.2995, 'learning_rate': 6.24e-05, 'epoch': 2.8}\n",
      "{'loss': 2.2129, 'learning_rate': 6.26e-05, 'epoch': 2.81}\n",
      "{'loss': 2.1952, 'learning_rate': 6.280000000000001e-05, 'epoch': 2.82}\n",
      "{'loss': 2.2384, 'learning_rate': 6.3e-05, 'epoch': 2.83}\n",
      "{'loss': 2.2459, 'learning_rate': 6.32e-05, 'epoch': 2.84}\n",
      "{'loss': 2.1684, 'learning_rate': 6.340000000000001e-05, 'epoch': 2.85}\n",
      "{'loss': 2.1883, 'learning_rate': 6.36e-05, 'epoch': 2.86}\n",
      "{'loss': 2.2171, 'learning_rate': 6.38e-05, 'epoch': 2.87}\n",
      "{'loss': 2.2438, 'learning_rate': 6.400000000000001e-05, 'epoch': 2.88}\n",
      "{'loss': 2.2479, 'learning_rate': 6.42e-05, 'epoch': 2.88}\n",
      "{'loss': 2.2254, 'learning_rate': 6.440000000000001e-05, 'epoch': 2.89}\n",
      "{'loss': 2.2074, 'learning_rate': 6.460000000000001e-05, 'epoch': 2.9}\n",
      "{'loss': 2.1553, 'learning_rate': 6.48e-05, 'epoch': 2.91}\n",
      "{'loss': 2.1366, 'learning_rate': 6.500000000000001e-05, 'epoch': 2.92}\n",
      "{'loss': 2.2462, 'learning_rate': 6.52e-05, 'epoch': 2.93}\n",
      "{'loss': 2.174, 'learning_rate': 6.54e-05, 'epoch': 2.94}\n",
      "{'loss': 2.1945, 'learning_rate': 6.560000000000001e-05, 'epoch': 2.95}\n",
      "{'loss': 2.1431, 'learning_rate': 6.58e-05, 'epoch': 2.96}\n",
      "{'loss': 2.1627, 'learning_rate': 6.6e-05, 'epoch': 2.96}\n",
      "{'loss': 2.2454, 'learning_rate': 6.620000000000001e-05, 'epoch': 2.97}\n",
      "{'loss': 2.1817, 'learning_rate': 6.64e-05, 'epoch': 2.98}\n",
      "{'loss': 2.2163, 'learning_rate': 6.66e-05, 'epoch': 2.99}\n",
      "{'loss': 2.1386, 'learning_rate': 6.680000000000001e-05, 'epoch': 3.0}\n",
      "{'loss': 2.1761, 'learning_rate': 6.7e-05, 'epoch': 3.01}\n",
      "{'loss': 2.0888, 'learning_rate': 6.720000000000001e-05, 'epoch': 3.02}\n",
      "{'loss': 2.131, 'learning_rate': 6.740000000000001e-05, 'epoch': 3.03}\n",
      "{'loss': 2.2157, 'learning_rate': 6.76e-05, 'epoch': 3.04}\n",
      "{'loss': 2.1843, 'learning_rate': 6.780000000000001e-05, 'epoch': 3.04}\n",
      "{'loss': 2.148, 'learning_rate': 6.800000000000001e-05, 'epoch': 3.05}\n",
      "{'loss': 2.1422, 'learning_rate': 6.82e-05, 'epoch': 3.06}\n",
      "{'loss': 2.1385, 'learning_rate': 6.840000000000001e-05, 'epoch': 3.07}\n",
      "{'loss': 2.1872, 'learning_rate': 6.860000000000001e-05, 'epoch': 3.08}\n",
      "{'loss': 2.158, 'learning_rate': 6.879999999999999e-05, 'epoch': 3.09}\n",
      "{'loss': 2.1887, 'learning_rate': 6.9e-05, 'epoch': 3.1}\n",
      "{'loss': 2.1394, 'learning_rate': 6.92e-05, 'epoch': 3.11}\n",
      "{'loss': 2.0994, 'learning_rate': 6.939999999999999e-05, 'epoch': 3.12}\n",
      "{'loss': 2.115, 'learning_rate': 6.96e-05, 'epoch': 3.12}\n",
      "{'loss': 2.1648, 'learning_rate': 6.98e-05, 'epoch': 3.13}\n",
      "{'loss': 2.1485, 'learning_rate': 7e-05, 'epoch': 3.14}\n",
      "{'loss': 2.219, 'learning_rate': 7.02e-05, 'epoch': 3.15}\n",
      "{'loss': 2.1107, 'learning_rate': 7.04e-05, 'epoch': 3.16}\n",
      "{'loss': 2.1551, 'learning_rate': 7.06e-05, 'epoch': 3.17}\n",
      "{'loss': 2.1336, 'learning_rate': 7.08e-05, 'epoch': 3.18}\n",
      "{'loss': 2.114, 'learning_rate': 7.1e-05, 'epoch': 3.19}\n",
      "{'loss': 2.153, 'learning_rate': 7.12e-05, 'epoch': 3.2}\n",
      "{'loss': 2.1392, 'learning_rate': 7.14e-05, 'epoch': 3.21}\n",
      "{'loss': 2.1776, 'learning_rate': 7.16e-05, 'epoch': 3.21}\n",
      "{'loss': 2.1235, 'learning_rate': 7.18e-05, 'epoch': 3.22}\n",
      "{'loss': 2.1091, 'learning_rate': 7.2e-05, 'epoch': 3.23}\n",
      "{'loss': 2.0777, 'learning_rate': 7.22e-05, 'epoch': 3.24}\n",
      "{'loss': 2.1362, 'learning_rate': 7.24e-05, 'epoch': 3.25}\n",
      "{'loss': 2.0535, 'learning_rate': 7.26e-05, 'epoch': 3.26}\n",
      "{'loss': 1.9977, 'learning_rate': 7.280000000000001e-05, 'epoch': 3.27}\n",
      "{'loss': 2.2052, 'learning_rate': 7.3e-05, 'epoch': 3.28}\n",
      "{'loss': 2.0889, 'learning_rate': 7.32e-05, 'epoch': 3.29}\n",
      "{'loss': 2.1464, 'learning_rate': 7.340000000000001e-05, 'epoch': 3.29}\n",
      "{'loss': 2.0926, 'learning_rate': 7.36e-05, 'epoch': 3.3}\n",
      "{'loss': 2.1734, 'learning_rate': 7.38e-05, 'epoch': 3.31}\n",
      "{'loss': 2.2044, 'learning_rate': 7.4e-05, 'epoch': 3.32}\n",
      "{'loss': 2.1341, 'learning_rate': 7.42e-05, 'epoch': 3.33}\n",
      "{'loss': 2.0653, 'learning_rate': 7.44e-05, 'epoch': 3.34}\n",
      "{'loss': 1.9728, 'learning_rate': 7.46e-05, 'epoch': 3.35}\n",
      "{'loss': 2.0362, 'learning_rate': 7.48e-05, 'epoch': 3.36}\n",
      "{'loss': 2.0498, 'learning_rate': 7.500000000000001e-05, 'epoch': 3.37}\n",
      "{'loss': 2.0549, 'learning_rate': 7.52e-05, 'epoch': 3.38}\n",
      "{'loss': 2.0039, 'learning_rate': 7.54e-05, 'epoch': 3.38}\n",
      "{'loss': 2.044, 'learning_rate': 7.560000000000001e-05, 'epoch': 3.39}\n",
      "{'loss': 2.0332, 'learning_rate': 7.58e-05, 'epoch': 3.4}\n",
      "{'loss': 2.0778, 'learning_rate': 7.6e-05, 'epoch': 3.41}\n",
      "{'loss': 2.1471, 'learning_rate': 7.620000000000001e-05, 'epoch': 3.42}\n",
      "{'loss': 2.1207, 'learning_rate': 7.64e-05, 'epoch': 3.43}\n",
      "{'loss': 2.1591, 'learning_rate': 7.66e-05, 'epoch': 3.44}\n",
      "{'loss': 2.1248, 'learning_rate': 7.680000000000001e-05, 'epoch': 3.45}\n",
      "{'loss': 2.1078, 'learning_rate': 7.7e-05, 'epoch': 3.46}\n",
      "{'loss': 2.1128, 'learning_rate': 7.72e-05, 'epoch': 3.46}\n",
      "{'loss': 2.0959, 'learning_rate': 7.740000000000001e-05, 'epoch': 3.47}\n",
      "{'loss': 2.1278, 'learning_rate': 7.76e-05, 'epoch': 3.48}\n",
      "{'loss': 2.0192, 'learning_rate': 7.780000000000001e-05, 'epoch': 3.49}\n",
      "{'loss': 2.0594, 'learning_rate': 7.800000000000001e-05, 'epoch': 3.5}\n",
      "{'loss': 2.0582, 'learning_rate': 7.82e-05, 'epoch': 3.51}\n",
      "{'loss': 2.1546, 'learning_rate': 7.840000000000001e-05, 'epoch': 3.52}\n",
      "{'loss': 2.1329, 'learning_rate': 7.860000000000001e-05, 'epoch': 3.53}\n",
      "{'loss': 2.0996, 'learning_rate': 7.88e-05, 'epoch': 3.54}\n",
      "{'loss': 2.0355, 'learning_rate': 7.900000000000001e-05, 'epoch': 3.54}\n",
      "{'loss': 2.0781, 'learning_rate': 7.920000000000001e-05, 'epoch': 3.55}\n",
      "{'loss': 2.1041, 'learning_rate': 7.94e-05, 'epoch': 3.56}\n",
      "{'loss': 2.1348, 'learning_rate': 7.960000000000001e-05, 'epoch': 3.57}\n",
      "{'loss': 2.1291, 'learning_rate': 7.98e-05, 'epoch': 3.58}\n",
      "{'loss': 2.0689, 'learning_rate': 8e-05, 'epoch': 3.59}\n",
      "{'loss': 2.0935, 'learning_rate': 8.020000000000001e-05, 'epoch': 3.6}\n",
      "{'loss': 2.0766, 'learning_rate': 8.04e-05, 'epoch': 3.61}\n",
      "{'loss': 2.0543, 'learning_rate': 8.060000000000001e-05, 'epoch': 3.62}\n",
      "{'loss': 2.0104, 'learning_rate': 8.080000000000001e-05, 'epoch': 3.62}\n",
      "{'loss': 2.0468, 'learning_rate': 8.1e-05, 'epoch': 3.63}\n",
      "{'loss': 2.0061, 'learning_rate': 8.120000000000001e-05, 'epoch': 3.64}\n",
      "{'loss': 2.013, 'learning_rate': 8.14e-05, 'epoch': 3.65}\n",
      "{'loss': 2.0447, 'learning_rate': 8.16e-05, 'epoch': 3.66}\n",
      "{'loss': 2.0897, 'learning_rate': 8.18e-05, 'epoch': 3.67}\n",
      "{'loss': 2.1305, 'learning_rate': 8.2e-05, 'epoch': 3.68}\n",
      "{'loss': 2.0639, 'learning_rate': 8.22e-05, 'epoch': 3.69}\n",
      "{'loss': 1.9986, 'learning_rate': 8.24e-05, 'epoch': 3.7}\n",
      "{'loss': 2.0264, 'learning_rate': 8.26e-05, 'epoch': 3.71}\n",
      "{'loss': 2.0165, 'learning_rate': 8.28e-05, 'epoch': 3.71}\n",
      "{'loss': 2.0144, 'learning_rate': 8.3e-05, 'epoch': 3.72}\n",
      "{'loss': 2.1081, 'learning_rate': 8.32e-05, 'epoch': 3.73}\n",
      "{'loss': 2.0383, 'learning_rate': 8.34e-05, 'epoch': 3.74}\n",
      "{'loss': 2.0823, 'learning_rate': 8.36e-05, 'epoch': 3.75}\n",
      "{'loss': 2.1291, 'learning_rate': 8.38e-05, 'epoch': 3.76}\n",
      "{'loss': 2.0384, 'learning_rate': 8.4e-05, 'epoch': 3.77}\n",
      "{'loss': 2.0433, 'learning_rate': 8.42e-05, 'epoch': 3.78}\n",
      "{'loss': 2.0671, 'learning_rate': 8.44e-05, 'epoch': 3.79}\n",
      "{'loss': 2.0921, 'learning_rate': 8.46e-05, 'epoch': 3.79}\n",
      "{'loss': 2.035, 'learning_rate': 8.48e-05, 'epoch': 3.8}\n",
      "{'loss': 2.0655, 'learning_rate': 8.5e-05, 'epoch': 3.81}\n",
      "{'loss': 2.0672, 'learning_rate': 8.52e-05, 'epoch': 3.82}\n",
      "{'loss': 2.035, 'learning_rate': 8.54e-05, 'epoch': 3.83}\n",
      "{'loss': 2.0889, 'learning_rate': 8.560000000000001e-05, 'epoch': 3.84}\n",
      "{'loss': 1.987, 'learning_rate': 8.58e-05, 'epoch': 3.85}\n",
      "{'loss': 2.0937, 'learning_rate': 8.6e-05, 'epoch': 3.86}\n",
      "{'loss': 2.0889, 'learning_rate': 8.620000000000001e-05, 'epoch': 3.87}\n",
      "{'loss': 1.9811, 'learning_rate': 8.64e-05, 'epoch': 3.88}\n",
      "{'loss': 1.9768, 'learning_rate': 8.66e-05, 'epoch': 3.88}\n",
      "{'loss': 2.097, 'learning_rate': 8.680000000000001e-05, 'epoch': 3.89}\n",
      "{'loss': 2.0847, 'learning_rate': 8.7e-05, 'epoch': 3.9}\n",
      "{'loss': 1.9982, 'learning_rate': 8.72e-05, 'epoch': 3.91}\n",
      "{'loss': 2.071, 'learning_rate': 8.740000000000001e-05, 'epoch': 3.92}\n",
      "{'loss': 1.9648, 'learning_rate': 8.76e-05, 'epoch': 3.93}\n",
      "{'loss': 2.0358, 'learning_rate': 8.78e-05, 'epoch': 3.94}\n",
      "{'loss': 2.0106, 'learning_rate': 8.800000000000001e-05, 'epoch': 3.95}\n",
      "{'loss': 1.9627, 'learning_rate': 8.82e-05, 'epoch': 3.96}\n",
      "{'loss': 2.0094, 'learning_rate': 8.840000000000001e-05, 'epoch': 3.96}\n",
      "{'loss': 1.9642, 'learning_rate': 8.86e-05, 'epoch': 3.97}\n",
      "{'loss': 2.0434, 'learning_rate': 8.88e-05, 'epoch': 3.98}\n",
      "{'loss': 2.02, 'learning_rate': 8.900000000000001e-05, 'epoch': 3.99}\n",
      "{'loss': 2.0163, 'learning_rate': 8.92e-05, 'epoch': 4.0}\n",
      "{'loss': 1.9941, 'learning_rate': 8.94e-05, 'epoch': 4.01}\n",
      "{'loss': 1.9937, 'learning_rate': 8.960000000000001e-05, 'epoch': 4.02}\n",
      "{'loss': 2.0359, 'learning_rate': 8.98e-05, 'epoch': 4.03}\n",
      "{'loss': 2.0302, 'learning_rate': 9e-05, 'epoch': 4.04}\n",
      "{'loss': 1.9959, 'learning_rate': 9.020000000000001e-05, 'epoch': 4.04}\n",
      "{'loss': 1.9637, 'learning_rate': 9.04e-05, 'epoch': 4.05}\n",
      "{'loss': 2.0151, 'learning_rate': 9.06e-05, 'epoch': 4.06}\n",
      "{'loss': 1.9617, 'learning_rate': 9.080000000000001e-05, 'epoch': 4.07}\n",
      "{'loss': 1.9597, 'learning_rate': 9.1e-05, 'epoch': 4.08}\n",
      "{'loss': 1.986, 'learning_rate': 9.120000000000001e-05, 'epoch': 4.09}\n",
      "{'loss': 2.0186, 'learning_rate': 9.140000000000001e-05, 'epoch': 4.1}\n",
      "{'loss': 2.0318, 'learning_rate': 9.16e-05, 'epoch': 4.11}\n",
      "{'loss': 2.0035, 'learning_rate': 9.180000000000001e-05, 'epoch': 4.12}\n",
      "{'loss': 1.9639, 'learning_rate': 9.200000000000001e-05, 'epoch': 4.12}\n",
      "{'loss': 2.0643, 'learning_rate': 9.22e-05, 'epoch': 4.13}\n",
      "{'loss': 1.9938, 'learning_rate': 9.240000000000001e-05, 'epoch': 4.14}\n",
      "{'loss': 2.0072, 'learning_rate': 9.260000000000001e-05, 'epoch': 4.15}\n",
      "{'loss': 1.9734, 'learning_rate': 9.28e-05, 'epoch': 4.16}\n",
      "{'loss': 1.9872, 'learning_rate': 9.300000000000001e-05, 'epoch': 4.17}\n",
      "{'loss': 2.07, 'learning_rate': 9.320000000000002e-05, 'epoch': 4.18}\n",
      "{'loss': 1.8931, 'learning_rate': 9.340000000000001e-05, 'epoch': 4.19}\n",
      "{'loss': 1.9822, 'learning_rate': 9.360000000000001e-05, 'epoch': 4.2}\n",
      "{'loss': 1.9466, 'learning_rate': 9.38e-05, 'epoch': 4.21}\n",
      "{'loss': 1.8816, 'learning_rate': 9.4e-05, 'epoch': 4.21}\n",
      "{'loss': 1.99, 'learning_rate': 9.42e-05, 'epoch': 4.22}\n",
      "{'loss': 2.0125, 'learning_rate': 9.44e-05, 'epoch': 4.23}\n",
      "{'loss': 1.9556, 'learning_rate': 9.46e-05, 'epoch': 4.24}\n",
      "{'loss': 1.939, 'learning_rate': 9.48e-05, 'epoch': 4.25}\n",
      "{'loss': 1.8618, 'learning_rate': 9.5e-05, 'epoch': 4.26}\n",
      "{'loss': 1.9569, 'learning_rate': 9.52e-05, 'epoch': 4.27}\n",
      "{'loss': 2.0487, 'learning_rate': 9.54e-05, 'epoch': 4.28}\n",
      "{'loss': 1.9266, 'learning_rate': 9.56e-05, 'epoch': 4.29}\n",
      "{'loss': 1.9187, 'learning_rate': 9.58e-05, 'epoch': 4.29}\n",
      "{'loss': 1.9818, 'learning_rate': 9.6e-05, 'epoch': 4.3}\n",
      "{'loss': 1.9147, 'learning_rate': 9.620000000000001e-05, 'epoch': 4.31}\n",
      "{'loss': 1.9511, 'learning_rate': 9.64e-05, 'epoch': 4.32}\n",
      "{'loss': 1.991, 'learning_rate': 9.66e-05, 'epoch': 4.33}\n",
      "{'loss': 2.0082, 'learning_rate': 9.680000000000001e-05, 'epoch': 4.34}\n",
      "{'loss': 1.9251, 'learning_rate': 9.7e-05, 'epoch': 4.35}\n",
      "{'loss': 1.9768, 'learning_rate': 9.72e-05, 'epoch': 4.36}\n",
      "{'loss': 1.9718, 'learning_rate': 9.74e-05, 'epoch': 4.37}\n",
      "{'loss': 1.9881, 'learning_rate': 9.76e-05, 'epoch': 4.38}\n",
      "{'loss': 1.9357, 'learning_rate': 9.78e-05, 'epoch': 4.38}\n",
      "{'loss': 1.9743, 'learning_rate': 9.8e-05, 'epoch': 4.39}\n",
      "{'loss': 1.9779, 'learning_rate': 9.82e-05, 'epoch': 4.4}\n",
      "{'loss': 1.9346, 'learning_rate': 9.84e-05, 'epoch': 4.41}\n",
      "{'loss': 2.0073, 'learning_rate': 9.86e-05, 'epoch': 4.42}\n",
      "{'loss': 1.9943, 'learning_rate': 9.88e-05, 'epoch': 4.43}\n",
      "{'loss': 1.9985, 'learning_rate': 9.900000000000001e-05, 'epoch': 4.44}\n",
      "{'loss': 1.982, 'learning_rate': 9.92e-05, 'epoch': 4.45}\n",
      "{'loss': 1.8963, 'learning_rate': 9.94e-05, 'epoch': 4.46}\n",
      "{'loss': 1.9858, 'learning_rate': 9.960000000000001e-05, 'epoch': 4.46}\n",
      "{'loss': 2.0528, 'learning_rate': 9.98e-05, 'epoch': 4.47}\n",
      "{'loss': 2.0145, 'learning_rate': 0.0001, 'epoch': 4.48}\n",
      "{'loss': 2.0114, 'learning_rate': 0.00010020000000000001, 'epoch': 4.49}\n",
      "{'loss': 1.9394, 'learning_rate': 0.0001004, 'epoch': 4.5}\n",
      "{'loss': 1.9484, 'learning_rate': 0.0001006, 'epoch': 4.51}\n",
      "{'loss': 1.9708, 'learning_rate': 0.00010080000000000001, 'epoch': 4.52}\n",
      "{'loss': 1.8772, 'learning_rate': 0.000101, 'epoch': 4.53}\n",
      "{'loss': 1.9369, 'learning_rate': 0.00010120000000000001, 'epoch': 4.54}\n",
      "{'loss': 2.0119, 'learning_rate': 0.00010140000000000001, 'epoch': 4.54}\n",
      "{'loss': 1.946, 'learning_rate': 0.0001016, 'epoch': 4.55}\n",
      "{'loss': 1.8973, 'learning_rate': 0.00010180000000000001, 'epoch': 4.56}\n",
      "{'loss': 1.9401, 'learning_rate': 0.00010200000000000001, 'epoch': 4.57}\n",
      "{'loss': 1.9298, 'learning_rate': 0.0001022, 'epoch': 4.58}\n",
      "{'loss': 1.9371, 'learning_rate': 0.00010240000000000001, 'epoch': 4.59}\n",
      "{'loss': 1.9297, 'learning_rate': 0.00010260000000000001, 'epoch': 4.6}\n",
      "{'loss': 1.9741, 'learning_rate': 0.0001028, 'epoch': 4.61}\n",
      "{'loss': 1.9565, 'learning_rate': 0.00010300000000000001, 'epoch': 4.62}\n",
      "{'loss': 1.9848, 'learning_rate': 0.0001032, 'epoch': 4.62}\n",
      "{'loss': 2.0128, 'learning_rate': 0.0001034, 'epoch': 4.63}\n",
      "{'loss': 1.9122, 'learning_rate': 0.00010360000000000001, 'epoch': 4.64}\n",
      "{'loss': 1.9661, 'learning_rate': 0.0001038, 'epoch': 4.65}\n",
      "{'loss': 1.8981, 'learning_rate': 0.00010400000000000001, 'epoch': 4.66}\n",
      "{'loss': 1.8935, 'learning_rate': 0.00010420000000000001, 'epoch': 4.67}\n",
      "{'loss': 1.9532, 'learning_rate': 0.0001044, 'epoch': 4.68}\n",
      "{'loss': 1.9405, 'learning_rate': 0.00010460000000000001, 'epoch': 4.69}\n",
      "{'loss': 1.8669, 'learning_rate': 0.00010480000000000001, 'epoch': 4.7}\n",
      "{'loss': 1.955, 'learning_rate': 0.000105, 'epoch': 4.71}\n",
      "{'loss': 1.9961, 'learning_rate': 0.00010520000000000001, 'epoch': 4.71}\n",
      "{'loss': 1.9988, 'learning_rate': 0.00010540000000000001, 'epoch': 4.72}\n",
      "{'loss': 1.953, 'learning_rate': 0.0001056, 'epoch': 4.73}\n",
      "{'loss': 1.9255, 'learning_rate': 0.00010580000000000001, 'epoch': 4.74}\n",
      "{'loss': 1.9028, 'learning_rate': 0.00010600000000000002, 'epoch': 4.75}\n",
      "{'loss': 1.9769, 'learning_rate': 0.0001062, 'epoch': 4.76}\n",
      "{'loss': 1.9878, 'learning_rate': 0.00010640000000000001, 'epoch': 4.77}\n",
      "{'loss': 1.9311, 'learning_rate': 0.00010660000000000002, 'epoch': 4.78}\n",
      "{'loss': 1.8878, 'learning_rate': 0.00010680000000000001, 'epoch': 4.79}\n",
      "{'loss': 1.9447, 'learning_rate': 0.00010700000000000001, 'epoch': 4.79}\n",
      "{'loss': 1.9187, 'learning_rate': 0.00010720000000000002, 'epoch': 4.8}\n",
      "{'loss': 1.8719, 'learning_rate': 0.00010740000000000001, 'epoch': 4.81}\n",
      "{'loss': 1.9503, 'learning_rate': 0.00010760000000000001, 'epoch': 4.82}\n",
      "{'loss': 1.8571, 'learning_rate': 0.00010780000000000002, 'epoch': 4.83}\n",
      "{'loss': 1.9182, 'learning_rate': 0.00010800000000000001, 'epoch': 4.84}\n",
      "{'loss': 1.9081, 'learning_rate': 0.00010820000000000001, 'epoch': 4.85}\n",
      "{'loss': 1.9354, 'learning_rate': 0.00010840000000000002, 'epoch': 4.86}\n",
      "{'loss': 1.8678, 'learning_rate': 0.00010860000000000001, 'epoch': 4.87}\n",
      "{'loss': 1.926, 'learning_rate': 0.00010880000000000002, 'epoch': 4.88}\n",
      "{'loss': 1.9663, 'learning_rate': 0.000109, 'epoch': 4.88}\n",
      "{'loss': 1.8784, 'learning_rate': 0.00010920000000000001, 'epoch': 4.89}\n",
      "{'loss': 1.9137, 'learning_rate': 0.00010940000000000002, 'epoch': 4.9}\n",
      "{'loss': 1.865, 'learning_rate': 0.00010960000000000001, 'epoch': 4.91}\n",
      "{'loss': 1.8925, 'learning_rate': 0.00010980000000000001, 'epoch': 4.92}\n",
      "{'loss': 1.8443, 'learning_rate': 0.00011000000000000002, 'epoch': 4.93}\n",
      "{'loss': 1.9355, 'learning_rate': 0.00011020000000000001, 'epoch': 4.94}\n",
      "{'loss': 1.8843, 'learning_rate': 0.00011040000000000001, 'epoch': 4.95}\n",
      "{'loss': 1.9564, 'learning_rate': 0.00011060000000000002, 'epoch': 4.96}\n",
      "{'loss': 1.8785, 'learning_rate': 0.00011080000000000001, 'epoch': 4.96}\n",
      "{'loss': 1.9132, 'learning_rate': 0.00011100000000000001, 'epoch': 4.97}\n",
      "{'loss': 1.9331, 'learning_rate': 0.00011120000000000002, 'epoch': 4.98}\n",
      "{'loss': 1.8654, 'learning_rate': 0.00011140000000000001, 'epoch': 4.99}\n",
      "{'loss': 1.9087, 'learning_rate': 0.00011160000000000002, 'epoch': 5.0}\n",
      "{'loss': 1.9341, 'learning_rate': 0.00011180000000000002, 'epoch': 5.01}\n",
      "{'loss': 1.858, 'learning_rate': 0.00011200000000000001, 'epoch': 5.02}\n",
      "{'loss': 1.8756, 'learning_rate': 0.00011220000000000002, 'epoch': 5.03}\n",
      "{'loss': 1.8675, 'learning_rate': 0.00011240000000000002, 'epoch': 5.04}\n",
      "{'loss': 1.8407, 'learning_rate': 0.0001126, 'epoch': 5.04}\n",
      "{'loss': 1.9012, 'learning_rate': 0.00011279999999999999, 'epoch': 5.05}\n",
      "{'loss': 1.9054, 'learning_rate': 0.000113, 'epoch': 5.06}\n",
      "{'loss': 1.8482, 'learning_rate': 0.0001132, 'epoch': 5.07}\n",
      "{'loss': 1.8509, 'learning_rate': 0.00011339999999999999, 'epoch': 5.08}\n",
      "{'loss': 1.9308, 'learning_rate': 0.0001136, 'epoch': 5.09}\n",
      "{'loss': 1.8513, 'learning_rate': 0.0001138, 'epoch': 5.1}\n",
      "{'loss': 1.8489, 'learning_rate': 0.00011399999999999999, 'epoch': 5.11}\n",
      "{'loss': 1.9523, 'learning_rate': 0.0001142, 'epoch': 5.12}\n",
      "{'loss': 1.863, 'learning_rate': 0.0001144, 'epoch': 5.12}\n",
      "{'loss': 1.9717, 'learning_rate': 0.0001146, 'epoch': 5.13}\n",
      "{'loss': 1.8702, 'learning_rate': 0.0001148, 'epoch': 5.14}\n",
      "{'loss': 1.819, 'learning_rate': 0.00011499999999999999, 'epoch': 5.15}\n",
      "{'loss': 1.8887, 'learning_rate': 0.0001152, 'epoch': 5.16}\n",
      "{'loss': 1.8656, 'learning_rate': 0.0001154, 'epoch': 5.17}\n",
      "{'loss': 1.8599, 'learning_rate': 0.00011559999999999999, 'epoch': 5.18}\n",
      "{'loss': 1.8657, 'learning_rate': 0.0001158, 'epoch': 5.19}\n",
      "{'loss': 1.8386, 'learning_rate': 0.000116, 'epoch': 5.2}\n",
      "{'loss': 1.8488, 'learning_rate': 0.00011619999999999999, 'epoch': 5.21}\n",
      "{'loss': 1.8844, 'learning_rate': 0.0001164, 'epoch': 5.21}\n",
      "{'loss': 1.7369, 'learning_rate': 0.0001166, 'epoch': 5.22}\n",
      "{'loss': 1.9115, 'learning_rate': 0.00011679999999999999, 'epoch': 5.23}\n",
      "{'loss': 1.844, 'learning_rate': 0.000117, 'epoch': 5.24}\n",
      "{'loss': 1.8548, 'learning_rate': 0.0001172, 'epoch': 5.25}\n",
      "{'loss': 1.785, 'learning_rate': 0.0001174, 'epoch': 5.26}\n",
      "{'loss': 1.8692, 'learning_rate': 0.0001176, 'epoch': 5.27}\n",
      "{'loss': 1.8415, 'learning_rate': 0.0001178, 'epoch': 5.28}\n",
      "{'loss': 1.8167, 'learning_rate': 0.000118, 'epoch': 5.29}\n",
      "{'loss': 1.8399, 'learning_rate': 0.0001182, 'epoch': 5.29}\n",
      "{'loss': 1.9049, 'learning_rate': 0.0001184, 'epoch': 5.3}\n",
      "{'loss': 1.7199, 'learning_rate': 0.0001186, 'epoch': 5.31}\n",
      "{'loss': 1.8291, 'learning_rate': 0.0001188, 'epoch': 5.32}\n",
      "{'loss': 1.9414, 'learning_rate': 0.000119, 'epoch': 5.33}\n",
      "{'loss': 1.8629, 'learning_rate': 0.0001192, 'epoch': 5.34}\n",
      "{'loss': 1.8393, 'learning_rate': 0.0001194, 'epoch': 5.35}\n",
      "{'loss': 1.8826, 'learning_rate': 0.00011960000000000001, 'epoch': 5.36}\n",
      "{'loss': 1.805, 'learning_rate': 0.0001198, 'epoch': 5.37}\n",
      "{'loss': 1.8371, 'learning_rate': 0.00012, 'epoch': 5.38}\n",
      "{'loss': 1.7781, 'learning_rate': 0.00012020000000000001, 'epoch': 5.38}\n",
      "{'loss': 1.8611, 'learning_rate': 0.0001204, 'epoch': 5.39}\n",
      "{'loss': 1.7822, 'learning_rate': 0.0001206, 'epoch': 5.4}\n",
      "{'loss': 1.8535, 'learning_rate': 0.0001208, 'epoch': 5.41}\n",
      "{'loss': 1.8864, 'learning_rate': 0.000121, 'epoch': 5.42}\n",
      "{'loss': 1.8608, 'learning_rate': 0.0001212, 'epoch': 5.43}\n",
      "{'loss': 1.8716, 'learning_rate': 0.0001214, 'epoch': 5.44}\n",
      "{'loss': 1.8407, 'learning_rate': 0.0001216, 'epoch': 5.45}\n",
      "{'loss': 1.8528, 'learning_rate': 0.0001218, 'epoch': 5.46}\n",
      "{'loss': 1.9116, 'learning_rate': 0.000122, 'epoch': 5.46}\n",
      "{'loss': 1.9266, 'learning_rate': 0.00012220000000000002, 'epoch': 5.47}\n",
      "{'loss': 1.871, 'learning_rate': 0.0001224, 'epoch': 5.48}\n",
      "{'loss': 1.8308, 'learning_rate': 0.0001226, 'epoch': 5.49}\n",
      "{'loss': 1.8733, 'learning_rate': 0.0001228, 'epoch': 5.5}\n",
      "{'loss': 1.8205, 'learning_rate': 0.000123, 'epoch': 5.51}\n",
      "{'loss': 1.8343, 'learning_rate': 0.0001232, 'epoch': 5.52}\n",
      "{'loss': 1.8124, 'learning_rate': 0.00012340000000000002, 'epoch': 5.53}\n",
      "{'loss': 1.8128, 'learning_rate': 0.0001236, 'epoch': 5.54}\n",
      "{'loss': 1.7906, 'learning_rate': 0.0001238, 'epoch': 5.54}\n",
      "{'loss': 1.8579, 'learning_rate': 0.000124, 'epoch': 5.55}\n",
      "{'loss': 1.8203, 'learning_rate': 0.0001242, 'epoch': 5.56}\n",
      "{'loss': 1.817, 'learning_rate': 0.00012440000000000002, 'epoch': 5.57}\n",
      "{'loss': 1.803, 'learning_rate': 0.0001246, 'epoch': 5.58}\n",
      "{'loss': 1.8983, 'learning_rate': 0.0001248, 'epoch': 5.59}\n",
      "{'loss': 1.8138, 'learning_rate': 0.000125, 'epoch': 5.6}\n",
      "{'loss': 1.7522, 'learning_rate': 0.0001252, 'epoch': 5.61}\n",
      "{'loss': 1.881, 'learning_rate': 0.0001254, 'epoch': 5.62}\n",
      "{'loss': 1.7822, 'learning_rate': 0.00012560000000000002, 'epoch': 5.62}\n",
      "{'loss': 1.7644, 'learning_rate': 0.0001258, 'epoch': 5.63}\n",
      "{'loss': 1.8163, 'learning_rate': 0.000126, 'epoch': 5.64}\n",
      "{'loss': 1.8722, 'learning_rate': 0.0001262, 'epoch': 5.65}\n",
      "{'loss': 1.8432, 'learning_rate': 0.0001264, 'epoch': 5.66}\n",
      "{'loss': 1.7872, 'learning_rate': 0.00012660000000000001, 'epoch': 5.67}\n",
      "{'loss': 1.9213, 'learning_rate': 0.00012680000000000002, 'epoch': 5.68}\n",
      "{'loss': 1.8623, 'learning_rate': 0.000127, 'epoch': 5.69}\n",
      "{'loss': 1.836, 'learning_rate': 0.0001272, 'epoch': 5.7}\n",
      "{'loss': 1.8824, 'learning_rate': 0.0001274, 'epoch': 5.71}\n",
      "{'loss': 1.8009, 'learning_rate': 0.0001276, 'epoch': 5.71}\n",
      "{'loss': 1.8761, 'learning_rate': 0.00012780000000000002, 'epoch': 5.72}\n",
      "{'loss': 1.8047, 'learning_rate': 0.00012800000000000002, 'epoch': 5.73}\n",
      "{'loss': 1.773, 'learning_rate': 0.0001282, 'epoch': 5.74}\n",
      "{'loss': 1.7919, 'learning_rate': 0.0001284, 'epoch': 5.75}\n",
      "{'loss': 1.8307, 'learning_rate': 0.0001286, 'epoch': 5.76}\n",
      "{'loss': 1.7548, 'learning_rate': 0.00012880000000000001, 'epoch': 5.77}\n",
      "{'loss': 1.7893, 'learning_rate': 0.00012900000000000002, 'epoch': 5.78}\n",
      "{'loss': 1.8543, 'learning_rate': 0.00012920000000000002, 'epoch': 5.79}\n",
      "{'loss': 1.7861, 'learning_rate': 0.0001294, 'epoch': 5.79}\n",
      "{'loss': 1.7891, 'learning_rate': 0.0001296, 'epoch': 5.8}\n",
      "{'loss': 1.7941, 'learning_rate': 0.0001298, 'epoch': 5.81}\n",
      "{'loss': 1.7361, 'learning_rate': 0.00013000000000000002, 'epoch': 5.82}\n",
      "{'loss': 1.8904, 'learning_rate': 0.00013020000000000002, 'epoch': 5.83}\n",
      "{'loss': 1.7919, 'learning_rate': 0.0001304, 'epoch': 5.84}\n",
      "{'loss': 1.7648, 'learning_rate': 0.0001306, 'epoch': 5.85}\n",
      "{'loss': 1.8227, 'learning_rate': 0.0001308, 'epoch': 5.86}\n",
      "{'loss': 1.7682, 'learning_rate': 0.000131, 'epoch': 5.87}\n",
      "{'loss': 1.7351, 'learning_rate': 0.00013120000000000002, 'epoch': 5.88}\n",
      "{'loss': 1.7583, 'learning_rate': 0.00013140000000000002, 'epoch': 5.88}\n",
      "{'loss': 1.6631, 'learning_rate': 0.0001316, 'epoch': 5.89}\n",
      "{'loss': 1.7251, 'learning_rate': 0.0001318, 'epoch': 5.9}\n",
      "{'loss': 1.7356, 'learning_rate': 0.000132, 'epoch': 5.91}\n",
      "{'loss': 1.776, 'learning_rate': 0.00013220000000000001, 'epoch': 5.92}\n",
      "{'loss': 1.8208, 'learning_rate': 0.00013240000000000002, 'epoch': 5.93}\n",
      "{'loss': 1.7911, 'learning_rate': 0.00013260000000000002, 'epoch': 5.94}\n",
      "{'loss': 1.7578, 'learning_rate': 0.0001328, 'epoch': 5.95}\n",
      "{'loss': 1.7745, 'learning_rate': 0.000133, 'epoch': 5.96}\n",
      "{'loss': 1.8408, 'learning_rate': 0.0001332, 'epoch': 5.96}\n",
      "{'loss': 1.8321, 'learning_rate': 0.00013340000000000002, 'epoch': 5.97}\n",
      "{'loss': 1.8207, 'learning_rate': 0.00013360000000000002, 'epoch': 5.98}\n",
      "{'loss': 1.7708, 'learning_rate': 0.00013380000000000003, 'epoch': 5.99}\n",
      "{'loss': 1.8396, 'learning_rate': 0.000134, 'epoch': 6.0}\n",
      "{'loss': 1.7944, 'learning_rate': 0.0001342, 'epoch': 6.01}\n",
      "{'loss': 1.7648, 'learning_rate': 0.00013440000000000001, 'epoch': 6.02}\n",
      "{'loss': 1.7827, 'learning_rate': 0.00013460000000000002, 'epoch': 6.03}\n",
      "{'loss': 1.7629, 'learning_rate': 0.00013480000000000002, 'epoch': 6.04}\n",
      "{'loss': 1.7698, 'learning_rate': 0.00013500000000000003, 'epoch': 6.04}\n",
      "{'loss': 1.7364, 'learning_rate': 0.0001352, 'epoch': 6.05}\n",
      "{'loss': 1.7753, 'learning_rate': 0.0001354, 'epoch': 6.06}\n",
      "{'loss': 1.7998, 'learning_rate': 0.00013560000000000002, 'epoch': 6.07}\n",
      "{'loss': 1.7479, 'learning_rate': 0.00013580000000000002, 'epoch': 6.08}\n",
      "{'loss': 1.791, 'learning_rate': 0.00013600000000000003, 'epoch': 6.09}\n",
      "{'loss': 1.832, 'learning_rate': 0.0001362, 'epoch': 6.1}\n",
      "{'loss': 1.873, 'learning_rate': 0.0001364, 'epoch': 6.11}\n",
      "{'loss': 1.7537, 'learning_rate': 0.0001366, 'epoch': 6.12}\n",
      "{'loss': 1.7083, 'learning_rate': 0.00013680000000000002, 'epoch': 6.12}\n",
      "{'loss': 1.8681, 'learning_rate': 0.00013700000000000002, 'epoch': 6.13}\n",
      "{'loss': 1.7735, 'learning_rate': 0.00013720000000000003, 'epoch': 6.14}\n",
      "{'loss': 1.7568, 'learning_rate': 0.0001374, 'epoch': 6.15}\n",
      "{'loss': 1.7189, 'learning_rate': 0.00013759999999999998, 'epoch': 6.16}\n",
      "{'loss': 1.7334, 'learning_rate': 0.0001378, 'epoch': 6.17}\n",
      "{'loss': 1.7236, 'learning_rate': 0.000138, 'epoch': 6.18}\n",
      "{'loss': 1.8133, 'learning_rate': 0.0001382, 'epoch': 6.19}\n",
      "{'loss': 1.7616, 'learning_rate': 0.0001384, 'epoch': 6.2}\n",
      "{'loss': 1.7051, 'learning_rate': 0.0001386, 'epoch': 6.21}\n",
      "{'loss': 1.827, 'learning_rate': 0.00013879999999999999, 'epoch': 6.21}\n",
      "{'loss': 1.7386, 'learning_rate': 0.000139, 'epoch': 6.22}\n",
      "{'loss': 1.7057, 'learning_rate': 0.0001392, 'epoch': 6.23}\n",
      "{'loss': 1.742, 'learning_rate': 0.0001394, 'epoch': 6.24}\n",
      "{'loss': 1.7536, 'learning_rate': 0.0001396, 'epoch': 6.25}\n",
      "{'loss': 1.6526, 'learning_rate': 0.0001398, 'epoch': 6.26}\n",
      "{'loss': 1.8159, 'learning_rate': 0.00014, 'epoch': 6.27}\n",
      "{'loss': 1.7907, 'learning_rate': 0.0001402, 'epoch': 6.28}\n",
      "{'loss': 1.8249, 'learning_rate': 0.0001404, 'epoch': 6.29}\n",
      "{'loss': 1.7895, 'learning_rate': 0.0001406, 'epoch': 6.29}\n",
      "{'loss': 1.7087, 'learning_rate': 0.0001408, 'epoch': 6.3}\n",
      "{'loss': 1.6952, 'learning_rate': 0.000141, 'epoch': 6.31}\n",
      "{'loss': 1.7293, 'learning_rate': 0.0001412, 'epoch': 6.32}\n",
      "{'loss': 1.7085, 'learning_rate': 0.0001414, 'epoch': 6.33}\n",
      "{'loss': 1.6867, 'learning_rate': 0.0001416, 'epoch': 6.34}\n",
      "{'loss': 1.7364, 'learning_rate': 0.0001418, 'epoch': 6.35}\n",
      "{'loss': 1.6585, 'learning_rate': 0.000142, 'epoch': 6.36}\n",
      "{'loss': 1.6726, 'learning_rate': 0.0001422, 'epoch': 6.37}\n",
      "{'loss': 1.7898, 'learning_rate': 0.0001424, 'epoch': 6.38}\n",
      "{'loss': 1.6717, 'learning_rate': 0.0001426, 'epoch': 6.38}\n",
      "{'loss': 1.7178, 'learning_rate': 0.0001428, 'epoch': 6.39}\n",
      "{'loss': 1.7111, 'learning_rate': 0.000143, 'epoch': 6.4}\n",
      "{'loss': 1.7962, 'learning_rate': 0.0001432, 'epoch': 6.41}\n",
      "{'loss': 1.7229, 'learning_rate': 0.0001434, 'epoch': 6.42}\n",
      "{'loss': 1.72, 'learning_rate': 0.0001436, 'epoch': 6.43}\n",
      "{'loss': 1.7019, 'learning_rate': 0.0001438, 'epoch': 6.44}\n",
      "{'loss': 1.7551, 'learning_rate': 0.000144, 'epoch': 6.45}\n",
      "{'loss': 1.7314, 'learning_rate': 0.0001442, 'epoch': 6.46}\n",
      "{'loss': 1.783, 'learning_rate': 0.0001444, 'epoch': 6.46}\n",
      "{'loss': 1.6015, 'learning_rate': 0.0001446, 'epoch': 6.47}\n",
      "{'loss': 1.7729, 'learning_rate': 0.0001448, 'epoch': 6.48}\n",
      "{'loss': 1.6802, 'learning_rate': 0.000145, 'epoch': 6.49}\n",
      "{'loss': 1.5866, 'learning_rate': 0.0001452, 'epoch': 6.5}\n",
      "{'loss': 1.7623, 'learning_rate': 0.0001454, 'epoch': 6.51}\n",
      "{'loss': 1.7032, 'learning_rate': 0.00014560000000000002, 'epoch': 6.52}\n",
      "{'loss': 1.6766, 'learning_rate': 0.0001458, 'epoch': 6.53}\n",
      "{'loss': 1.679, 'learning_rate': 0.000146, 'epoch': 6.54}\n",
      "{'loss': 1.6502, 'learning_rate': 0.0001462, 'epoch': 6.54}\n",
      "{'loss': 1.706, 'learning_rate': 0.0001464, 'epoch': 6.55}\n",
      "{'loss': 1.6162, 'learning_rate': 0.0001466, 'epoch': 6.56}\n",
      "{'loss': 1.727, 'learning_rate': 0.00014680000000000002, 'epoch': 6.57}\n",
      "{'loss': 1.6873, 'learning_rate': 0.000147, 'epoch': 6.58}\n",
      "{'loss': 1.6499, 'learning_rate': 0.0001472, 'epoch': 6.59}\n",
      "{'loss': 1.5696, 'learning_rate': 0.0001474, 'epoch': 6.6}\n",
      "{'loss': 1.6031, 'learning_rate': 0.0001476, 'epoch': 6.61}\n",
      "{'loss': 1.6343, 'learning_rate': 0.00014780000000000001, 'epoch': 6.62}\n",
      "{'loss': 1.7595, 'learning_rate': 0.000148, 'epoch': 6.62}\n",
      "{'loss': 1.7044, 'learning_rate': 0.0001482, 'epoch': 6.63}\n",
      "{'loss': 1.7627, 'learning_rate': 0.0001484, 'epoch': 6.64}\n",
      "{'loss': 1.5854, 'learning_rate': 0.0001486, 'epoch': 6.65}\n",
      "{'loss': 1.6727, 'learning_rate': 0.0001488, 'epoch': 6.66}\n",
      "{'loss': 1.7142, 'learning_rate': 0.00014900000000000002, 'epoch': 6.67}\n",
      "{'loss': 1.6895, 'learning_rate': 0.0001492, 'epoch': 6.68}\n",
      "{'loss': 1.6263, 'learning_rate': 0.0001494, 'epoch': 6.69}\n",
      "{'loss': 1.7129, 'learning_rate': 0.0001496, 'epoch': 6.7}\n",
      "{'loss': 1.7236, 'learning_rate': 0.0001498, 'epoch': 6.71}\n",
      "{'loss': 1.5794, 'learning_rate': 0.00015000000000000001, 'epoch': 6.71}\n",
      "{'loss': 1.7656, 'learning_rate': 0.00015020000000000002, 'epoch': 6.72}\n",
      "{'loss': 1.6842, 'learning_rate': 0.0001504, 'epoch': 6.73}\n",
      "{'loss': 1.6906, 'learning_rate': 0.0001506, 'epoch': 6.74}\n",
      "{'loss': 1.5679, 'learning_rate': 0.0001508, 'epoch': 6.75}\n",
      "{'loss': 1.6695, 'learning_rate': 0.000151, 'epoch': 6.76}\n",
      "{'loss': 1.7304, 'learning_rate': 0.00015120000000000002, 'epoch': 6.77}\n",
      "{'loss': 1.6781, 'learning_rate': 0.00015140000000000002, 'epoch': 6.78}\n",
      "{'loss': 1.6695, 'learning_rate': 0.0001516, 'epoch': 6.79}\n",
      "{'loss': 1.7285, 'learning_rate': 0.0001518, 'epoch': 6.79}\n",
      "{'loss': 1.6497, 'learning_rate': 0.000152, 'epoch': 6.8}\n",
      "{'loss': 1.697, 'learning_rate': 0.0001522, 'epoch': 6.81}\n",
      "{'loss': 1.6839, 'learning_rate': 0.00015240000000000002, 'epoch': 6.82}\n",
      "{'loss': 1.6677, 'learning_rate': 0.00015260000000000002, 'epoch': 6.83}\n",
      "{'loss': 1.7595, 'learning_rate': 0.0001528, 'epoch': 6.84}\n",
      "{'loss': 1.6086, 'learning_rate': 0.000153, 'epoch': 6.85}\n",
      "{'loss': 1.6894, 'learning_rate': 0.0001532, 'epoch': 6.86}\n",
      "{'loss': 1.7103, 'learning_rate': 0.00015340000000000002, 'epoch': 6.87}\n",
      "{'loss': 1.616, 'learning_rate': 0.00015360000000000002, 'epoch': 6.88}\n",
      "{'loss': 1.68, 'learning_rate': 0.0001538, 'epoch': 6.88}\n",
      "{'loss': 1.6613, 'learning_rate': 0.000154, 'epoch': 6.89}\n",
      "{'loss': 1.6514, 'learning_rate': 0.0001542, 'epoch': 6.9}\n",
      "{'loss': 1.6943, 'learning_rate': 0.0001544, 'epoch': 6.91}\n",
      "{'loss': 1.6559, 'learning_rate': 0.00015460000000000002, 'epoch': 6.92}\n",
      "{'loss': 1.6827, 'learning_rate': 0.00015480000000000002, 'epoch': 6.93}\n",
      "{'loss': 1.6342, 'learning_rate': 0.000155, 'epoch': 6.94}\n",
      "{'loss': 1.6985, 'learning_rate': 0.0001552, 'epoch': 6.95}\n",
      "{'loss': 1.6959, 'learning_rate': 0.0001554, 'epoch': 6.96}\n",
      "{'loss': 1.5775, 'learning_rate': 0.00015560000000000001, 'epoch': 6.96}\n",
      "{'loss': 1.6235, 'learning_rate': 0.00015580000000000002, 'epoch': 6.97}\n",
      "{'loss': 1.6237, 'learning_rate': 0.00015600000000000002, 'epoch': 6.98}\n",
      "{'loss': 1.5773, 'learning_rate': 0.0001562, 'epoch': 6.99}\n",
      "{'loss': 1.6652, 'learning_rate': 0.0001564, 'epoch': 7.0}\n",
      "{'loss': 1.6246, 'learning_rate': 0.0001566, 'epoch': 7.01}\n",
      "{'loss': 1.6044, 'learning_rate': 0.00015680000000000002, 'epoch': 7.02}\n",
      "{'loss': 1.624, 'learning_rate': 0.00015700000000000002, 'epoch': 7.03}\n",
      "{'loss': 1.6017, 'learning_rate': 0.00015720000000000003, 'epoch': 7.04}\n",
      "{'loss': 1.704, 'learning_rate': 0.0001574, 'epoch': 7.04}\n",
      "{'loss': 1.5898, 'learning_rate': 0.0001576, 'epoch': 7.05}\n",
      "{'loss': 1.707, 'learning_rate': 0.00015780000000000001, 'epoch': 7.06}\n",
      "{'loss': 1.5727, 'learning_rate': 0.00015800000000000002, 'epoch': 7.07}\n",
      "{'loss': 1.5735, 'learning_rate': 0.00015820000000000002, 'epoch': 7.08}\n",
      "{'loss': 1.5692, 'learning_rate': 0.00015840000000000003, 'epoch': 7.09}\n",
      "{'loss': 1.6554, 'learning_rate': 0.0001586, 'epoch': 7.1}\n",
      "{'loss': 1.5789, 'learning_rate': 0.0001588, 'epoch': 7.11}\n",
      "{'loss': 1.6314, 'learning_rate': 0.00015900000000000002, 'epoch': 7.12}\n",
      "{'loss': 1.6202, 'learning_rate': 0.00015920000000000002, 'epoch': 7.12}\n",
      "{'loss': 1.6052, 'learning_rate': 0.00015940000000000003, 'epoch': 7.13}\n",
      "{'loss': 1.6839, 'learning_rate': 0.0001596, 'epoch': 7.14}\n",
      "{'loss': 1.7362, 'learning_rate': 0.0001598, 'epoch': 7.15}\n",
      "{'loss': 1.7239, 'learning_rate': 0.00016, 'epoch': 7.16}\n",
      "{'loss': 1.6208, 'learning_rate': 0.00016020000000000002, 'epoch': 7.17}\n",
      "{'loss': 1.6039, 'learning_rate': 0.00016040000000000002, 'epoch': 7.18}\n",
      "{'loss': 1.6015, 'learning_rate': 0.00016060000000000003, 'epoch': 7.19}\n",
      "{'loss': 1.6396, 'learning_rate': 0.0001608, 'epoch': 7.2}\n",
      "{'loss': 1.689, 'learning_rate': 0.000161, 'epoch': 7.21}\n",
      "{'loss': 1.6064, 'learning_rate': 0.00016120000000000002, 'epoch': 7.21}\n",
      "{'loss': 1.6309, 'learning_rate': 0.00016140000000000002, 'epoch': 7.22}\n",
      "{'loss': 1.5696, 'learning_rate': 0.00016160000000000002, 'epoch': 7.23}\n",
      "{'loss': 1.5616, 'learning_rate': 0.00016180000000000003, 'epoch': 7.24}\n",
      "{'loss': 1.6114, 'learning_rate': 0.000162, 'epoch': 7.25}\n",
      "{'loss': 1.5317, 'learning_rate': 0.0001622, 'epoch': 7.26}\n",
      "{'loss': 1.6412, 'learning_rate': 0.00016240000000000002, 'epoch': 7.27}\n",
      "{'loss': 1.594, 'learning_rate': 0.0001626, 'epoch': 7.28}\n",
      "{'loss': 1.5463, 'learning_rate': 0.0001628, 'epoch': 7.29}\n",
      "{'loss': 1.5734, 'learning_rate': 0.000163, 'epoch': 7.29}\n",
      "{'loss': 1.7151, 'learning_rate': 0.0001632, 'epoch': 7.3}\n",
      "{'loss': 1.4453, 'learning_rate': 0.0001634, 'epoch': 7.31}\n",
      "{'loss': 1.5634, 'learning_rate': 0.0001636, 'epoch': 7.32}\n",
      "{'loss': 1.5732, 'learning_rate': 0.0001638, 'epoch': 7.33}\n",
      "{'loss': 1.6584, 'learning_rate': 0.000164, 'epoch': 7.34}\n",
      "{'loss': 1.7097, 'learning_rate': 0.0001642, 'epoch': 7.35}\n",
      "{'loss': 1.6523, 'learning_rate': 0.0001644, 'epoch': 7.36}\n",
      "{'loss': 1.5894, 'learning_rate': 0.0001646, 'epoch': 7.37}\n",
      "{'loss': 1.5655, 'learning_rate': 0.0001648, 'epoch': 7.38}\n",
      "{'loss': 1.6059, 'learning_rate': 0.000165, 'epoch': 7.38}\n",
      "{'loss': 1.5869, 'learning_rate': 0.0001652, 'epoch': 7.39}\n",
      "{'loss': 1.4933, 'learning_rate': 0.0001654, 'epoch': 7.4}\n",
      "{'loss': 1.6102, 'learning_rate': 0.0001656, 'epoch': 7.41}\n",
      "{'loss': 1.5799, 'learning_rate': 0.0001658, 'epoch': 7.42}\n",
      "{'loss': 1.5949, 'learning_rate': 0.000166, 'epoch': 7.43}\n",
      "{'loss': 1.6005, 'learning_rate': 0.0001662, 'epoch': 7.44}\n",
      "{'loss': 1.4821, 'learning_rate': 0.0001664, 'epoch': 7.45}\n",
      "{'loss': 1.5662, 'learning_rate': 0.0001666, 'epoch': 7.46}\n",
      "{'loss': 1.6074, 'learning_rate': 0.0001668, 'epoch': 7.46}\n",
      "{'loss': 1.6303, 'learning_rate': 0.000167, 'epoch': 7.47}\n",
      "{'loss': 1.6418, 'learning_rate': 0.0001672, 'epoch': 7.48}\n",
      "{'loss': 1.5277, 'learning_rate': 0.0001674, 'epoch': 7.49}\n",
      "{'loss': 1.7533, 'learning_rate': 0.0001676, 'epoch': 7.5}\n",
      "{'loss': 1.6945, 'learning_rate': 0.0001678, 'epoch': 7.51}\n",
      "{'loss': 1.5582, 'learning_rate': 0.000168, 'epoch': 7.52}\n",
      "{'loss': 1.5866, 'learning_rate': 0.0001682, 'epoch': 7.53}\n",
      "{'loss': 1.5582, 'learning_rate': 0.0001684, 'epoch': 7.54}\n",
      "{'loss': 1.6198, 'learning_rate': 0.0001686, 'epoch': 7.54}\n",
      "{'loss': 1.6403, 'learning_rate': 0.0001688, 'epoch': 7.55}\n",
      "{'loss': 1.5745, 'learning_rate': 0.00016900000000000002, 'epoch': 7.56}\n",
      "{'loss': 1.4554, 'learning_rate': 0.0001692, 'epoch': 7.57}\n",
      "{'loss': 1.4724, 'learning_rate': 0.0001694, 'epoch': 7.58}\n",
      "{'loss': 1.5249, 'learning_rate': 0.0001696, 'epoch': 7.59}\n",
      "{'loss': 1.4995, 'learning_rate': 0.0001698, 'epoch': 7.6}\n",
      "{'loss': 1.6477, 'learning_rate': 0.00017, 'epoch': 7.61}\n",
      "{'loss': 1.5876, 'learning_rate': 0.00017020000000000002, 'epoch': 7.62}\n",
      "{'loss': 1.5261, 'learning_rate': 0.0001704, 'epoch': 7.62}\n",
      "{'loss': 1.5808, 'learning_rate': 0.0001706, 'epoch': 7.63}\n",
      "{'loss': 1.6701, 'learning_rate': 0.0001708, 'epoch': 7.64}\n",
      "{'loss': 1.5499, 'learning_rate': 0.000171, 'epoch': 7.65}\n",
      "{'loss': 1.7025, 'learning_rate': 0.00017120000000000001, 'epoch': 7.66}\n",
      "{'loss': 1.565, 'learning_rate': 0.0001714, 'epoch': 7.67}\n",
      "{'loss': 1.5081, 'learning_rate': 0.0001716, 'epoch': 7.68}\n",
      "{'loss': 1.5957, 'learning_rate': 0.0001718, 'epoch': 7.69}\n",
      "{'loss': 1.6861, 'learning_rate': 0.000172, 'epoch': 7.7}\n",
      "{'loss': 1.584, 'learning_rate': 0.0001722, 'epoch': 7.71}\n",
      "{'loss': 1.5586, 'learning_rate': 0.00017240000000000002, 'epoch': 7.71}\n",
      "{'loss': 1.5917, 'learning_rate': 0.0001726, 'epoch': 7.72}\n",
      "{'loss': 1.4347, 'learning_rate': 0.0001728, 'epoch': 7.73}\n",
      "{'loss': 1.62, 'learning_rate': 0.000173, 'epoch': 7.74}\n",
      "{'loss': 1.5485, 'learning_rate': 0.0001732, 'epoch': 7.75}\n",
      "{'loss': 1.5717, 'learning_rate': 0.0001734, 'epoch': 7.76}\n",
      "{'loss': 1.5137, 'learning_rate': 0.00017360000000000002, 'epoch': 7.77}\n",
      "{'loss': 1.5256, 'learning_rate': 0.0001738, 'epoch': 7.78}\n",
      "{'loss': 1.5028, 'learning_rate': 0.000174, 'epoch': 7.79}\n",
      "{'loss': 1.5371, 'learning_rate': 0.0001742, 'epoch': 7.79}\n",
      "{'loss': 1.5156, 'learning_rate': 0.0001744, 'epoch': 7.8}\n",
      "{'loss': 1.5638, 'learning_rate': 0.00017460000000000002, 'epoch': 7.81}\n",
      "{'loss': 1.5919, 'learning_rate': 0.00017480000000000002, 'epoch': 7.82}\n",
      "{'loss': 1.4868, 'learning_rate': 0.000175, 'epoch': 7.83}\n",
      "{'loss': 1.5184, 'learning_rate': 0.0001752, 'epoch': 7.84}\n",
      "{'loss': 1.5754, 'learning_rate': 0.0001754, 'epoch': 7.85}\n",
      "{'loss': 1.541, 'learning_rate': 0.0001756, 'epoch': 7.86}\n",
      "{'loss': 1.5829, 'learning_rate': 0.00017580000000000002, 'epoch': 7.87}\n",
      "{'loss': 1.5706, 'learning_rate': 0.00017600000000000002, 'epoch': 7.88}\n",
      "{'loss': 1.4973, 'learning_rate': 0.0001762, 'epoch': 7.88}\n",
      "{'loss': 1.4543, 'learning_rate': 0.0001764, 'epoch': 7.89}\n",
      "{'loss': 1.5135, 'learning_rate': 0.0001766, 'epoch': 7.9}\n",
      "{'loss': 1.4266, 'learning_rate': 0.00017680000000000001, 'epoch': 7.91}\n",
      "{'loss': 1.4875, 'learning_rate': 0.00017700000000000002, 'epoch': 7.92}\n",
      "{'loss': 1.4561, 'learning_rate': 0.0001772, 'epoch': 7.93}\n",
      "{'loss': 1.5183, 'learning_rate': 0.0001774, 'epoch': 7.94}\n",
      "{'loss': 1.5484, 'learning_rate': 0.0001776, 'epoch': 7.95}\n",
      "{'loss': 1.4207, 'learning_rate': 0.0001778, 'epoch': 7.96}\n",
      "{'loss': 1.5441, 'learning_rate': 0.00017800000000000002, 'epoch': 7.96}\n",
      "{'loss': 1.6108, 'learning_rate': 0.00017820000000000002, 'epoch': 7.97}\n",
      "{'loss': 1.5338, 'learning_rate': 0.0001784, 'epoch': 7.98}\n",
      "{'loss': 1.4848, 'learning_rate': 0.0001786, 'epoch': 7.99}\n",
      "{'loss': 1.484, 'learning_rate': 0.0001788, 'epoch': 8.0}\n",
      "{'loss': 1.5151, 'learning_rate': 0.00017900000000000001, 'epoch': 8.01}\n",
      "{'loss': 1.5806, 'learning_rate': 0.00017920000000000002, 'epoch': 8.02}\n",
      "{'loss': 1.529, 'learning_rate': 0.00017940000000000002, 'epoch': 8.03}\n",
      "{'loss': 1.4416, 'learning_rate': 0.0001796, 'epoch': 8.04}\n",
      "{'loss': 1.5876, 'learning_rate': 0.0001798, 'epoch': 8.04}\n",
      "{'loss': 1.6207, 'learning_rate': 0.00018, 'epoch': 8.05}\n",
      "{'loss': 1.5177, 'learning_rate': 0.00018020000000000002, 'epoch': 8.06}\n",
      "{'loss': 1.526, 'learning_rate': 0.00018040000000000002, 'epoch': 8.07}\n",
      "{'loss': 1.4113, 'learning_rate': 0.00018060000000000003, 'epoch': 8.08}\n",
      "{'loss': 1.4738, 'learning_rate': 0.0001808, 'epoch': 8.09}\n",
      "{'loss': 1.5589, 'learning_rate': 0.000181, 'epoch': 8.1}\n",
      "{'loss': 1.4938, 'learning_rate': 0.0001812, 'epoch': 8.11}\n",
      "{'loss': 1.4532, 'learning_rate': 0.00018140000000000002, 'epoch': 8.12}\n",
      "{'loss': 1.4963, 'learning_rate': 0.00018160000000000002, 'epoch': 8.12}\n",
      "{'loss': 1.4951, 'learning_rate': 0.00018180000000000003, 'epoch': 8.13}\n",
      "{'loss': 1.4151, 'learning_rate': 0.000182, 'epoch': 8.14}\n",
      "{'loss': 1.2706, 'learning_rate': 0.0001822, 'epoch': 8.15}\n",
      "{'loss': 1.3923, 'learning_rate': 0.00018240000000000002, 'epoch': 8.16}\n",
      "{'loss': 1.4914, 'learning_rate': 0.00018260000000000002, 'epoch': 8.17}\n",
      "{'loss': 1.5411, 'learning_rate': 0.00018280000000000003, 'epoch': 8.18}\n",
      "{'loss': 1.6435, 'learning_rate': 0.000183, 'epoch': 8.19}\n",
      "{'loss': 1.5338, 'learning_rate': 0.0001832, 'epoch': 8.2}\n",
      "{'loss': 1.387, 'learning_rate': 0.0001834, 'epoch': 8.21}\n",
      "{'loss': 1.5156, 'learning_rate': 0.00018360000000000002, 'epoch': 8.21}\n",
      "{'loss': 1.4026, 'learning_rate': 0.00018380000000000002, 'epoch': 8.22}\n",
      "{'loss': 1.4315, 'learning_rate': 0.00018400000000000003, 'epoch': 8.23}\n",
      "{'loss': 1.4818, 'learning_rate': 0.0001842, 'epoch': 8.24}\n",
      "{'loss': 1.5288, 'learning_rate': 0.0001844, 'epoch': 8.25}\n",
      "{'loss': 1.5462, 'learning_rate': 0.00018460000000000001, 'epoch': 8.26}\n",
      "{'loss': 1.5662, 'learning_rate': 0.00018480000000000002, 'epoch': 8.27}\n",
      "{'loss': 1.5304, 'learning_rate': 0.00018500000000000002, 'epoch': 8.28}\n",
      "{'loss': 1.3997, 'learning_rate': 0.00018520000000000003, 'epoch': 8.29}\n",
      "{'loss': 1.4734, 'learning_rate': 0.0001854, 'epoch': 8.29}\n",
      "{'loss': 1.4975, 'learning_rate': 0.0001856, 'epoch': 8.3}\n",
      "{'loss': 1.4136, 'learning_rate': 0.00018580000000000002, 'epoch': 8.31}\n",
      "{'loss': 1.4683, 'learning_rate': 0.00018600000000000002, 'epoch': 8.32}\n",
      "{'loss': 1.4235, 'learning_rate': 0.00018620000000000003, 'epoch': 8.33}\n",
      "{'loss': 1.4094, 'learning_rate': 0.00018640000000000003, 'epoch': 8.34}\n",
      "{'loss': 1.6006, 'learning_rate': 0.0001866, 'epoch': 8.35}\n",
      "{'loss': 1.3457, 'learning_rate': 0.00018680000000000001, 'epoch': 8.36}\n",
      "{'loss': 1.536, 'learning_rate': 0.00018700000000000002, 'epoch': 8.37}\n",
      "{'loss': 1.493, 'learning_rate': 0.00018720000000000002, 'epoch': 8.38}\n",
      "{'loss': 1.3349, 'learning_rate': 0.00018740000000000003, 'epoch': 8.38}\n",
      "{'loss': 1.6131, 'learning_rate': 0.0001876, 'epoch': 8.39}\n",
      "{'loss': 1.4698, 'learning_rate': 0.0001878, 'epoch': 8.4}\n",
      "{'loss': 1.5593, 'learning_rate': 0.000188, 'epoch': 8.41}\n",
      "{'loss': 1.514, 'learning_rate': 0.0001882, 'epoch': 8.42}\n",
      "{'loss': 1.5614, 'learning_rate': 0.0001884, 'epoch': 8.43}\n",
      "{'loss': 1.5198, 'learning_rate': 0.0001886, 'epoch': 8.44}\n",
      "{'loss': 1.4283, 'learning_rate': 0.0001888, 'epoch': 8.45}\n",
      "{'loss': 1.4343, 'learning_rate': 0.00018899999999999999, 'epoch': 8.46}\n",
      "{'loss': 1.4869, 'learning_rate': 0.0001892, 'epoch': 8.46}\n",
      "{'loss': 1.4525, 'learning_rate': 0.0001894, 'epoch': 8.47}\n",
      "{'loss': 1.6279, 'learning_rate': 0.0001896, 'epoch': 8.48}\n",
      "{'loss': 1.505, 'learning_rate': 0.0001898, 'epoch': 8.49}\n",
      "{'loss': 1.4023, 'learning_rate': 0.00019, 'epoch': 8.5}\n",
      "{'loss': 1.393, 'learning_rate': 0.0001902, 'epoch': 8.51}\n",
      "{'loss': 1.4447, 'learning_rate': 0.0001904, 'epoch': 8.52}\n",
      "{'loss': 1.4289, 'learning_rate': 0.0001906, 'epoch': 8.53}\n",
      "{'loss': 1.3876, 'learning_rate': 0.0001908, 'epoch': 8.54}\n",
      "{'loss': 1.4768, 'learning_rate': 0.000191, 'epoch': 8.54}\n",
      "{'loss': 1.5001, 'learning_rate': 0.0001912, 'epoch': 8.55}\n",
      "{'loss': 1.4113, 'learning_rate': 0.0001914, 'epoch': 8.56}\n",
      "{'loss': 1.4472, 'learning_rate': 0.0001916, 'epoch': 8.57}\n",
      "{'loss': 1.4392, 'learning_rate': 0.0001918, 'epoch': 8.58}\n",
      "{'loss': 1.5254, 'learning_rate': 0.000192, 'epoch': 8.59}\n",
      "{'loss': 1.5035, 'learning_rate': 0.0001922, 'epoch': 8.6}\n",
      "{'loss': 1.3589, 'learning_rate': 0.00019240000000000001, 'epoch': 8.61}\n",
      "{'loss': 1.3674, 'learning_rate': 0.0001926, 'epoch': 8.62}\n",
      "{'loss': 1.4837, 'learning_rate': 0.0001928, 'epoch': 8.62}\n",
      "{'loss': 1.3946, 'learning_rate': 0.000193, 'epoch': 8.63}\n",
      "{'loss': 1.3298, 'learning_rate': 0.0001932, 'epoch': 8.64}\n",
      "{'loss': 1.3339, 'learning_rate': 0.0001934, 'epoch': 8.65}\n",
      "{'loss': 1.4655, 'learning_rate': 0.00019360000000000002, 'epoch': 8.66}\n",
      "{'loss': 1.3985, 'learning_rate': 0.0001938, 'epoch': 8.67}\n",
      "{'loss': 1.4133, 'learning_rate': 0.000194, 'epoch': 8.68}\n",
      "{'loss': 1.3656, 'learning_rate': 0.0001942, 'epoch': 8.69}\n",
      "{'loss': 1.4895, 'learning_rate': 0.0001944, 'epoch': 8.7}\n",
      "{'loss': 1.5275, 'learning_rate': 0.00019460000000000001, 'epoch': 8.71}\n",
      "{'loss': 1.3977, 'learning_rate': 0.0001948, 'epoch': 8.71}\n",
      "{'loss': 1.5291, 'learning_rate': 0.000195, 'epoch': 8.72}\n",
      "{'loss': 1.4438, 'learning_rate': 0.0001952, 'epoch': 8.73}\n",
      "{'loss': 1.4739, 'learning_rate': 0.0001954, 'epoch': 8.74}\n",
      "{'loss': 1.4537, 'learning_rate': 0.0001956, 'epoch': 8.75}\n",
      "{'loss': 1.3257, 'learning_rate': 0.00019580000000000002, 'epoch': 8.76}\n",
      "{'loss': 1.4724, 'learning_rate': 0.000196, 'epoch': 8.77}\n",
      "{'loss': 1.4535, 'learning_rate': 0.0001962, 'epoch': 8.78}\n",
      "{'loss': 1.4, 'learning_rate': 0.0001964, 'epoch': 8.79}\n",
      "{'loss': 1.4095, 'learning_rate': 0.0001966, 'epoch': 8.79}\n",
      "{'loss': 1.3483, 'learning_rate': 0.0001968, 'epoch': 8.8}\n",
      "{'loss': 1.4127, 'learning_rate': 0.00019700000000000002, 'epoch': 8.81}\n",
      "{'loss': 1.3599, 'learning_rate': 0.0001972, 'epoch': 8.82}\n",
      "{'loss': 1.3936, 'learning_rate': 0.0001974, 'epoch': 8.83}\n",
      "{'loss': 1.5814, 'learning_rate': 0.0001976, 'epoch': 8.84}\n",
      "{'loss': 1.3429, 'learning_rate': 0.0001978, 'epoch': 8.85}\n",
      "{'loss': 1.3506, 'learning_rate': 0.00019800000000000002, 'epoch': 8.86}\n",
      "{'loss': 1.4371, 'learning_rate': 0.00019820000000000002, 'epoch': 8.87}\n",
      "{'loss': 1.403, 'learning_rate': 0.0001984, 'epoch': 8.88}\n",
      "{'loss': 1.4108, 'learning_rate': 0.0001986, 'epoch': 8.88}\n",
      "{'loss': 1.3613, 'learning_rate': 0.0001988, 'epoch': 8.89}\n",
      "{'loss': 1.3405, 'learning_rate': 0.000199, 'epoch': 8.9}\n",
      "{'loss': 1.3366, 'learning_rate': 0.00019920000000000002, 'epoch': 8.91}\n",
      "{'loss': 1.3291, 'learning_rate': 0.00019940000000000002, 'epoch': 8.92}\n",
      "{'loss': 1.4202, 'learning_rate': 0.0001996, 'epoch': 8.93}\n",
      "{'loss': 1.4559, 'learning_rate': 0.0001998, 'epoch': 8.94}\n",
      "{'loss': 1.3988, 'learning_rate': 0.0002, 'epoch': 8.95}\n",
      "{'loss': 1.4531, 'learning_rate': 0.0001997058823529412, 'epoch': 8.96}\n",
      "{'loss': 1.2972, 'learning_rate': 0.00019941176470588236, 'epoch': 8.96}\n",
      "{'loss': 1.4056, 'learning_rate': 0.00019911764705882355, 'epoch': 8.97}\n",
      "{'loss': 1.3876, 'learning_rate': 0.00019882352941176472, 'epoch': 8.98}\n",
      "{'loss': 1.4593, 'learning_rate': 0.0001985294117647059, 'epoch': 8.99}\n",
      "{'loss': 1.4475, 'learning_rate': 0.00019823529411764707, 'epoch': 9.0}\n",
      "{'loss': 1.3765, 'learning_rate': 0.00019794117647058826, 'epoch': 9.01}\n",
      "{'loss': 1.3686, 'learning_rate': 0.00019764705882352942, 'epoch': 9.02}\n",
      "{'loss': 1.3244, 'learning_rate': 0.0001973529411764706, 'epoch': 9.03}\n",
      "{'loss': 1.384, 'learning_rate': 0.00019705882352941177, 'epoch': 9.04}\n",
      "{'loss': 1.422, 'learning_rate': 0.00019676470588235294, 'epoch': 9.04}\n",
      "{'loss': 1.3381, 'learning_rate': 0.00019647058823529413, 'epoch': 9.05}\n",
      "{'loss': 1.3806, 'learning_rate': 0.0001961764705882353, 'epoch': 9.06}\n",
      "{'loss': 1.3436, 'learning_rate': 0.00019588235294117648, 'epoch': 9.07}\n",
      "{'loss': 1.3537, 'learning_rate': 0.00019558823529411764, 'epoch': 9.08}\n",
      "{'loss': 1.4276, 'learning_rate': 0.00019529411764705883, 'epoch': 9.09}\n",
      "{'loss': 1.307, 'learning_rate': 0.000195, 'epoch': 9.1}\n",
      "{'loss': 1.4572, 'learning_rate': 0.0001947058823529412, 'epoch': 9.11}\n",
      "{'loss': 1.4614, 'learning_rate': 0.00019441176470588235, 'epoch': 9.12}\n",
      "{'loss': 1.3988, 'learning_rate': 0.00019411764705882354, 'epoch': 9.12}\n",
      "{'loss': 1.3964, 'learning_rate': 0.0001938235294117647, 'epoch': 9.13}\n",
      "{'loss': 1.3437, 'learning_rate': 0.0001935294117647059, 'epoch': 9.14}\n",
      "{'loss': 1.4124, 'learning_rate': 0.00019323529411764708, 'epoch': 9.15}\n",
      "{'loss': 1.35, 'learning_rate': 0.00019294117647058825, 'epoch': 9.16}\n",
      "{'loss': 1.34, 'learning_rate': 0.00019264705882352944, 'epoch': 9.17}\n",
      "{'loss': 1.3069, 'learning_rate': 0.0001923529411764706, 'epoch': 9.18}\n",
      "{'loss': 1.2718, 'learning_rate': 0.0001920588235294118, 'epoch': 9.19}\n",
      "{'loss': 1.4861, 'learning_rate': 0.00019176470588235295, 'epoch': 9.2}\n",
      "{'loss': 1.3643, 'learning_rate': 0.00019147058823529414, 'epoch': 9.21}\n",
      "{'loss': 1.2927, 'learning_rate': 0.0001911764705882353, 'epoch': 9.21}\n",
      "{'loss': 1.2285, 'learning_rate': 0.0001908823529411765, 'epoch': 9.22}\n",
      "{'loss': 1.4093, 'learning_rate': 0.00019058823529411766, 'epoch': 9.23}\n",
      "{'loss': 1.4064, 'learning_rate': 0.00019029411764705882, 'epoch': 9.24}\n",
      "{'loss': 1.4195, 'learning_rate': 0.00019, 'epoch': 9.25}\n",
      "{'loss': 1.2841, 'learning_rate': 0.00018970588235294117, 'epoch': 9.26}\n",
      "{'loss': 1.3269, 'learning_rate': 0.00018941176470588236, 'epoch': 9.27}\n",
      "{'loss': 1.5227, 'learning_rate': 0.00018911764705882353, 'epoch': 9.28}\n",
      "{'loss': 1.2695, 'learning_rate': 0.00018882352941176472, 'epoch': 9.29}\n",
      "{'loss': 1.4562, 'learning_rate': 0.00018852941176470588, 'epoch': 9.29}\n",
      "{'loss': 1.2103, 'learning_rate': 0.00018823529411764707, 'epoch': 9.3}\n",
      "{'loss': 1.3744, 'learning_rate': 0.00018794117647058823, 'epoch': 9.31}\n",
      "{'loss': 1.458, 'learning_rate': 0.00018764705882352942, 'epoch': 9.32}\n",
      "{'loss': 1.2789, 'learning_rate': 0.00018735294117647059, 'epoch': 9.33}\n",
      "{'loss': 1.4402, 'learning_rate': 0.00018705882352941178, 'epoch': 9.34}\n",
      "{'loss': 1.3847, 'learning_rate': 0.00018676470588235297, 'epoch': 9.35}\n",
      "{'loss': 1.2462, 'learning_rate': 0.00018647058823529413, 'epoch': 9.36}\n",
      "{'loss': 1.2929, 'learning_rate': 0.00018617647058823532, 'epoch': 9.37}\n",
      "{'loss': 1.4425, 'learning_rate': 0.00018588235294117648, 'epoch': 9.38}\n",
      "{'loss': 1.2614, 'learning_rate': 0.00018558823529411767, 'epoch': 9.38}\n",
      "{'loss': 1.3116, 'learning_rate': 0.00018529411764705883, 'epoch': 9.39}\n",
      "{'loss': 1.3141, 'learning_rate': 0.00018500000000000002, 'epoch': 9.4}\n",
      "{'loss': 1.3632, 'learning_rate': 0.0001847058823529412, 'epoch': 9.41}\n",
      "{'loss': 1.3245, 'learning_rate': 0.00018441176470588238, 'epoch': 9.42}\n",
      "{'loss': 1.401, 'learning_rate': 0.00018411764705882354, 'epoch': 9.43}\n",
      "{'loss': 1.3447, 'learning_rate': 0.0001838235294117647, 'epoch': 9.44}\n",
      "{'loss': 1.3179, 'learning_rate': 0.0001835294117647059, 'epoch': 9.45}\n",
      "{'loss': 1.4814, 'learning_rate': 0.00018323529411764706, 'epoch': 9.46}\n",
      "{'loss': 1.3784, 'learning_rate': 0.00018294117647058825, 'epoch': 9.46}\n",
      "{'loss': 1.3212, 'learning_rate': 0.0001826470588235294, 'epoch': 9.47}\n",
      "{'loss': 1.3306, 'learning_rate': 0.0001823529411764706, 'epoch': 9.48}\n",
      "{'loss': 1.347, 'learning_rate': 0.00018205882352941176, 'epoch': 9.49}\n",
      "{'loss': 1.2857, 'learning_rate': 0.00018176470588235295, 'epoch': 9.5}\n",
      "{'loss': 1.4246, 'learning_rate': 0.00018147058823529412, 'epoch': 9.51}\n",
      "{'loss': 1.339, 'learning_rate': 0.0001811764705882353, 'epoch': 9.52}\n",
      "{'loss': 1.3976, 'learning_rate': 0.00018088235294117647, 'epoch': 9.53}\n",
      "{'loss': 1.3662, 'learning_rate': 0.00018058823529411766, 'epoch': 9.54}\n",
      "{'loss': 1.2439, 'learning_rate': 0.00018029411764705885, 'epoch': 9.54}\n",
      "{'loss': 1.2335, 'learning_rate': 0.00018, 'epoch': 9.55}\n",
      "{'loss': 1.4283, 'learning_rate': 0.0001797058823529412, 'epoch': 9.56}\n",
      "{'loss': 1.3441, 'learning_rate': 0.00017941176470588236, 'epoch': 9.57}\n",
      "{'loss': 1.286, 'learning_rate': 0.00017911764705882355, 'epoch': 9.58}\n",
      "{'loss': 1.3712, 'learning_rate': 0.00017882352941176472, 'epoch': 9.59}\n",
      "{'loss': 1.3415, 'learning_rate': 0.0001785294117647059, 'epoch': 9.6}\n",
      "{'loss': 1.359, 'learning_rate': 0.00017823529411764707, 'epoch': 9.61}\n",
      "{'loss': 1.3884, 'learning_rate': 0.00017794117647058823, 'epoch': 9.62}\n",
      "{'loss': 1.4119, 'learning_rate': 0.00017764705882352942, 'epoch': 9.62}\n",
      "{'loss': 1.3144, 'learning_rate': 0.00017735294117647059, 'epoch': 9.63}\n",
      "{'loss': 1.2396, 'learning_rate': 0.00017705882352941178, 'epoch': 9.64}\n",
      "{'loss': 1.3058, 'learning_rate': 0.00017676470588235294, 'epoch': 9.65}\n",
      "{'loss': 1.2145, 'learning_rate': 0.00017647058823529413, 'epoch': 9.66}\n",
      "{'loss': 1.3931, 'learning_rate': 0.0001761764705882353, 'epoch': 9.67}\n",
      "{'loss': 1.2192, 'learning_rate': 0.00017588235294117648, 'epoch': 9.68}\n",
      "{'loss': 1.2977, 'learning_rate': 0.00017558823529411765, 'epoch': 9.69}\n",
      "{'loss': 1.276, 'learning_rate': 0.00017529411764705884, 'epoch': 9.7}\n",
      "{'loss': 1.387, 'learning_rate': 0.000175, 'epoch': 9.71}\n",
      "{'loss': 1.3251, 'learning_rate': 0.0001747058823529412, 'epoch': 9.71}\n",
      "{'loss': 1.2865, 'learning_rate': 0.00017441176470588235, 'epoch': 9.72}\n",
      "{'loss': 1.2672, 'learning_rate': 0.00017411764705882354, 'epoch': 9.73}\n",
      "{'loss': 1.2811, 'learning_rate': 0.00017382352941176473, 'epoch': 9.74}\n",
      "{'loss': 1.3293, 'learning_rate': 0.0001735294117647059, 'epoch': 9.75}\n",
      "{'loss': 1.4311, 'learning_rate': 0.00017323529411764708, 'epoch': 9.76}\n",
      "{'loss': 1.2074, 'learning_rate': 0.00017294117647058825, 'epoch': 9.77}\n",
      "{'loss': 1.2378, 'learning_rate': 0.00017264705882352944, 'epoch': 9.78}\n",
      "{'loss': 1.2886, 'learning_rate': 0.0001723529411764706, 'epoch': 9.79}\n",
      "{'loss': 1.3864, 'learning_rate': 0.0001720588235294118, 'epoch': 9.79}\n",
      "{'loss': 1.3205, 'learning_rate': 0.00017176470588235293, 'epoch': 9.8}\n",
      "{'loss': 1.3925, 'learning_rate': 0.00017147058823529412, 'epoch': 9.81}\n",
      "{'loss': 1.3111, 'learning_rate': 0.0001711764705882353, 'epoch': 9.82}\n",
      "{'loss': 1.3582, 'learning_rate': 0.00017088235294117647, 'epoch': 9.83}\n",
      "{'loss': 1.319, 'learning_rate': 0.00017058823529411766, 'epoch': 9.84}\n",
      "{'loss': 1.2576, 'learning_rate': 0.00017029411764705882, 'epoch': 9.85}\n",
      "{'loss': 1.5044, 'learning_rate': 0.00017, 'epoch': 9.86}\n",
      "{'loss': 1.3177, 'learning_rate': 0.00016970588235294118, 'epoch': 9.87}\n",
      "{'loss': 1.4631, 'learning_rate': 0.00016941176470588237, 'epoch': 9.88}\n",
      "{'loss': 1.3571, 'learning_rate': 0.00016911764705882353, 'epoch': 9.88}\n",
      "{'loss': 1.3093, 'learning_rate': 0.00016882352941176472, 'epoch': 9.89}\n",
      "{'loss': 1.3187, 'learning_rate': 0.00016852941176470588, 'epoch': 9.9}\n",
      "{'loss': 1.2446, 'learning_rate': 0.00016823529411764707, 'epoch': 9.91}\n",
      "{'loss': 1.4179, 'learning_rate': 0.00016794117647058823, 'epoch': 9.92}\n",
      "{'loss': 1.2317, 'learning_rate': 0.00016764705882352942, 'epoch': 9.93}\n",
      "{'loss': 1.2703, 'learning_rate': 0.00016735294117647061, 'epoch': 9.94}\n",
      "{'loss': 1.272, 'learning_rate': 0.00016705882352941178, 'epoch': 9.95}\n",
      "{'loss': 1.386, 'learning_rate': 0.00016676470588235297, 'epoch': 9.96}\n",
      "{'loss': 1.2486, 'learning_rate': 0.00016647058823529413, 'epoch': 9.96}\n",
      "{'loss': 1.1444, 'learning_rate': 0.00016617647058823532, 'epoch': 9.97}\n",
      "{'loss': 1.2569, 'learning_rate': 0.00016588235294117648, 'epoch': 9.98}\n",
      "{'loss': 1.3174, 'learning_rate': 0.00016558823529411765, 'epoch': 9.99}\n",
      "{'loss': 1.2051, 'learning_rate': 0.0001652941176470588, 'epoch': 10.0}\n",
      "{'loss': 1.2752, 'learning_rate': 0.000165, 'epoch': 10.01}\n",
      "{'loss': 1.2593, 'learning_rate': 0.0001647058823529412, 'epoch': 10.02}\n",
      "{'loss': 1.3168, 'learning_rate': 0.00016441176470588235, 'epoch': 10.03}\n",
      "{'loss': 1.3117, 'learning_rate': 0.00016411764705882354, 'epoch': 10.04}\n",
      "{'loss': 1.2859, 'learning_rate': 0.0001638235294117647, 'epoch': 10.04}\n",
      "{'loss': 1.3913, 'learning_rate': 0.0001635294117647059, 'epoch': 10.05}\n",
      "{'loss': 1.2408, 'learning_rate': 0.00016323529411764706, 'epoch': 10.06}\n",
      "{'loss': 1.3294, 'learning_rate': 0.00016294117647058825, 'epoch': 10.07}\n",
      "{'loss': 1.3611, 'learning_rate': 0.0001626470588235294, 'epoch': 10.08}\n",
      "{'loss': 1.3458, 'learning_rate': 0.0001623529411764706, 'epoch': 10.09}\n",
      "{'loss': 1.3459, 'learning_rate': 0.00016205882352941176, 'epoch': 10.1}\n",
      "{'loss': 1.3049, 'learning_rate': 0.00016176470588235295, 'epoch': 10.11}\n",
      "{'loss': 1.2114, 'learning_rate': 0.00016147058823529412, 'epoch': 10.12}\n",
      "{'loss': 1.3366, 'learning_rate': 0.0001611764705882353, 'epoch': 10.12}\n",
      "{'loss': 1.3294, 'learning_rate': 0.0001608823529411765, 'epoch': 10.13}\n",
      "{'loss': 1.2617, 'learning_rate': 0.00016058823529411766, 'epoch': 10.14}\n",
      "{'loss': 1.4565, 'learning_rate': 0.00016029411764705885, 'epoch': 10.15}\n",
      "{'loss': 1.3349, 'learning_rate': 0.00016, 'epoch': 10.16}\n",
      "{'loss': 1.142, 'learning_rate': 0.0001597058823529412, 'epoch': 10.17}\n",
      "{'loss': 1.1386, 'learning_rate': 0.00015941176470588237, 'epoch': 10.18}\n",
      "{'loss': 1.1713, 'learning_rate': 0.00015911764705882353, 'epoch': 10.19}\n",
      "{'loss': 1.2121, 'learning_rate': 0.0001588235294117647, 'epoch': 10.2}\n",
      "{'loss': 1.2239, 'learning_rate': 0.00015852941176470588, 'epoch': 10.21}\n",
      "{'loss': 1.2754, 'learning_rate': 0.00015823529411764707, 'epoch': 10.21}\n",
      "{'loss': 1.1022, 'learning_rate': 0.00015794117647058824, 'epoch': 10.22}\n",
      "{'loss': 1.2701, 'learning_rate': 0.00015764705882352943, 'epoch': 10.23}\n",
      "{'loss': 1.2869, 'learning_rate': 0.0001573529411764706, 'epoch': 10.24}\n",
      "{'loss': 1.2161, 'learning_rate': 0.00015705882352941178, 'epoch': 10.25}\n",
      "{'loss': 1.3229, 'learning_rate': 0.00015676470588235294, 'epoch': 10.26}\n",
      "{'loss': 1.231, 'learning_rate': 0.00015647058823529413, 'epoch': 10.27}\n",
      "{'loss': 1.2134, 'learning_rate': 0.0001561764705882353, 'epoch': 10.28}\n",
      "{'loss': 1.2141, 'learning_rate': 0.00015588235294117648, 'epoch': 10.29}\n",
      "{'loss': 1.304, 'learning_rate': 0.00015558823529411765, 'epoch': 10.29}\n",
      "{'loss': 1.2751, 'learning_rate': 0.00015529411764705884, 'epoch': 10.3}\n",
      "{'loss': 1.456, 'learning_rate': 0.000155, 'epoch': 10.31}\n",
      "{'loss': 1.2597, 'learning_rate': 0.0001547058823529412, 'epoch': 10.32}\n",
      "{'loss': 1.2223, 'learning_rate': 0.00015441176470588238, 'epoch': 10.33}\n",
      "{'loss': 1.2601, 'learning_rate': 0.00015411764705882354, 'epoch': 10.34}\n",
      "{'loss': 1.3012, 'learning_rate': 0.00015382352941176473, 'epoch': 10.35}\n",
      "{'loss': 1.2824, 'learning_rate': 0.0001535294117647059, 'epoch': 10.36}\n",
      "{'loss': 1.2255, 'learning_rate': 0.00015323529411764709, 'epoch': 10.37}\n",
      "{'loss': 1.2588, 'learning_rate': 0.00015294117647058822, 'epoch': 10.38}\n",
      "{'loss': 1.2346, 'learning_rate': 0.0001526470588235294, 'epoch': 10.38}\n",
      "{'loss': 1.214, 'learning_rate': 0.00015235294117647057, 'epoch': 10.39}\n",
      "{'loss': 1.1419, 'learning_rate': 0.00015205882352941176, 'epoch': 10.4}\n",
      "{'loss': 1.4156, 'learning_rate': 0.00015176470588235295, 'epoch': 10.41}\n",
      "{'loss': 1.1698, 'learning_rate': 0.00015147058823529412, 'epoch': 10.42}\n",
      "{'loss': 1.3191, 'learning_rate': 0.0001511764705882353, 'epoch': 10.43}\n",
      "{'loss': 1.2659, 'learning_rate': 0.00015088235294117647, 'epoch': 10.44}\n",
      "{'loss': 1.3234, 'learning_rate': 0.00015058823529411766, 'epoch': 10.45}\n",
      "{'loss': 1.3311, 'learning_rate': 0.00015029411764705882, 'epoch': 10.46}\n",
      "{'loss': 1.2467, 'learning_rate': 0.00015000000000000001, 'epoch': 10.46}\n",
      "{'loss': 1.2168, 'learning_rate': 0.00014970588235294118, 'epoch': 10.47}\n",
      "{'loss': 1.1949, 'learning_rate': 0.00014941176470588237, 'epoch': 10.48}\n",
      "{'loss': 1.2856, 'learning_rate': 0.00014911764705882353, 'epoch': 10.49}\n",
      "{'loss': 1.3309, 'learning_rate': 0.00014882352941176472, 'epoch': 10.5}\n",
      "{'loss': 1.2522, 'learning_rate': 0.00014852941176470588, 'epoch': 10.51}\n",
      "{'loss': 1.3778, 'learning_rate': 0.00014823529411764707, 'epoch': 10.52}\n",
      "{'loss': 1.3529, 'learning_rate': 0.00014794117647058826, 'epoch': 10.53}\n",
      "{'loss': 1.1874, 'learning_rate': 0.00014764705882352943, 'epoch': 10.54}\n",
      "{'loss': 1.2409, 'learning_rate': 0.00014735294117647062, 'epoch': 10.54}\n",
      "{'loss': 1.2136, 'learning_rate': 0.00014705882352941178, 'epoch': 10.55}\n",
      "{'loss': 1.3059, 'learning_rate': 0.00014676470588235294, 'epoch': 10.56}\n",
      "{'loss': 1.2788, 'learning_rate': 0.0001464705882352941, 'epoch': 10.57}\n",
      "{'loss': 1.274, 'learning_rate': 0.0001461764705882353, 'epoch': 10.58}\n",
      "{'loss': 1.2503, 'learning_rate': 0.00014588235294117646, 'epoch': 10.59}\n",
      "{'loss': 1.1287, 'learning_rate': 0.00014558823529411765, 'epoch': 10.6}\n",
      "{'loss': 1.2714, 'learning_rate': 0.00014529411764705884, 'epoch': 10.61}\n",
      "{'loss': 1.3306, 'learning_rate': 0.000145, 'epoch': 10.62}\n",
      "{'loss': 1.1928, 'learning_rate': 0.0001447058823529412, 'epoch': 10.62}\n",
      "{'loss': 1.2511, 'learning_rate': 0.00014441176470588235, 'epoch': 10.63}\n",
      "{'loss': 1.1657, 'learning_rate': 0.00014411764705882354, 'epoch': 10.64}\n",
      "{'loss': 1.1709, 'learning_rate': 0.0001438235294117647, 'epoch': 10.65}\n",
      "{'loss': 1.2325, 'learning_rate': 0.0001435294117647059, 'epoch': 10.66}\n",
      "{'loss': 1.258, 'learning_rate': 0.00014323529411764706, 'epoch': 10.67}\n",
      "{'loss': 1.2382, 'learning_rate': 0.00014294117647058825, 'epoch': 10.68}\n",
      "{'loss': 1.2121, 'learning_rate': 0.0001426470588235294, 'epoch': 10.69}\n",
      "{'loss': 1.178, 'learning_rate': 0.0001423529411764706, 'epoch': 10.7}\n",
      "{'loss': 1.3449, 'learning_rate': 0.00014205882352941177, 'epoch': 10.71}\n",
      "{'loss': 1.2249, 'learning_rate': 0.00014176470588235296, 'epoch': 10.71}\n",
      "{'loss': 1.1782, 'learning_rate': 0.00014147058823529415, 'epoch': 10.72}\n",
      "{'loss': 1.1387, 'learning_rate': 0.0001411764705882353, 'epoch': 10.73}\n",
      "{'loss': 1.2258, 'learning_rate': 0.0001408823529411765, 'epoch': 10.74}\n",
      "{'loss': 1.1252, 'learning_rate': 0.00014058823529411763, 'epoch': 10.75}\n",
      "{'loss': 1.2081, 'learning_rate': 0.00014029411764705882, 'epoch': 10.76}\n",
      "{'loss': 1.2037, 'learning_rate': 0.00014, 'epoch': 10.77}\n",
      "{'loss': 1.1559, 'learning_rate': 0.00013970588235294118, 'epoch': 10.78}\n",
      "{'loss': 1.386, 'learning_rate': 0.00013941176470588234, 'epoch': 10.79}\n",
      "{'loss': 1.1351, 'learning_rate': 0.00013911764705882353, 'epoch': 10.79}\n",
      "{'loss': 1.2097, 'learning_rate': 0.00013882352941176472, 'epoch': 10.8}\n",
      "{'loss': 1.1542, 'learning_rate': 0.00013852941176470588, 'epoch': 10.81}\n",
      "{'loss': 1.1919, 'learning_rate': 0.00013823529411764707, 'epoch': 10.82}\n",
      "{'loss': 1.0986, 'learning_rate': 0.00013794117647058824, 'epoch': 10.83}\n",
      "{'loss': 1.332, 'learning_rate': 0.00013764705882352943, 'epoch': 10.84}\n",
      "{'loss': 1.2941, 'learning_rate': 0.0001373529411764706, 'epoch': 10.85}\n",
      "{'loss': 1.2502, 'learning_rate': 0.00013705882352941178, 'epoch': 10.86}\n",
      "{'loss': 1.2221, 'learning_rate': 0.00013676470588235294, 'epoch': 10.87}\n",
      "{'loss': 1.1292, 'learning_rate': 0.00013647058823529413, 'epoch': 10.88}\n",
      "{'loss': 1.3406, 'learning_rate': 0.0001361764705882353, 'epoch': 10.88}\n",
      "{'loss': 1.2581, 'learning_rate': 0.00013588235294117649, 'epoch': 10.89}\n",
      "{'loss': 1.1355, 'learning_rate': 0.00013558823529411765, 'epoch': 10.9}\n",
      "{'loss': 1.2024, 'learning_rate': 0.00013529411764705884, 'epoch': 10.91}\n",
      "{'loss': 1.1683, 'learning_rate': 0.00013500000000000003, 'epoch': 10.92}\n",
      "{'loss': 1.2233, 'learning_rate': 0.0001347058823529412, 'epoch': 10.93}\n",
      "{'loss': 1.2329, 'learning_rate': 0.00013441176470588238, 'epoch': 10.94}\n",
      "{'loss': 1.2078, 'learning_rate': 0.00013411764705882352, 'epoch': 10.95}\n",
      "{'loss': 1.4346, 'learning_rate': 0.0001338235294117647, 'epoch': 10.96}\n",
      "{'loss': 1.2441, 'learning_rate': 0.00013352941176470587, 'epoch': 10.96}\n",
      "{'loss': 1.0993, 'learning_rate': 0.00013323529411764706, 'epoch': 10.97}\n",
      "{'loss': 1.2398, 'learning_rate': 0.00013294117647058822, 'epoch': 10.98}\n",
      "{'loss': 1.2894, 'learning_rate': 0.00013264705882352941, 'epoch': 10.99}\n",
      "{'loss': 1.3426, 'learning_rate': 0.0001323529411764706, 'epoch': 11.0}\n",
      "{'loss': 1.2651, 'learning_rate': 0.00013205882352941177, 'epoch': 11.01}\n",
      "{'loss': 1.1124, 'learning_rate': 0.00013176470588235296, 'epoch': 11.02}\n",
      "{'loss': 1.3298, 'learning_rate': 0.00013147058823529412, 'epoch': 11.03}\n",
      "{'loss': 1.1577, 'learning_rate': 0.0001311764705882353, 'epoch': 11.04}\n",
      "{'loss': 1.1674, 'learning_rate': 0.00013088235294117647, 'epoch': 11.04}\n",
      "{'loss': 1.0974, 'learning_rate': 0.00013058823529411766, 'epoch': 11.05}\n",
      "{'loss': 1.3214, 'learning_rate': 0.00013029411764705883, 'epoch': 11.06}\n",
      "{'loss': 1.3096, 'learning_rate': 0.00013000000000000002, 'epoch': 11.07}\n",
      "{'loss': 1.2761, 'learning_rate': 0.00012970588235294118, 'epoch': 11.08}\n",
      "{'loss': 1.2749, 'learning_rate': 0.00012941176470588237, 'epoch': 11.09}\n",
      "{'loss': 1.2197, 'learning_rate': 0.00012911764705882353, 'epoch': 11.1}\n",
      "{'loss': 1.2287, 'learning_rate': 0.00012882352941176472, 'epoch': 11.11}\n",
      "{'loss': 1.1456, 'learning_rate': 0.00012852941176470588, 'epoch': 11.12}\n",
      "{'loss': 1.236, 'learning_rate': 0.00012823529411764707, 'epoch': 11.12}\n",
      "{'loss': 1.284, 'learning_rate': 0.00012794117647058824, 'epoch': 11.13}\n",
      "{'loss': 1.3305, 'learning_rate': 0.0001276470588235294, 'epoch': 11.14}\n",
      "{'loss': 1.247, 'learning_rate': 0.0001273529411764706, 'epoch': 11.15}\n",
      "{'loss': 1.2465, 'learning_rate': 0.00012705882352941175, 'epoch': 11.16}\n",
      "{'loss': 1.2356, 'learning_rate': 0.00012676470588235294, 'epoch': 11.17}\n",
      "{'loss': 1.2205, 'learning_rate': 0.0001264705882352941, 'epoch': 11.18}\n",
      "{'loss': 1.171, 'learning_rate': 0.0001261764705882353, 'epoch': 11.19}\n",
      "{'loss': 1.2102, 'learning_rate': 0.0001258823529411765, 'epoch': 11.2}\n",
      "{'loss': 1.2066, 'learning_rate': 0.00012558823529411765, 'epoch': 11.21}\n",
      "{'loss': 1.2105, 'learning_rate': 0.00012529411764705884, 'epoch': 11.21}\n",
      "{'loss': 1.3009, 'learning_rate': 0.000125, 'epoch': 11.22}\n",
      "{'loss': 1.151, 'learning_rate': 0.0001247058823529412, 'epoch': 11.23}\n",
      "{'loss': 1.1377, 'learning_rate': 0.00012441176470588236, 'epoch': 11.24}\n",
      "{'loss': 1.3071, 'learning_rate': 0.00012411764705882355, 'epoch': 11.25}\n",
      "{'loss': 1.2353, 'learning_rate': 0.0001238235294117647, 'epoch': 11.26}\n",
      "{'loss': 1.1839, 'learning_rate': 0.0001235294117647059, 'epoch': 11.27}\n",
      "{'loss': 1.2025, 'learning_rate': 0.00012323529411764706, 'epoch': 11.28}\n",
      "{'loss': 1.1109, 'learning_rate': 0.00012294117647058825, 'epoch': 11.29}\n",
      "{'loss': 1.1902, 'learning_rate': 0.00012264705882352941, 'epoch': 11.29}\n",
      "{'loss': 1.2237, 'learning_rate': 0.0001223529411764706, 'epoch': 11.3}\n",
      "{'loss': 1.132, 'learning_rate': 0.00012205882352941178, 'epoch': 11.31}\n",
      "{'loss': 1.1869, 'learning_rate': 0.00012176470588235293, 'epoch': 11.32}\n",
      "{'loss': 1.1275, 'learning_rate': 0.00012147058823529412, 'epoch': 11.33}\n",
      "{'loss': 1.1908, 'learning_rate': 0.0001211764705882353, 'epoch': 11.34}\n",
      "{'loss': 1.2627, 'learning_rate': 0.00012088235294117647, 'epoch': 11.35}\n",
      "{'loss': 1.2316, 'learning_rate': 0.00012058823529411765, 'epoch': 11.36}\n",
      "{'loss': 1.1398, 'learning_rate': 0.00012029411764705883, 'epoch': 11.37}\n",
      "{'loss': 1.2013, 'learning_rate': 0.00012, 'epoch': 11.38}\n",
      "{'loss': 1.081, 'learning_rate': 0.00011970588235294118, 'epoch': 11.38}\n",
      "{'loss': 1.1298, 'learning_rate': 0.00011941176470588236, 'epoch': 11.39}\n",
      "{'loss': 1.2515, 'learning_rate': 0.00011911764705882353, 'epoch': 11.4}\n",
      "{'loss': 1.2428, 'learning_rate': 0.00011882352941176471, 'epoch': 11.41}\n",
      "{'loss': 1.143, 'learning_rate': 0.00011852941176470589, 'epoch': 11.42}\n",
      "{'loss': 1.1755, 'learning_rate': 0.00011823529411764706, 'epoch': 11.43}\n",
      "{'loss': 1.158, 'learning_rate': 0.00011794117647058824, 'epoch': 11.44}\n",
      "{'loss': 1.1121, 'learning_rate': 0.00011764705882352942, 'epoch': 11.45}\n",
      "{'loss': 1.2637, 'learning_rate': 0.0001173529411764706, 'epoch': 11.46}\n",
      "{'loss': 1.1204, 'learning_rate': 0.00011705882352941178, 'epoch': 11.46}\n",
      "{'loss': 1.3308, 'learning_rate': 0.00011676470588235296, 'epoch': 11.47}\n",
      "{'loss': 1.1315, 'learning_rate': 0.00011647058823529413, 'epoch': 11.48}\n",
      "{'loss': 1.1928, 'learning_rate': 0.00011617647058823531, 'epoch': 11.49}\n",
      "{'loss': 1.1045, 'learning_rate': 0.00011588235294117649, 'epoch': 11.5}\n",
      "{'loss': 1.3383, 'learning_rate': 0.00011558823529411764, 'epoch': 11.51}\n",
      "{'loss': 1.2293, 'learning_rate': 0.00011529411764705881, 'epoch': 11.52}\n",
      "{'loss': 1.1192, 'learning_rate': 0.00011499999999999999, 'epoch': 11.53}\n",
      "{'loss': 1.2938, 'learning_rate': 0.00011470588235294118, 'epoch': 11.54}\n",
      "{'loss': 1.2539, 'learning_rate': 0.00011441176470588236, 'epoch': 11.54}\n",
      "{'loss': 1.123, 'learning_rate': 0.00011411764705882353, 'epoch': 11.55}\n",
      "{'loss': 1.1001, 'learning_rate': 0.00011382352941176471, 'epoch': 11.56}\n",
      "{'loss': 1.1842, 'learning_rate': 0.00011352941176470589, 'epoch': 11.57}\n",
      "{'loss': 1.1301, 'learning_rate': 0.00011323529411764706, 'epoch': 11.58}\n",
      "{'loss': 1.2857, 'learning_rate': 0.00011294117647058824, 'epoch': 11.59}\n",
      "{'loss': 1.1775, 'learning_rate': 0.00011264705882352942, 'epoch': 11.6}\n",
      "{'loss': 1.1983, 'learning_rate': 0.00011235294117647059, 'epoch': 11.61}\n",
      "{'loss': 1.1323, 'learning_rate': 0.00011205882352941177, 'epoch': 11.62}\n",
      "{'loss': 1.2179, 'learning_rate': 0.00011176470588235294, 'epoch': 11.62}\n",
      "{'loss': 1.2374, 'learning_rate': 0.00011147058823529412, 'epoch': 11.63}\n",
      "{'loss': 1.2687, 'learning_rate': 0.0001111764705882353, 'epoch': 11.64}\n",
      "{'loss': 1.1453, 'learning_rate': 0.00011088235294117649, 'epoch': 11.65}\n",
      "{'loss': 1.1182, 'learning_rate': 0.00011058823529411766, 'epoch': 11.66}\n",
      "{'loss': 1.2652, 'learning_rate': 0.00011029411764705884, 'epoch': 11.67}\n",
      "{'loss': 1.1906, 'learning_rate': 0.00011000000000000002, 'epoch': 11.68}\n",
      "{'loss': 1.2948, 'learning_rate': 0.0001097058823529412, 'epoch': 11.69}\n",
      "{'loss': 1.1996, 'learning_rate': 0.00010941176470588237, 'epoch': 11.7}\n",
      "{'loss': 1.1395, 'learning_rate': 0.00010911764705882352, 'epoch': 11.71}\n",
      "{'loss': 1.1897, 'learning_rate': 0.0001088235294117647, 'epoch': 11.71}\n",
      "{'loss': 0.9627, 'learning_rate': 0.00010852941176470587, 'epoch': 11.72}\n",
      "{'loss': 1.1842, 'learning_rate': 0.00010823529411764706, 'epoch': 11.73}\n",
      "{'loss': 1.2, 'learning_rate': 0.00010794117647058824, 'epoch': 11.74}\n",
      "{'loss': 1.0985, 'learning_rate': 0.00010764705882352942, 'epoch': 11.75}\n",
      "{'loss': 1.1942, 'learning_rate': 0.00010735294117647059, 'epoch': 11.76}\n",
      "{'loss': 1.1562, 'learning_rate': 0.00010705882352941177, 'epoch': 11.77}\n",
      "{'loss': 1.1463, 'learning_rate': 0.00010676470588235295, 'epoch': 11.78}\n",
      "{'loss': 1.1689, 'learning_rate': 0.00010647058823529412, 'epoch': 11.79}\n",
      "{'loss': 1.2428, 'learning_rate': 0.0001061764705882353, 'epoch': 11.79}\n",
      "{'loss': 1.1809, 'learning_rate': 0.00010588235294117647, 'epoch': 11.8}\n",
      "{'loss': 1.2764, 'learning_rate': 0.00010558823529411765, 'epoch': 11.81}\n",
      "{'loss': 1.0082, 'learning_rate': 0.00010529411764705883, 'epoch': 11.82}\n",
      "{'loss': 1.2801, 'learning_rate': 0.000105, 'epoch': 11.83}\n",
      "{'loss': 1.0433, 'learning_rate': 0.00010470588235294118, 'epoch': 11.84}\n",
      "{'loss': 1.1777, 'learning_rate': 0.00010441176470588237, 'epoch': 11.85}\n",
      "{'loss': 1.2118, 'learning_rate': 0.00010411764705882355, 'epoch': 11.86}\n",
      "{'loss': 1.2297, 'learning_rate': 0.00010382352941176472, 'epoch': 11.87}\n",
      "{'loss': 1.1485, 'learning_rate': 0.0001035294117647059, 'epoch': 11.88}\n",
      "{'loss': 1.1471, 'learning_rate': 0.00010323529411764708, 'epoch': 11.88}\n",
      "{'loss': 1.1982, 'learning_rate': 0.00010294117647058823, 'epoch': 11.89}\n",
      "{'loss': 1.0699, 'learning_rate': 0.0001026470588235294, 'epoch': 11.9}\n",
      "{'loss': 1.0721, 'learning_rate': 0.00010235294117647058, 'epoch': 11.91}\n",
      "{'loss': 1.0459, 'learning_rate': 0.00010205882352941176, 'epoch': 11.92}\n",
      "{'loss': 1.1619, 'learning_rate': 0.00010176470588235295, 'epoch': 11.93}\n",
      "{'loss': 1.1981, 'learning_rate': 0.00010147058823529412, 'epoch': 11.94}\n",
      "{'loss': 1.1606, 'learning_rate': 0.0001011764705882353, 'epoch': 11.95}\n",
      "{'loss': 1.258, 'learning_rate': 0.00010088235294117648, 'epoch': 11.96}\n",
      "{'loss': 1.2279, 'learning_rate': 0.00010058823529411765, 'epoch': 11.96}\n",
      "{'loss': 1.2669, 'learning_rate': 0.00010029411764705883, 'epoch': 11.97}\n",
      "{'loss': 1.1314, 'learning_rate': 0.0001, 'epoch': 11.98}\n",
      "{'loss': 1.2826, 'learning_rate': 9.970588235294118e-05, 'epoch': 11.99}\n",
      "{'loss': 1.2407, 'learning_rate': 9.941176470588236e-05, 'epoch': 12.0}\n",
      "{'loss': 1.1541, 'learning_rate': 9.911764705882353e-05, 'epoch': 12.01}\n",
      "{'loss': 1.1992, 'learning_rate': 9.882352941176471e-05, 'epoch': 12.02}\n",
      "{'loss': 1.1163, 'learning_rate': 9.852941176470589e-05, 'epoch': 12.03}\n",
      "{'loss': 1.2148, 'learning_rate': 9.823529411764706e-05, 'epoch': 12.04}\n",
      "{'loss': 1.2401, 'learning_rate': 9.794117647058824e-05, 'epoch': 12.04}\n",
      "{'loss': 1.167, 'learning_rate': 9.764705882352942e-05, 'epoch': 12.05}\n",
      "{'loss': 1.1057, 'learning_rate': 9.73529411764706e-05, 'epoch': 12.06}\n",
      "{'loss': 1.0819, 'learning_rate': 9.705882352941177e-05, 'epoch': 12.07}\n",
      "{'loss': 1.1472, 'learning_rate': 9.676470588235295e-05, 'epoch': 12.08}\n",
      "{'loss': 1.3537, 'learning_rate': 9.647058823529412e-05, 'epoch': 12.09}\n",
      "{'loss': 1.1442, 'learning_rate': 9.61764705882353e-05, 'epoch': 12.1}\n",
      "{'loss': 1.2276, 'learning_rate': 9.588235294117648e-05, 'epoch': 12.11}\n",
      "{'loss': 1.3804, 'learning_rate': 9.558823529411765e-05, 'epoch': 12.12}\n",
      "{'loss': 1.0773, 'learning_rate': 9.529411764705883e-05, 'epoch': 12.12}\n",
      "{'loss': 1.0933, 'learning_rate': 9.5e-05, 'epoch': 12.13}\n",
      "{'loss': 1.1202, 'learning_rate': 9.470588235294118e-05, 'epoch': 12.14}\n",
      "{'loss': 1.0575, 'learning_rate': 9.441176470588236e-05, 'epoch': 12.15}\n",
      "{'loss': 1.1485, 'learning_rate': 9.411764705882353e-05, 'epoch': 12.16}\n",
      "{'loss': 1.1266, 'learning_rate': 9.382352941176471e-05, 'epoch': 12.17}\n",
      "{'loss': 1.0791, 'learning_rate': 9.352941176470589e-05, 'epoch': 12.18}\n",
      "{'loss': 1.2731, 'learning_rate': 9.323529411764706e-05, 'epoch': 12.19}\n",
      "{'loss': 1.13, 'learning_rate': 9.294117647058824e-05, 'epoch': 12.2}\n",
      "{'loss': 1.3024, 'learning_rate': 9.264705882352942e-05, 'epoch': 12.21}\n",
      "{'loss': 1.1534, 'learning_rate': 9.23529411764706e-05, 'epoch': 12.21}\n",
      "{'loss': 1.1784, 'learning_rate': 9.205882352941177e-05, 'epoch': 12.22}\n",
      "{'loss': 1.291, 'learning_rate': 9.176470588235295e-05, 'epoch': 12.23}\n",
      "{'loss': 1.2291, 'learning_rate': 9.147058823529412e-05, 'epoch': 12.24}\n",
      "{'loss': 1.2016, 'learning_rate': 9.11764705882353e-05, 'epoch': 12.25}\n",
      "{'loss': 1.1765, 'learning_rate': 9.088235294117648e-05, 'epoch': 12.26}\n",
      "{'loss': 1.0864, 'learning_rate': 9.058823529411765e-05, 'epoch': 12.27}\n",
      "{'loss': 1.2272, 'learning_rate': 9.029411764705883e-05, 'epoch': 12.28}\n",
      "{'loss': 1.053, 'learning_rate': 9e-05, 'epoch': 12.29}\n",
      "{'loss': 1.2138, 'learning_rate': 8.970588235294118e-05, 'epoch': 12.29}\n",
      "{'loss': 1.0761, 'learning_rate': 8.941176470588236e-05, 'epoch': 12.3}\n",
      "{'loss': 1.3255, 'learning_rate': 8.911764705882354e-05, 'epoch': 12.31}\n",
      "{'loss': 1.2498, 'learning_rate': 8.882352941176471e-05, 'epoch': 12.32}\n",
      "{'loss': 1.108, 'learning_rate': 8.852941176470589e-05, 'epoch': 12.33}\n",
      "{'loss': 1.1405, 'learning_rate': 8.823529411764706e-05, 'epoch': 12.34}\n",
      "{'loss': 0.9958, 'learning_rate': 8.794117647058824e-05, 'epoch': 12.35}\n",
      "{'loss': 1.1301, 'learning_rate': 8.764705882352942e-05, 'epoch': 12.36}\n",
      "{'loss': 1.3221, 'learning_rate': 8.73529411764706e-05, 'epoch': 12.37}\n",
      "{'loss': 1.0701, 'learning_rate': 8.705882352941177e-05, 'epoch': 12.38}\n",
      "{'loss': 0.9355, 'learning_rate': 8.676470588235295e-05, 'epoch': 12.38}\n",
      "{'loss': 1.1317, 'learning_rate': 8.647058823529412e-05, 'epoch': 12.39}\n",
      "{'loss': 1.0741, 'learning_rate': 8.61764705882353e-05, 'epoch': 12.4}\n",
      "{'loss': 1.1206, 'learning_rate': 8.588235294117646e-05, 'epoch': 12.41}\n",
      "{'loss': 1.1699, 'learning_rate': 8.558823529411765e-05, 'epoch': 12.42}\n",
      "{'loss': 1.3319, 'learning_rate': 8.529411764705883e-05, 'epoch': 12.43}\n",
      "{'loss': 1.1728, 'learning_rate': 8.5e-05, 'epoch': 12.44}\n",
      "{'loss': 1.1875, 'learning_rate': 8.470588235294118e-05, 'epoch': 12.45}\n",
      "{'loss': 1.2121, 'learning_rate': 8.441176470588236e-05, 'epoch': 12.46}\n",
      "{'loss': 1.175, 'learning_rate': 8.411764705882354e-05, 'epoch': 12.46}\n",
      "{'loss': 1.1817, 'learning_rate': 8.382352941176471e-05, 'epoch': 12.47}\n",
      "{'loss': 1.093, 'learning_rate': 8.352941176470589e-05, 'epoch': 12.48}\n",
      "{'loss': 1.1333, 'learning_rate': 8.323529411764707e-05, 'epoch': 12.49}\n",
      "{'loss': 1.2413, 'learning_rate': 8.294117647058824e-05, 'epoch': 12.5}\n",
      "{'loss': 1.1446, 'learning_rate': 8.26470588235294e-05, 'epoch': 12.51}\n",
      "{'loss': 1.2277, 'learning_rate': 8.23529411764706e-05, 'epoch': 12.52}\n",
      "{'loss': 1.2862, 'learning_rate': 8.205882352941177e-05, 'epoch': 12.53}\n",
      "{'loss': 1.0275, 'learning_rate': 8.176470588235295e-05, 'epoch': 12.54}\n",
      "{'loss': 0.979, 'learning_rate': 8.147058823529412e-05, 'epoch': 12.54}\n",
      "{'loss': 1.1997, 'learning_rate': 8.11764705882353e-05, 'epoch': 12.55}\n",
      "{'loss': 1.2096, 'learning_rate': 8.088235294117648e-05, 'epoch': 12.56}\n",
      "{'loss': 1.062, 'learning_rate': 8.058823529411765e-05, 'epoch': 12.57}\n",
      "{'loss': 1.108, 'learning_rate': 8.029411764705883e-05, 'epoch': 12.58}\n",
      "{'loss': 1.1622, 'learning_rate': 8e-05, 'epoch': 12.59}\n",
      "{'loss': 1.121, 'learning_rate': 7.970588235294118e-05, 'epoch': 12.6}\n",
      "{'loss': 1.1578, 'learning_rate': 7.941176470588235e-05, 'epoch': 12.61}\n",
      "{'loss': 1.0887, 'learning_rate': 7.911764705882354e-05, 'epoch': 12.62}\n",
      "{'loss': 1.1318, 'learning_rate': 7.882352941176471e-05, 'epoch': 12.62}\n",
      "{'loss': 1.1027, 'learning_rate': 7.852941176470589e-05, 'epoch': 12.63}\n",
      "{'loss': 1.0757, 'learning_rate': 7.823529411764707e-05, 'epoch': 12.64}\n",
      "{'loss': 1.1566, 'learning_rate': 7.794117647058824e-05, 'epoch': 12.65}\n",
      "{'loss': 1.033, 'learning_rate': 7.764705882352942e-05, 'epoch': 12.66}\n",
      "{'loss': 1.1548, 'learning_rate': 7.73529411764706e-05, 'epoch': 12.67}\n",
      "{'loss': 1.1106, 'learning_rate': 7.705882352941177e-05, 'epoch': 12.68}\n",
      "{'loss': 1.1197, 'learning_rate': 7.676470588235295e-05, 'epoch': 12.69}\n",
      "{'loss': 1.0484, 'learning_rate': 7.647058823529411e-05, 'epoch': 12.7}\n",
      "{'loss': 1.2174, 'learning_rate': 7.617647058823529e-05, 'epoch': 12.71}\n",
      "{'loss': 1.2261, 'learning_rate': 7.588235294117648e-05, 'epoch': 12.71}\n",
      "{'loss': 1.1612, 'learning_rate': 7.558823529411765e-05, 'epoch': 12.72}\n",
      "{'loss': 1.1328, 'learning_rate': 7.529411764705883e-05, 'epoch': 12.73}\n",
      "{'loss': 1.1426, 'learning_rate': 7.500000000000001e-05, 'epoch': 12.74}\n",
      "{'loss': 1.1591, 'learning_rate': 7.470588235294118e-05, 'epoch': 12.75}\n",
      "{'loss': 1.2382, 'learning_rate': 7.441176470588236e-05, 'epoch': 12.76}\n",
      "{'loss': 1.112, 'learning_rate': 7.411764705882354e-05, 'epoch': 12.77}\n",
      "{'loss': 1.1384, 'learning_rate': 7.382352941176471e-05, 'epoch': 12.78}\n",
      "{'loss': 1.1433, 'learning_rate': 7.352941176470589e-05, 'epoch': 12.79}\n",
      "{'loss': 1.1595, 'learning_rate': 7.323529411764705e-05, 'epoch': 12.79}\n",
      "{'loss': 1.1355, 'learning_rate': 7.294117647058823e-05, 'epoch': 12.8}\n",
      "{'loss': 0.956, 'learning_rate': 7.264705882352942e-05, 'epoch': 12.81}\n",
      "{'loss': 1.1391, 'learning_rate': 7.23529411764706e-05, 'epoch': 12.82}\n",
      "{'loss': 1.0883, 'learning_rate': 7.205882352941177e-05, 'epoch': 12.83}\n",
      "{'loss': 1.1484, 'learning_rate': 7.176470588235295e-05, 'epoch': 12.84}\n",
      "{'loss': 1.1172, 'learning_rate': 7.147058823529412e-05, 'epoch': 12.85}\n",
      "{'loss': 1.2271, 'learning_rate': 7.11764705882353e-05, 'epoch': 12.86}\n",
      "{'loss': 1.1581, 'learning_rate': 7.088235294117648e-05, 'epoch': 12.87}\n",
      "{'loss': 1.2053, 'learning_rate': 7.058823529411765e-05, 'epoch': 12.88}\n",
      "{'loss': 1.2778, 'learning_rate': 7.029411764705882e-05, 'epoch': 12.88}\n",
      "{'loss': 1.2173, 'learning_rate': 7e-05, 'epoch': 12.89}\n",
      "{'loss': 1.1842, 'learning_rate': 6.970588235294117e-05, 'epoch': 12.9}\n",
      "{'loss': 1.1388, 'learning_rate': 6.941176470588236e-05, 'epoch': 12.91}\n",
      "{'loss': 1.0565, 'learning_rate': 6.911764705882354e-05, 'epoch': 12.92}\n",
      "{'loss': 1.172, 'learning_rate': 6.882352941176471e-05, 'epoch': 12.93}\n",
      "{'loss': 1.1085, 'learning_rate': 6.852941176470589e-05, 'epoch': 12.94}\n",
      "{'loss': 1.0666, 'learning_rate': 6.823529411764707e-05, 'epoch': 12.95}\n",
      "{'loss': 1.1838, 'learning_rate': 6.794117647058824e-05, 'epoch': 12.96}\n",
      "{'loss': 1.1201, 'learning_rate': 6.764705882352942e-05, 'epoch': 12.96}\n",
      "{'loss': 1.2012, 'learning_rate': 6.73529411764706e-05, 'epoch': 12.97}\n",
      "{'loss': 1.148, 'learning_rate': 6.705882352941176e-05, 'epoch': 12.98}\n",
      "{'loss': 1.035, 'learning_rate': 6.676470588235294e-05, 'epoch': 12.99}\n",
      "{'loss': 1.1106, 'learning_rate': 6.647058823529411e-05, 'epoch': 13.0}\n",
      "{'loss': 0.9796, 'learning_rate': 6.61764705882353e-05, 'epoch': 13.01}\n",
      "{'loss': 1.071, 'learning_rate': 6.588235294117648e-05, 'epoch': 13.02}\n",
      "{'loss': 1.0858, 'learning_rate': 6.558823529411765e-05, 'epoch': 13.03}\n",
      "{'loss': 1.272, 'learning_rate': 6.529411764705883e-05, 'epoch': 13.04}\n",
      "{'loss': 1.2244, 'learning_rate': 6.500000000000001e-05, 'epoch': 13.04}\n",
      "{'loss': 1.2818, 'learning_rate': 6.470588235294118e-05, 'epoch': 13.05}\n",
      "{'loss': 1.135, 'learning_rate': 6.441176470588236e-05, 'epoch': 13.06}\n",
      "{'loss': 1.0802, 'learning_rate': 6.411764705882354e-05, 'epoch': 13.07}\n",
      "{'loss': 1.2358, 'learning_rate': 6.38235294117647e-05, 'epoch': 13.08}\n",
      "{'loss': 1.0778, 'learning_rate': 6.352941176470588e-05, 'epoch': 13.09}\n",
      "{'loss': 1.1503, 'learning_rate': 6.323529411764705e-05, 'epoch': 13.1}\n",
      "{'loss': 1.0817, 'learning_rate': 6.294117647058824e-05, 'epoch': 13.11}\n",
      "{'loss': 1.0873, 'learning_rate': 6.264705882352942e-05, 'epoch': 13.12}\n",
      "{'loss': 1.1202, 'learning_rate': 6.23529411764706e-05, 'epoch': 13.12}\n",
      "{'loss': 1.0838, 'learning_rate': 6.205882352941177e-05, 'epoch': 13.13}\n",
      "{'loss': 1.0966, 'learning_rate': 6.176470588235295e-05, 'epoch': 13.14}\n",
      "{'loss': 1.186, 'learning_rate': 6.147058823529413e-05, 'epoch': 13.15}\n",
      "{'loss': 1.1384, 'learning_rate': 6.11764705882353e-05, 'epoch': 13.16}\n",
      "{'loss': 1.1278, 'learning_rate': 6.0882352941176465e-05, 'epoch': 13.17}\n",
      "{'loss': 1.2813, 'learning_rate': 6.058823529411765e-05, 'epoch': 13.18}\n",
      "{'loss': 1.1917, 'learning_rate': 6.0294117647058825e-05, 'epoch': 13.19}\n",
      "{'loss': 1.1518, 'learning_rate': 6e-05, 'epoch': 13.2}\n",
      "{'loss': 1.2108, 'learning_rate': 5.970588235294118e-05, 'epoch': 13.21}\n",
      "{'loss': 1.2163, 'learning_rate': 5.9411764705882355e-05, 'epoch': 13.21}\n",
      "{'loss': 1.0641, 'learning_rate': 5.911764705882353e-05, 'epoch': 13.22}\n",
      "{'loss': 1.1896, 'learning_rate': 5.882352941176471e-05, 'epoch': 13.23}\n",
      "{'loss': 1.0695, 'learning_rate': 5.852941176470589e-05, 'epoch': 13.24}\n",
      "{'loss': 1.1142, 'learning_rate': 5.823529411764707e-05, 'epoch': 13.25}\n",
      "{'loss': 1.1705, 'learning_rate': 5.7941176470588244e-05, 'epoch': 13.26}\n",
      "{'loss': 1.1002, 'learning_rate': 5.764705882352941e-05, 'epoch': 13.27}\n",
      "{'loss': 1.099, 'learning_rate': 5.735294117647059e-05, 'epoch': 13.28}\n",
      "{'loss': 1.1219, 'learning_rate': 5.7058823529411766e-05, 'epoch': 13.29}\n",
      "{'loss': 1.1275, 'learning_rate': 5.676470588235294e-05, 'epoch': 13.29}\n",
      "{'loss': 0.9946, 'learning_rate': 5.647058823529412e-05, 'epoch': 13.3}\n",
      "{'loss': 1.0956, 'learning_rate': 5.6176470588235296e-05, 'epoch': 13.31}\n",
      "{'loss': 1.0007, 'learning_rate': 5.588235294117647e-05, 'epoch': 13.32}\n",
      "{'loss': 1.155, 'learning_rate': 5.558823529411765e-05, 'epoch': 13.33}\n",
      "{'loss': 1.1413, 'learning_rate': 5.529411764705883e-05, 'epoch': 13.34}\n",
      "{'loss': 1.1192, 'learning_rate': 5.500000000000001e-05, 'epoch': 13.35}\n",
      "{'loss': 1.0909, 'learning_rate': 5.4705882352941185e-05, 'epoch': 13.36}\n",
      "{'loss': 1.0759, 'learning_rate': 5.441176470588235e-05, 'epoch': 13.37}\n",
      "{'loss': 1.1431, 'learning_rate': 5.411764705882353e-05, 'epoch': 13.38}\n",
      "{'loss': 1.1324, 'learning_rate': 5.382352941176471e-05, 'epoch': 13.38}\n",
      "{'loss': 1.0859, 'learning_rate': 5.3529411764705884e-05, 'epoch': 13.39}\n",
      "{'loss': 1.096, 'learning_rate': 5.323529411764706e-05, 'epoch': 13.4}\n",
      "{'loss': 1.0828, 'learning_rate': 5.294117647058824e-05, 'epoch': 13.41}\n",
      "{'loss': 1.0173, 'learning_rate': 5.2647058823529414e-05, 'epoch': 13.42}\n",
      "{'loss': 0.9244, 'learning_rate': 5.235294117647059e-05, 'epoch': 13.43}\n",
      "{'loss': 1.0833, 'learning_rate': 5.2058823529411774e-05, 'epoch': 13.44}\n",
      "{'loss': 1.1552, 'learning_rate': 5.176470588235295e-05, 'epoch': 13.45}\n",
      "{'loss': 1.1565, 'learning_rate': 5.147058823529411e-05, 'epoch': 13.46}\n",
      "{'loss': 1.1884, 'learning_rate': 5.117647058823529e-05, 'epoch': 13.46}\n",
      "{'loss': 1.1089, 'learning_rate': 5.088235294117647e-05, 'epoch': 13.47}\n",
      "{'loss': 1.2178, 'learning_rate': 5.058823529411765e-05, 'epoch': 13.48}\n",
      "{'loss': 1.1068, 'learning_rate': 5.0294117647058826e-05, 'epoch': 13.49}\n",
      "{'loss': 1.1119, 'learning_rate': 5e-05, 'epoch': 13.5}\n",
      "{'loss': 1.2482, 'learning_rate': 4.970588235294118e-05, 'epoch': 13.51}\n",
      "{'loss': 1.2184, 'learning_rate': 4.9411764705882355e-05, 'epoch': 13.52}\n",
      "{'loss': 1.1984, 'learning_rate': 4.911764705882353e-05, 'epoch': 13.53}\n",
      "{'loss': 1.0499, 'learning_rate': 4.882352941176471e-05, 'epoch': 13.54}\n",
      "{'loss': 1.0681, 'learning_rate': 4.8529411764705885e-05, 'epoch': 13.54}\n",
      "{'loss': 1.0675, 'learning_rate': 4.823529411764706e-05, 'epoch': 13.55}\n",
      "{'loss': 1.0094, 'learning_rate': 4.794117647058824e-05, 'epoch': 13.56}\n",
      "{'loss': 1.1432, 'learning_rate': 4.7647058823529414e-05, 'epoch': 13.57}\n",
      "{'loss': 1.0774, 'learning_rate': 4.735294117647059e-05, 'epoch': 13.58}\n",
      "{'loss': 1.2021, 'learning_rate': 4.705882352941177e-05, 'epoch': 13.59}\n",
      "{'loss': 1.176, 'learning_rate': 4.6764705882352944e-05, 'epoch': 13.6}\n",
      "{'loss': 1.2654, 'learning_rate': 4.647058823529412e-05, 'epoch': 13.61}\n",
      "{'loss': 1.2233, 'learning_rate': 4.61764705882353e-05, 'epoch': 13.62}\n",
      "{'loss': 1.0638, 'learning_rate': 4.588235294117647e-05, 'epoch': 13.62}\n",
      "{'loss': 1.0369, 'learning_rate': 4.558823529411765e-05, 'epoch': 13.63}\n",
      "{'loss': 1.1148, 'learning_rate': 4.5294117647058826e-05, 'epoch': 13.64}\n",
      "{'loss': 0.9506, 'learning_rate': 4.5e-05, 'epoch': 13.65}\n",
      "{'loss': 1.0269, 'learning_rate': 4.470588235294118e-05, 'epoch': 13.66}\n",
      "{'loss': 1.0632, 'learning_rate': 4.4411764705882356e-05, 'epoch': 13.67}\n",
      "{'loss': 1.0967, 'learning_rate': 4.411764705882353e-05, 'epoch': 13.68}\n",
      "{'loss': 1.104, 'learning_rate': 4.382352941176471e-05, 'epoch': 13.69}\n",
      "{'loss': 1.22, 'learning_rate': 4.3529411764705885e-05, 'epoch': 13.7}\n",
      "{'loss': 1.2614, 'learning_rate': 4.323529411764706e-05, 'epoch': 13.71}\n",
      "{'loss': 1.2101, 'learning_rate': 4.294117647058823e-05, 'epoch': 13.71}\n",
      "{'loss': 1.0889, 'learning_rate': 4.2647058823529415e-05, 'epoch': 13.72}\n",
      "{'loss': 0.9886, 'learning_rate': 4.235294117647059e-05, 'epoch': 13.73}\n",
      "{'loss': 1.143, 'learning_rate': 4.205882352941177e-05, 'epoch': 13.74}\n",
      "{'loss': 1.1729, 'learning_rate': 4.1764705882352944e-05, 'epoch': 13.75}\n",
      "{'loss': 1.1843, 'learning_rate': 4.147058823529412e-05, 'epoch': 13.76}\n",
      "{'loss': 1.1174, 'learning_rate': 4.11764705882353e-05, 'epoch': 13.77}\n",
      "{'loss': 1.1472, 'learning_rate': 4.0882352941176474e-05, 'epoch': 13.78}\n",
      "{'loss': 1.0835, 'learning_rate': 4.058823529411765e-05, 'epoch': 13.79}\n",
      "{'loss': 1.2151, 'learning_rate': 4.029411764705883e-05, 'epoch': 13.79}\n",
      "{'loss': 1.0355, 'learning_rate': 4e-05, 'epoch': 13.8}\n",
      "{'loss': 1.1396, 'learning_rate': 3.970588235294117e-05, 'epoch': 13.81}\n",
      "{'loss': 1.1679, 'learning_rate': 3.9411764705882356e-05, 'epoch': 13.82}\n",
      "{'loss': 1.019, 'learning_rate': 3.911764705882353e-05, 'epoch': 13.83}\n",
      "{'loss': 1.0904, 'learning_rate': 3.882352941176471e-05, 'epoch': 13.84}\n",
      "{'loss': 1.1383, 'learning_rate': 3.8529411764705886e-05, 'epoch': 13.85}\n",
      "{'loss': 1.1408, 'learning_rate': 3.8235294117647055e-05, 'epoch': 13.86}\n",
      "{'loss': 1.1187, 'learning_rate': 3.794117647058824e-05, 'epoch': 13.87}\n",
      "{'loss': 1.1589, 'learning_rate': 3.7647058823529415e-05, 'epoch': 13.88}\n",
      "{'loss': 1.0287, 'learning_rate': 3.735294117647059e-05, 'epoch': 13.88}\n",
      "{'loss': 1.2349, 'learning_rate': 3.705882352941177e-05, 'epoch': 13.89}\n",
      "{'loss': 1.174, 'learning_rate': 3.6764705882352945e-05, 'epoch': 13.9}\n",
      "{'loss': 1.2429, 'learning_rate': 3.6470588235294114e-05, 'epoch': 13.91}\n",
      "{'loss': 1.146, 'learning_rate': 3.61764705882353e-05, 'epoch': 13.92}\n",
      "{'loss': 1.1517, 'learning_rate': 3.5882352941176474e-05, 'epoch': 13.93}\n",
      "{'loss': 1.0411, 'learning_rate': 3.558823529411765e-05, 'epoch': 13.94}\n",
      "{'loss': 1.0678, 'learning_rate': 3.529411764705883e-05, 'epoch': 13.95}\n",
      "{'loss': 1.1342, 'learning_rate': 3.5e-05, 'epoch': 13.96}\n",
      "{'loss': 1.0788, 'learning_rate': 3.470588235294118e-05, 'epoch': 13.96}\n",
      "{'loss': 1.2321, 'learning_rate': 3.441176470588236e-05, 'epoch': 13.97}\n",
      "{'loss': 1.1775, 'learning_rate': 3.411764705882353e-05, 'epoch': 13.98}\n",
      "{'loss': 1.1672, 'learning_rate': 3.382352941176471e-05, 'epoch': 13.99}\n",
      "{'loss': 1.1775, 'learning_rate': 3.352941176470588e-05, 'epoch': 14.0}\n",
      "{'loss': 1.1774, 'learning_rate': 3.3235294117647056e-05, 'epoch': 14.01}\n",
      "{'loss': 1.1106, 'learning_rate': 3.294117647058824e-05, 'epoch': 14.02}\n",
      "{'loss': 1.217, 'learning_rate': 3.2647058823529416e-05, 'epoch': 14.03}\n",
      "{'loss': 0.9959, 'learning_rate': 3.235294117647059e-05, 'epoch': 14.04}\n",
      "{'loss': 1.01, 'learning_rate': 3.205882352941177e-05, 'epoch': 14.04}\n",
      "{'loss': 1.2074, 'learning_rate': 3.176470588235294e-05, 'epoch': 14.05}\n",
      "{'loss': 1.1306, 'learning_rate': 3.147058823529412e-05, 'epoch': 14.06}\n",
      "{'loss': 1.0843, 'learning_rate': 3.11764705882353e-05, 'epoch': 14.07}\n",
      "{'loss': 1.0722, 'learning_rate': 3.0882352941176475e-05, 'epoch': 14.08}\n",
      "{'loss': 1.1386, 'learning_rate': 3.058823529411765e-05, 'epoch': 14.09}\n",
      "{'loss': 1.1358, 'learning_rate': 3.0294117647058824e-05, 'epoch': 14.1}\n",
      "{'loss': 1.0675, 'learning_rate': 3e-05, 'epoch': 14.11}\n",
      "{'loss': 1.1599, 'learning_rate': 2.9705882352941177e-05, 'epoch': 14.12}\n",
      "{'loss': 1.0712, 'learning_rate': 2.9411764705882354e-05, 'epoch': 14.12}\n",
      "{'loss': 1.1779, 'learning_rate': 2.9117647058823534e-05, 'epoch': 14.13}\n",
      "{'loss': 1.0848, 'learning_rate': 2.8823529411764703e-05, 'epoch': 14.14}\n",
      "{'loss': 1.1604, 'learning_rate': 2.8529411764705883e-05, 'epoch': 14.15}\n",
      "{'loss': 1.2462, 'learning_rate': 2.823529411764706e-05, 'epoch': 14.16}\n",
      "{'loss': 0.9924, 'learning_rate': 2.7941176470588236e-05, 'epoch': 14.17}\n",
      "{'loss': 1.2972, 'learning_rate': 2.7647058823529416e-05, 'epoch': 14.18}\n",
      "{'loss': 1.13, 'learning_rate': 2.7352941176470593e-05, 'epoch': 14.19}\n",
      "{'loss': 1.1079, 'learning_rate': 2.7058823529411766e-05, 'epoch': 14.2}\n",
      "{'loss': 1.2066, 'learning_rate': 2.6764705882352942e-05, 'epoch': 14.21}\n",
      "{'loss': 1.1476, 'learning_rate': 2.647058823529412e-05, 'epoch': 14.21}\n",
      "{'loss': 1.162, 'learning_rate': 2.6176470588235295e-05, 'epoch': 14.22}\n",
      "{'loss': 1.1117, 'learning_rate': 2.5882352941176475e-05, 'epoch': 14.23}\n",
      "{'loss': 1.1351, 'learning_rate': 2.5588235294117645e-05, 'epoch': 14.24}\n",
      "{'loss': 1.1991, 'learning_rate': 2.5294117647058825e-05, 'epoch': 14.25}\n",
      "{'loss': 1.0386, 'learning_rate': 2.5e-05, 'epoch': 14.26}\n",
      "{'loss': 1.1054, 'learning_rate': 2.4705882352941178e-05, 'epoch': 14.27}\n",
      "{'loss': 0.9569, 'learning_rate': 2.4411764705882354e-05, 'epoch': 14.28}\n",
      "{'loss': 1.084, 'learning_rate': 2.411764705882353e-05, 'epoch': 14.29}\n",
      "{'loss': 1.212, 'learning_rate': 2.3823529411764707e-05, 'epoch': 14.29}\n",
      "{'loss': 1.2027, 'learning_rate': 2.3529411764705884e-05, 'epoch': 14.3}\n",
      "{'loss': 1.2459, 'learning_rate': 2.323529411764706e-05, 'epoch': 14.31}\n",
      "{'loss': 1.068, 'learning_rate': 2.2941176470588237e-05, 'epoch': 14.32}\n",
      "{'loss': 1.1299, 'learning_rate': 2.2647058823529413e-05, 'epoch': 14.33}\n",
      "{'loss': 1.2202, 'learning_rate': 2.235294117647059e-05, 'epoch': 14.34}\n",
      "{'loss': 1.1772, 'learning_rate': 2.2058823529411766e-05, 'epoch': 14.35}\n",
      "{'loss': 1.09, 'learning_rate': 2.1764705882352943e-05, 'epoch': 14.36}\n",
      "{'loss': 1.0554, 'learning_rate': 2.1470588235294116e-05, 'epoch': 14.37}\n",
      "{'loss': 1.2202, 'learning_rate': 2.1176470588235296e-05, 'epoch': 14.38}\n",
      "{'loss': 1.1429, 'learning_rate': 2.0882352941176472e-05, 'epoch': 14.38}\n",
      "{'loss': 1.1037, 'learning_rate': 2.058823529411765e-05, 'epoch': 14.39}\n",
      "{'loss': 1.0006, 'learning_rate': 2.0294117647058825e-05, 'epoch': 14.4}\n",
      "{'loss': 1.0373, 'learning_rate': 2e-05, 'epoch': 14.41}\n",
      "{'loss': 1.0838, 'learning_rate': 1.9705882352941178e-05, 'epoch': 14.42}\n",
      "{'loss': 1.1082, 'learning_rate': 1.9411764705882355e-05, 'epoch': 14.43}\n",
      "{'loss': 1.3161, 'learning_rate': 1.9117647058823528e-05, 'epoch': 14.44}\n",
      "{'loss': 1.1004, 'learning_rate': 1.8823529411764708e-05, 'epoch': 14.45}\n",
      "{'loss': 1.0438, 'learning_rate': 1.8529411764705884e-05, 'epoch': 14.46}\n",
      "{'loss': 1.154, 'learning_rate': 1.8235294117647057e-05, 'epoch': 14.46}\n",
      "{'loss': 1.1953, 'learning_rate': 1.7941176470588237e-05, 'epoch': 14.47}\n",
      "{'loss': 0.999, 'learning_rate': 1.7647058823529414e-05, 'epoch': 14.48}\n",
      "{'loss': 1.1415, 'learning_rate': 1.735294117647059e-05, 'epoch': 14.49}\n",
      "{'loss': 1.0923, 'learning_rate': 1.7058823529411767e-05, 'epoch': 14.5}\n",
      "{'loss': 1.0525, 'learning_rate': 1.676470588235294e-05, 'epoch': 14.51}\n",
      "{'loss': 1.1824, 'learning_rate': 1.647058823529412e-05, 'epoch': 14.52}\n",
      "{'loss': 1.094, 'learning_rate': 1.6176470588235296e-05, 'epoch': 14.53}\n",
      "{'loss': 1.1461, 'learning_rate': 1.588235294117647e-05, 'epoch': 14.54}\n",
      "{'loss': 1.1174, 'learning_rate': 1.558823529411765e-05, 'epoch': 14.54}\n",
      "{'loss': 1.1661, 'learning_rate': 1.5294117647058826e-05, 'epoch': 14.55}\n",
      "{'loss': 1.1334, 'learning_rate': 1.5e-05, 'epoch': 14.56}\n",
      "{'loss': 1.178, 'learning_rate': 1.4705882352941177e-05, 'epoch': 14.57}\n",
      "{'loss': 1.1747, 'learning_rate': 1.4411764705882352e-05, 'epoch': 14.58}\n",
      "{'loss': 1.0674, 'learning_rate': 1.411764705882353e-05, 'epoch': 14.59}\n",
      "{'loss': 1.2547, 'learning_rate': 1.3823529411764708e-05, 'epoch': 14.6}\n",
      "{'loss': 1.0161, 'learning_rate': 1.3529411764705883e-05, 'epoch': 14.61}\n",
      "{'loss': 1.1969, 'learning_rate': 1.323529411764706e-05, 'epoch': 14.62}\n",
      "{'loss': 0.9157, 'learning_rate': 1.2941176470588238e-05, 'epoch': 14.62}\n",
      "{'loss': 1.0582, 'learning_rate': 1.2647058823529412e-05, 'epoch': 14.63}\n",
      "{'loss': 1.2264, 'learning_rate': 1.2352941176470589e-05, 'epoch': 14.64}\n",
      "{'loss': 1.0346, 'learning_rate': 1.2058823529411765e-05, 'epoch': 14.65}\n",
      "{'loss': 1.0494, 'learning_rate': 1.1764705882352942e-05, 'epoch': 14.66}\n",
      "{'loss': 1.0139, 'learning_rate': 1.1470588235294118e-05, 'epoch': 14.67}\n",
      "{'loss': 0.9856, 'learning_rate': 1.1176470588235295e-05, 'epoch': 14.68}\n",
      "{'loss': 1.1893, 'learning_rate': 1.0882352941176471e-05, 'epoch': 14.69}\n",
      "{'loss': 1.1341, 'learning_rate': 1.0588235294117648e-05, 'epoch': 14.7}\n",
      "{'loss': 1.1604, 'learning_rate': 1.0294117647058824e-05, 'epoch': 14.71}\n",
      "{'loss': 1.0633, 'learning_rate': 1e-05, 'epoch': 14.71}\n",
      "{'loss': 1.2479, 'learning_rate': 9.705882352941177e-06, 'epoch': 14.72}\n",
      "{'loss': 1.0841, 'learning_rate': 9.411764705882354e-06, 'epoch': 14.73}\n",
      "{'loss': 1.1292, 'learning_rate': 9.117647058823529e-06, 'epoch': 14.74}\n",
      "{'loss': 0.963, 'learning_rate': 8.823529411764707e-06, 'epoch': 14.75}\n",
      "{'loss': 1.0602, 'learning_rate': 8.529411764705883e-06, 'epoch': 14.76}\n",
      "{'loss': 1.1929, 'learning_rate': 8.23529411764706e-06, 'epoch': 14.77}\n",
      "{'loss': 0.9841, 'learning_rate': 7.941176470588235e-06, 'epoch': 14.78}\n",
      "{'loss': 1.1571, 'learning_rate': 7.647058823529413e-06, 'epoch': 14.79}\n",
      "{'loss': 1.103, 'learning_rate': 7.3529411764705884e-06, 'epoch': 14.79}\n",
      "{'loss': 1.1168, 'learning_rate': 7.058823529411765e-06, 'epoch': 14.8}\n",
      "{'loss': 1.1235, 'learning_rate': 6.7647058823529414e-06, 'epoch': 14.81}\n",
      "{'loss': 1.0733, 'learning_rate': 6.470588235294119e-06, 'epoch': 14.82}\n",
      "{'loss': 1.1936, 'learning_rate': 6.1764705882352944e-06, 'epoch': 14.83}\n",
      "{'loss': 1.1078, 'learning_rate': 5.882352941176471e-06, 'epoch': 14.84}\n",
      "{'loss': 1.0746, 'learning_rate': 5.588235294117647e-06, 'epoch': 14.85}\n",
      "{'loss': 1.0714, 'learning_rate': 5.294117647058824e-06, 'epoch': 14.86}\n",
      "{'loss': 0.978, 'learning_rate': 5e-06, 'epoch': 14.87}\n",
      "{'loss': 1.1489, 'learning_rate': 4.705882352941177e-06, 'epoch': 14.88}\n",
      "{'loss': 1.1297, 'learning_rate': 4.411764705882353e-06, 'epoch': 14.88}\n",
      "{'loss': 1.0492, 'learning_rate': 4.11764705882353e-06, 'epoch': 14.89}\n",
      "{'loss': 1.1753, 'learning_rate': 3.823529411764706e-06, 'epoch': 14.9}\n",
      "{'loss': 1.0529, 'learning_rate': 3.5294117647058825e-06, 'epoch': 14.91}\n",
      "{'loss': 1.004, 'learning_rate': 3.2352941176470594e-06, 'epoch': 14.92}\n",
      "{'loss': 1.0897, 'learning_rate': 2.9411764705882355e-06, 'epoch': 14.93}\n",
      "{'loss': 1.1373, 'learning_rate': 2.647058823529412e-06, 'epoch': 14.94}\n",
      "{'loss': 1.0323, 'learning_rate': 2.3529411764705885e-06, 'epoch': 14.95}\n",
      "{'loss': 0.994, 'learning_rate': 2.058823529411765e-06, 'epoch': 14.96}\n",
      "{'loss': 1.036, 'learning_rate': 1.7647058823529412e-06, 'epoch': 14.96}\n",
      "{'loss': 1.1829, 'learning_rate': 1.4705882352941177e-06, 'epoch': 14.97}\n",
      "{'loss': 1.0892, 'learning_rate': 1.1764705882352942e-06, 'epoch': 14.98}\n",
      "{'loss': 1.1122, 'learning_rate': 8.823529411764706e-07, 'epoch': 14.99}\n",
      "{'loss': 1.1569, 'learning_rate': 5.882352941176471e-07, 'epoch': 15.0}\n",
      "{'train_runtime': 25381.8502, 'train_samples_per_second': 4.236, 'train_steps_per_second': 0.066, 'train_loss': 1.7369381053816704, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1680, training_loss=1.7369381053816704, metrics={'train_runtime': 25381.8502, 'train_samples_per_second': 4.236, 'train_steps_per_second': 0.066, 'train_loss': 1.7369381053816704, 'epoch': 15.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=8, \n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=1000, \n",
    "        num_train_epochs=15,\n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        report_to=\"wandb\",\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir='GPT_w_ner'\n",
    "    ),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇███▇▇▆▆▆▅▅▄▄▃▃▂▂▂▁</td></tr><tr><td>train/loss</td><td>███▇▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>15.0</td></tr><tr><td>train/global_step</td><td>1680</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1569</td></tr><tr><td>train/total_flos</td><td>2.0071882986356736e+17</td></tr><tr><td>train/train_loss</td><td>1.73694</td></tr><tr><td>train/train_runtime</td><td>25381.8502</td></tr><tr><td>train/train_samples_per_second</td><td>4.236</td></tr><tr><td>train/train_steps_per_second</td><td>0.066</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BioGPT_w_ner_epoch_15_balanced_train_data</strong> at: <a href='https://wandb.ai/tian1995/GPT2/runs/5a0xl1hw' target=\"_blank\">https://wandb.ai/tian1995/GPT2/runs/5a0xl1hw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230703_004027-5a0xl1hw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "trainer.save_model(\"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced_train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced_train_data.peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are key-unmatches in the trainer.save_model(), we need to rename the keys and load the paras in the model\n",
    "\n",
    "embed_tokens_state_dict = torch.load(\"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced_train_data/pytorch_model.bin\")\n",
    "\n",
    "old_keys = [\"base_model.model.biogpt.embed_tokens.0.weight\", \"base_model.model.output_projection.0.weight\"]\n",
    "new_keys = [\"base_model.model.biogpt.embed_tokens.weight\", \"base_model.model.output_projection.weight\"]\n",
    "\n",
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    # Get the value of the old key\n",
    "    value = embed_tokens_state_dict[old_key]\n",
    "\n",
    "    # Create a new key-value pair with the updated name\n",
    "    embed_tokens_state_dict[new_key] = value\n",
    "\n",
    "    # Delete the old key if desired\n",
    "    del embed_tokens_state_dict[old_key]\n",
    "\n",
    "torch.save(embed_tokens_state_dict, \"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced_train_data/pytorch_model-af.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BioGptForCausalLM(\n",
       "      (biogpt): BioGptModel(\n",
       "        (embed_tokens): CastOutputToFloat(\n",
       "          (0): Embedding(42401, 1024)\n",
       "        )\n",
       "        (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x BioGptDecoderLayer(\n",
       "            (self_attn): BioGptAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (q_proj): Linear(\n",
       "                in_features=1024, out_features=1024, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (output_projection): CastOutputToFloat(\n",
       "        (0): Linear(in_features=1024, out_features=42401, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/tian/mambaforge/envs/BioRED did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/lib/x86_64-linux-gnu/gallium-pipe'), PosixPath('/usr/lib/mesa-diverted/x86_64-linux-gnu'), PosixPath('/usr/lib/x86_64-linux-gnu/mesa')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib/mesa-diverted/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/mesa:/usr/lib/x86_64-linux-gnu/dri:/usr/lib/x86_64-linux-gnu/gallium-pipe did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('0')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/tian-desktop'), PosixPath('@/tmp/.ICE-unix/4416,unix/tian-desktop')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('file'), PosixPath('/home/tian/mambaforge/etc/xml/catalog file')}\n",
      "  warn(msg)\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "\n",
    "peft_model_id = \"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced_train_data.peft\"\n",
    "# config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GPT_w_ner/GPT_w_ner_tokenizer\")\n",
    "\n",
    "# resize the token embeddings to match the tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load the Lora model\n",
    "# the resized embedding layer are still uncorrected, need to load the weights manually\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"GPT_w_ner/models/GPT_w_ner_epoch_15_balanced_train_data/pytorch_model-af.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text: @ HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label: the health care provider.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from data_preprocessing import make_GPT_re_data, GPT_w_ner_preprocess_function\n",
    "additional_tokens, _, _, _ = get_labels(mode='GPT_w_ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 line:\n",
      " []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49396200098a43478e882581afb8b6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load test data and preprocess\n",
    "test_file_path = 'data/BioRED/processed/test.tsv'\n",
    "test_data = make_GPT_re_data(file_path=test_file_path, lower=True)\n",
    "\n",
    "test_dataset_raw = Dataset.from_dict(test_data)\n",
    "# test_dataset = test_dataset_raw.map(NER_preprocess_function, batched=False)\n",
    "# with bert only:\n",
    "test_dataset = test_dataset_raw.map(lambda example: GPT_w_ner_preprocess_function(example, tokenizer, mode=\"gpt_w_ner\", infer=True), batched=True, remove_columns=['pmids', 'text', 'entities', 'outputs'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids'])\n",
    "# the test_dataset has two columns: input_ids and labels, split the labels coloumn into test_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad1dcfd5bc647c2ac753825ed3a3f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = []\n",
    "model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "    # for i in range(1):\n",
    "        output = model.generate(input_ids=test_dataset[i][\"input_ids\"].unsqueeze(0).to(\"cuda\"), max_new_tokens=50)\n",
    "        output_text = tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0]\n",
    "        outputs.append(output_text.split(\"[learn1] [learn2] [learn3] [learn4] [learn5] [learn6] \")[1])\n",
    "\n",
    "    # print(tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>',\n",
       " 'the relation between source [entity1] and target [entity2] is [None]. </s>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Association] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Positive_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Negative_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Positive_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Positive_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Association] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Association] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Negative_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Association] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [Negative_Correlation] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ',\n",
       " 'the relation between source [entity1] and target [entity2] is [None] . ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['labels'][30:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {f\"[{v}]\": 0 for v in relations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lines in train_dataset['labels']:\n",
    "    relation_dict[lines.split(\" \")[-3]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[None]': 18720,\n",
       " '[Association]': 2183,\n",
       " '[Bind]': 60,\n",
       " '[Comparison]': 28,\n",
       " '[Conversion]': 3,\n",
       " '[Cotreatment]': 31,\n",
       " '[Drug_Interaction]': 11,\n",
       " '[Negative_Correlation]': 763,\n",
       " '[Positive_Correlation]': 1088}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"output\": [],\n",
    "    \"label\": []\n",
    "}\n",
    "\n",
    "for output, label in zip(outputs, test_dataset['labels']):\n",
    "    result['output'].append(output)\n",
    "    result['label'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result dictionary\n",
    "import pickle\n",
    "with open(\"GPT_w_ner/result/GPT_w_ner_epoch_15_balanced_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post-processing and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result dictionary\n",
    "import pickle\n",
    "with open(\"GPT_w_ner/result/GPT_w_ner_epoch_15_balanced_result.pkl\", \"rb\") as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorrected = 0\n",
    "for i in range(len(result['output'])):\n",
    "    if result['output'][i][:-6] != result['label'][i][:-3]:\n",
    "        uncorrected += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7590"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(result['output'])):\n",
    "    if result['label'][i][-9:-3] != '[None]':\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [None]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation]. </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5360, 5380):\n",
    "    print(result['output'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Cotreatment] . \n",
      "the relation between source [entity1] and target [entity2] is [Positive_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [Cotreatment] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Cotreatment] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n",
      "the relation between source [entity1] and target [entity2] is [Negative_Correlation] . \n",
      "the relation between source [entity1] and target [entity2] is [None] . \n"
     ]
    }
   ],
   "source": [
    "for i in range(5360, 5380):\n",
    "    print(result['label'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970    s [Positive_Correlation]\n",
      "971    s [Positive_Correlation]\n",
      "972    s [Positive_Correlation]\n",
      "973    s [Positive_Correlation]\n",
      "974    s [Positive_Correlation]\n",
      "975    s [Positive_Correlation]\n",
      "977    s [Positive_Correlation]\n",
      "978    s [Positive_Correlation]\n",
      "980    s [Positive_Correlation]\n",
      "981    s [Positive_Correlation]\n",
      "982    s [Positive_Correlation]\n",
      "983    s [Positive_Correlation]\n",
      "984    s [Positive_Correlation]\n",
      "985    s [Positive_Correlation]\n",
      "986    s [Positive_Correlation]\n",
      "989    s [Positive_Correlation]\n",
      "991    s [Positive_Correlation]\n",
      "992    s [Positive_Correlation]\n",
      "993    s [Positive_Correlation]\n",
      "996    s [Positive_Correlation]\n",
      "997    s [Positive_Correlation]\n",
      "998    s [Positive_Correlation]\n",
      "999    s [Positive_Correlation]\n",
      "1468    ntity2] is [Association]\n",
      "1483    ntity2] is [Association]\n",
      "1488    ntity2] is [Association]\n",
      "1490    ntity2] is [Association]\n",
      "1494    ntity2] is [Association]\n",
      "1502    ntity2] is [Association]\n",
      "1509    ntity2] is [Association]\n",
      "1539    ntity2] is [Association]\n",
      "1554    ntity2] is [Association]\n",
      "1557    ntity2] is [Association]\n",
      "1566    ntity2] is [Association]\n",
      "2793    ntity2] is [Association]\n",
      "4356    s [Positive_Correlation]\n",
      "4357    s [Positive_Correlation]\n",
      "4358    s [Positive_Correlation]\n",
      "4359    s [Positive_Correlation]\n",
      "4360    s [Positive_Correlation]\n",
      "4361    s [Positive_Correlation]\n",
      "4362    s [Positive_Correlation]\n",
      "4363    s [Positive_Correlation]\n",
      "4364    s [Positive_Correlation]\n",
      "4365    s [Positive_Correlation]\n",
      "4366    s [Positive_Correlation]\n",
      "4367    s [Positive_Correlation]\n",
      "4368    s [Positive_Correlation]\n",
      "4369    s [Positive_Correlation]\n",
      "4370    s [Positive_Correlation]\n",
      "4371    s [Positive_Correlation]\n",
      "4372    s [Positive_Correlation]\n",
      "4373    s [Positive_Correlation]\n",
      "4374    s [Positive_Correlation]\n",
      "4375    s [Positive_Correlation]\n",
      "4376    s [Positive_Correlation]\n",
      "4377    s [Positive_Correlation]\n",
      "4378    s [Positive_Correlation]\n",
      "4379    s [Positive_Correlation]\n",
      "4380    s [Positive_Correlation]\n",
      "4381    s [Positive_Correlation]\n",
      "4382    s [Positive_Correlation]\n",
      "4383    s [Positive_Correlation]\n",
      "4384    s [Positive_Correlation]\n",
      "4385    s [Positive_Correlation]\n",
      "4386    s [Positive_Correlation]\n",
      "4387    s [Positive_Correlation]\n",
      "4388    s [Positive_Correlation]\n",
      "4424    ntity2] is [Association]\n",
      "4426    ntity2] is [Association]\n",
      "4454    ntity2] is [Association]\n",
      "4459    ntity2] is [Association]\n",
      "4464    ntity2] is [Association]\n",
      "4545    ntity2] is [Association]\n",
      "4553    ntity2] is [Association]\n",
      "4555    ntity2] is [Association]\n",
      "4558    ntity2] is [Association]\n",
      "4561    ntity2] is [Association]\n",
      "4616    s [Positive_Correlation]\n",
      "4617    s [Positive_Correlation]\n",
      "4618    s [Positive_Correlation]\n",
      "4619    s [Positive_Correlation]\n",
      "4620    s [Positive_Correlation]\n",
      "4621    s [Positive_Correlation]\n",
      "4622    s [Positive_Correlation]\n",
      "4623    s [Positive_Correlation]\n",
      "4624    s [Positive_Correlation]\n",
      "4896    ntity2] is [Association]\n",
      "4897    ntity2] is [Association]\n",
      "4898    ntity2] is [Association]\n",
      "4899    ntity2] is [Association]\n",
      "4900    ntity2] is [Association]\n",
      "4903    ntity2] is [Association]\n",
      "4905    ntity2] is [Association]\n",
      "4906    ntity2] is [Association]\n",
      "4907    ntity2] is [Association]\n",
      "5363    s [Positive_Correlation]\n",
      "5364    s [Positive_Correlation]\n",
      "5365    s [Positive_Correlation]\n",
      "5366    s [Positive_Correlation]\n",
      "5367    s [Positive_Correlation]\n",
      "5368    s [Positive_Correlation]\n",
      "5370    s [Positive_Correlation]\n",
      "5372    s [Positive_Correlation]\n",
      "5374    s [Positive_Correlation]\n",
      "5375    s [Positive_Correlation]\n",
      "5377    s [Positive_Correlation]\n",
      "5378    s [Positive_Correlation]\n",
      "5379    s [Positive_Correlation]\n",
      "5380    s [Positive_Correlation]\n",
      "5381    s [Positive_Correlation]\n",
      "5382    s [Positive_Correlation]\n",
      "5383    s [Positive_Correlation]\n",
      "5384    s [Positive_Correlation]\n",
      "5385    s [Positive_Correlation]\n",
      "6485    s [Positive_Correlation]\n",
      "6497    s [Positive_Correlation]\n",
      "6517    s [Positive_Correlation]\n",
      "6524    s [Positive_Correlation]\n",
      "6528    s [Positive_Correlation]\n",
      "6532    s [Positive_Correlation]\n",
      "6983    s [Positive_Correlation]\n",
      "6984    s [Positive_Correlation]\n",
      "6985    s [Positive_Correlation]\n",
      "6986    s [Positive_Correlation]\n",
      "7256    ntity2] is [Association]\n",
      "7283    ntity2] is [Association]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(result['output'])):\n",
    "    if result['output'][i][-12:-6] != '[None]':\n",
    "        print(i,\"  \", result['output'][i][-30:-6])\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
