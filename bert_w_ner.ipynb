{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model: \n",
    "\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "The input be like:\n",
    "\n",
    "    [cls] [relation] [sep] \"text w ner1 and ner2 tags\"\n",
    "\n",
    "(the ner1 and ner2 tags depends on the first occurence orders)\n",
    "\n",
    "The output be like:\n",
    "\n",
    "    [src] [none] [tgt] [none]\n",
    "\n",
    "or\n",
    "\n",
    "    [src] [ner2] [tgt] [ner1]\n",
    "\n",
    "or\n",
    "\n",
    "    [src] [ner1] [tgt] [ner2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_special_tokens': ['[ner1]', '[/ner1]', '[ner2]', '[/ner2]', '[Association]', '[Bind]', '[Comparison]', '[Conversion]', '[Cotreatment]', '[Drug_Interaction]', '[Negative_Correlation]', '[Positive_Correlation]']} \n",
      " ['[pad]', '[src]', '[ner1]', '[ner2]', '[tgt]', '[none]']\n"
     ]
    }
   ],
   "source": [
    "# load labels for bert_w_ner\n",
    "additional_tokens, labels, id2label, label2id = get_labels(mode='bert_w_ner')\n",
    "print(additional_tokens, \"\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [UNK]\n",
      "3: [SEP]\n",
      "0: [PAD]\n",
      "2: [CLS]\n",
      "4: [MASK]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "for id in [1, 3, 0, 2, 4]:\n",
    "    print(f\"{id}: {tokenizer.decode(id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 12 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert_w_ner/bert_w_ner_tokenizer/tokenizer_config.json',\n",
       " 'bert_w_ner/bert_w_ner_tokenizer/special_tokens_map.json',\n",
       " 'bert_w_ner/bert_w_ner_tokenizer/vocab.txt',\n",
       " 'bert_w_ner/bert_w_ner_tokenizer/added_tokens.json',\n",
       " 'bert_w_ner/bert_w_ner_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding new tokens to the tokenizer\n",
    "# since I haven't load the model so I will resize the embedding of the model later]\n",
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(\"bert_w_ner/bert_w_ner_tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import make_bert_re_data\n",
    "from data_preprocessing import bert_w_ner_preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and valid file paths\n",
    "train_file_path = 'data/BioRED/processed/train.tsv'\n",
    "valid_file_path = 'data/BioRED/processed/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bert_re data\n",
    "train_data_raw = make_bert_re_data(file_path=train_file_path, lower=True, output_none=True)\n",
    "valid_data_raw = make_bert_re_data(file_path=valid_file_path, lower=True, output_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into Dataset type\n",
    "train_data_raw = Dataset.from_dict(train_data_raw)\n",
    "valid_data_raw = Dataset.from_dict(valid_data_raw)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_data_raw,\n",
    "    \"valid\": valid_data_raw\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c217c6028774c9380d1eaf8b1c9aef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/183160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ddcb2495cf44ddbff776fb969a416f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"inputs\", \"outputs\", \"pmids\"])\n",
    "tokenized_datasets = dataset.map(lambda example: bert_w_ner_preprocess_function(example, tokenizer), batched=True, remove_columns=[\"input_texts\", \"input_relations\", \"outputs\", \"pmids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 183160\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 53264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the number of trainable parameters in the model.\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[pad]', '[src]', '[ner1]', '[ner2]', '[tgt]', '[none]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint,\n",
    "                                                        num_labels=len(labels),\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id,\n",
    "                                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding special tokens to the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30534, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108905478 || all params: 108905478 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,\n",
      "        output_type=TokenClassifierOutput,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,\n",
      "        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.Tensor] = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        token_type_ids: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.Tensor] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        inputs_embeds: Optional[torch.Tensor] = None,\n",
      "        labels: Optional[torch.Tensor] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n",
      "        r\"\"\"\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
      "        \"\"\"\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        outputs = self.bert(\n",
      "            input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            token_type_ids=token_type_ids,\n",
      "            position_ids=position_ids,\n",
      "            head_mask=head_mask,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        sequence_output = outputs[0]\n",
      "\n",
      "        sequence_output = self.dropout(sequence_output)\n",
      "        logits = self.classifier(sequence_output)\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[2:]\n",
      "            return ((loss,) + output) if loss is not None else output\n",
      "\n",
      "        return TokenClassifierOutput(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(model.forward))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # only consider non-padding tokens [:4]\n",
    "    true_labels = [[id2label[l.item()] for l in label[:4]] for label in labels]\n",
    "    true_predictions = [\n",
    "    [id2label[p.item()] for (p, l) in zip(prediction[:4], label[:4])]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customized loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the feature name \"labels\" to \"labels_\" of tokenized_datasets\n",
    "# to not trigger the default loss function the enssembled in Auto model\n",
    "\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"labels\", \"labels_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 183160\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 53264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 5, 4,  ..., 0, 0, 0],\n",
       "        [1, 5, 4,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "tokenized_datasets\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        # only consider first 4 columns of the batched loss and the batched labels\n",
    "        logits = logits[:, :4, :]\n",
    "        labels = labels[:, :4]\n",
    "        loss = loss_fct(logits.reshape(-1, self.model.config.num_labels), labels.reshape(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/BioRED/wandb/run-20230624_173553-5dntu29g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/BERT/runs/5dntu29g' target=\"_blank\">bert_w_ner_epoch_5_loss2</a></strong> to <a href='https://wandb.ai/tian1995/BERT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/BERT' target=\"_blank\">https://wandb.ai/tian1995/BERT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/BERT/runs/5dntu29g' target=\"_blank\">https://wandb.ai/tian1995/BERT/runs/5dntu29g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/BERT/runs/5dntu29g?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f519f262b90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"BERT\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"bert_w_ner_epoch_5_loss2\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",\n",
    "    # per_device_train_batch_size=4,\n",
    "    # per_device_eval_batch_size=4,\n",
    "    auto_find_batch_size=True,\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09c85ced34b4e23ad27208da8395afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0271, 'learning_rate': 1.9912644682245032e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0008, 'learning_rate': 1.9825289364490066e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0006, 'learning_rate': 1.9737934046735096e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0005, 'learning_rate': 1.9650578728980127e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0005, 'learning_rate': 1.956322341122516e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0004, 'learning_rate': 1.947586809347019e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0004, 'learning_rate': 1.938851277571522e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0003, 'learning_rate': 1.9301157457960255e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0004, 'learning_rate': 1.9213802140205286e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0003, 'learning_rate': 1.9126446822450316e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0003, 'learning_rate': 1.903909150469535e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0004, 'learning_rate': 1.895173618694038e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0004, 'learning_rate': 1.8864380869185414e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0004, 'learning_rate': 1.8777025551430444e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0003, 'learning_rate': 1.8689670233675478e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0003, 'learning_rate': 1.860231491592051e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0003, 'learning_rate': 1.851495959816554e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0003, 'learning_rate': 1.8427604280410573e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0003, 'learning_rate': 1.8340248962655603e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0003, 'learning_rate': 1.8252893644900637e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0006, 'learning_rate': 1.8165538327145667e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0004, 'learning_rate': 1.8078183009390698e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0003, 'learning_rate': 1.799082769163573e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7903472373880762e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7816117056125792e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7728761738370826e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7641406420615857e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7554051102860887e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0003, 'learning_rate': 1.746669578510592e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0003, 'learning_rate': 1.737934046735095e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7291985149595982e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7204629831841016e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7117274514086046e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7029919196331076e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0003, 'learning_rate': 1.694256387857611e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0002, 'learning_rate': 1.685520856082114e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0003, 'learning_rate': 1.676785324306617e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6680497925311205e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6593142607556235e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6505787289801266e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0002, 'learning_rate': 1.64184319720463e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0003, 'learning_rate': 1.633107665429133e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6243721336536364e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6156366018781394e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6069010701026428e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b6e829ca9e418d99ddc5e04d295473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002926877059508115, 'eval_precision': 0.9894863322319015, 'eval_recall': 0.9894863322319015, 'eval_f1': 0.9894863322319015, 'eval_accuracy': 0.9894863322319015, 'eval_runtime': 681.9754, 'eval_samples_per_second': 78.103, 'eval_steps_per_second': 9.763, 'epoch': 1.0}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5981655383271458e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0003, 'learning_rate': 1.589430006551649e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5806944747761522e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5719589430006553e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5632234112251587e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5544878794496617e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5457523476741647e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0003, 'learning_rate': 1.537016815898668e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5282812841231712e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5195457523476742e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5108102205721776e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5020746887966806e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4933391570211837e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0003, 'learning_rate': 1.484603625245687e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4758680934701901e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4671325616946931e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4583970299191965e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0003, 'learning_rate': 1.4496614981436996e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4409259663682028e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0002, 'learning_rate': 1.432190434592706e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0002, 'learning_rate': 1.423454902817209e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4147193710417122e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4059838392662154e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3972483074907187e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3885127757152217e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0002, 'learning_rate': 1.379777243939725e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3710417121642281e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0003, 'learning_rate': 1.3623061803887312e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3535706486132345e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3448351168377376e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3360995850622406e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0002, 'learning_rate': 1.327364053286744e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0002, 'learning_rate': 1.318628521511247e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3098929897357503e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0003, 'learning_rate': 1.3011574579602535e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0003, 'learning_rate': 1.2924219261847567e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2836863944092597e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0002, 'learning_rate': 1.274950862633763e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2662153308582661e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0003, 'learning_rate': 1.2574797990827692e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0003, 'learning_rate': 1.2487442673072726e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2400087355317756e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2312732037562786e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0002, 'learning_rate': 1.222537671980782e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0002, 'learning_rate': 1.213802140205285e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2050666084297881e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e98bfe86034f1ea7264b77967f1b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002620388404466212, 'eval_precision': 0.989373685791529, 'eval_recall': 0.989373685791529, 'eval_f1': 0.989373685791529, 'eval_accuracy': 0.989373685791529, 'eval_runtime': 679.4867, 'eval_samples_per_second': 78.389, 'eval_steps_per_second': 9.799, 'epoch': 2.0}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1963310766542915e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1875955448787945e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1788600131032977e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0003, 'learning_rate': 1.170124481327801e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0003, 'learning_rate': 1.1613889495523042e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1526534177768072e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1439178860013104e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1351823542258136e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1264468224503167e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0002, 'learning_rate': 1.11771129067482e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0002, 'learning_rate': 1.108975758899323e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1002402271238261e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0915046953483295e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0827691635728325e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0740336317973356e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0002, 'learning_rate': 1.065298100021839e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0002, 'learning_rate': 1.056562568246342e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0478270364708452e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0390915046953484e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0303559729198516e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0216204411443547e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0128849093688579e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0041493775933611e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0002, 'learning_rate': 9.954138458178643e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0002, 'learning_rate': 9.866783140423674e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0002, 'learning_rate': 9.779427822668706e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0002, 'learning_rate': 9.692072504913738e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0002, 'learning_rate': 9.604717187158768e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0002, 'learning_rate': 9.5173618694038e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0002, 'learning_rate': 9.430006551648832e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0002, 'learning_rate': 9.342651233893863e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0002, 'learning_rate': 9.255295916138895e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0002, 'learning_rate': 9.167940598383927e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0002, 'learning_rate': 9.080585280628959e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0002, 'learning_rate': 8.993229962873991e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0002, 'learning_rate': 8.905874645119023e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0002, 'learning_rate': 8.818519327364054e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0002, 'learning_rate': 8.731164009609086e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0002, 'learning_rate': 8.643808691854118e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0002, 'learning_rate': 8.556453374099148e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0002, 'learning_rate': 8.46909805634418e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0002, 'learning_rate': 8.381742738589213e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0002, 'learning_rate': 8.294387420834243e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0002, 'learning_rate': 8.207032103079275e-06, 'epoch': 2.95}\n",
      "{'loss': 0.0002, 'learning_rate': 8.119676785324307e-06, 'epoch': 2.97}\n",
      "{'loss': 0.0001, 'learning_rate': 8.032321467569338e-06, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac41a194d9d04485acf54cb3508c1c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002686600782908499, 'eval_precision': 0.9890686016821868, 'eval_recall': 0.9890686016821868, 'eval_f1': 0.9890686016821868, 'eval_accuracy': 0.9890686016821868, 'eval_runtime': 679.4823, 'eval_samples_per_second': 78.389, 'eval_steps_per_second': 9.799, 'epoch': 3.0}\n",
      "{'loss': 0.0002, 'learning_rate': 7.94496614981437e-06, 'epoch': 3.01}\n",
      "{'loss': 0.0002, 'learning_rate': 7.857610832059402e-06, 'epoch': 3.04}\n",
      "{'loss': 0.0002, 'learning_rate': 7.770255514304434e-06, 'epoch': 3.06}\n",
      "{'loss': 0.0002, 'learning_rate': 7.682900196549466e-06, 'epoch': 3.08}\n",
      "{'loss': 0.0002, 'learning_rate': 7.595544878794497e-06, 'epoch': 3.1}\n",
      "{'loss': 0.0002, 'learning_rate': 7.508189561039529e-06, 'epoch': 3.12}\n",
      "{'loss': 0.0002, 'learning_rate': 7.420834243284561e-06, 'epoch': 3.14}\n",
      "{'loss': 0.0002, 'learning_rate': 7.333478925529593e-06, 'epoch': 3.17}\n",
      "{'loss': 0.0002, 'learning_rate': 7.246123607774624e-06, 'epoch': 3.19}\n",
      "{'loss': 0.0002, 'learning_rate': 7.158768290019655e-06, 'epoch': 3.21}\n",
      "{'loss': 0.0002, 'learning_rate': 7.0714129722646874e-06, 'epoch': 3.23}\n",
      "{'loss': 0.0002, 'learning_rate': 6.9840576545097195e-06, 'epoch': 3.25}\n",
      "{'loss': 0.0002, 'learning_rate': 6.89670233675475e-06, 'epoch': 3.28}\n",
      "{'loss': 0.0002, 'learning_rate': 6.809347018999782e-06, 'epoch': 3.3}\n",
      "{'loss': 0.0002, 'learning_rate': 6.721991701244814e-06, 'epoch': 3.32}\n",
      "{'loss': 0.0002, 'learning_rate': 6.6346363834898454e-06, 'epoch': 3.34}\n",
      "{'loss': 0.0001, 'learning_rate': 6.5472810657348775e-06, 'epoch': 3.36}\n",
      "{'loss': 0.0002, 'learning_rate': 6.45992574797991e-06, 'epoch': 3.39}\n",
      "{'loss': 0.0002, 'learning_rate': 6.37257043022494e-06, 'epoch': 3.41}\n",
      "{'loss': 0.0002, 'learning_rate': 6.285215112469972e-06, 'epoch': 3.43}\n",
      "{'loss': 0.0002, 'learning_rate': 6.197859794715004e-06, 'epoch': 3.45}\n",
      "{'loss': 0.0002, 'learning_rate': 6.1105044769600355e-06, 'epoch': 3.47}\n",
      "{'loss': 0.0002, 'learning_rate': 6.023149159205068e-06, 'epoch': 3.49}\n",
      "{'loss': 0.0002, 'learning_rate': 5.935793841450099e-06, 'epoch': 3.52}\n",
      "{'loss': 0.0002, 'learning_rate': 5.84843852369513e-06, 'epoch': 3.54}\n",
      "{'loss': 0.0002, 'learning_rate': 5.761083205940162e-06, 'epoch': 3.56}\n",
      "{'loss': 0.0001, 'learning_rate': 5.673727888185194e-06, 'epoch': 3.58}\n",
      "{'loss': 0.0001, 'learning_rate': 5.586372570430225e-06, 'epoch': 3.6}\n",
      "{'loss': 0.0002, 'learning_rate': 5.499017252675257e-06, 'epoch': 3.63}\n",
      "{'loss': 0.0002, 'learning_rate': 5.411661934920289e-06, 'epoch': 3.65}\n",
      "{'loss': 0.0001, 'learning_rate': 5.32430661716532e-06, 'epoch': 3.67}\n",
      "{'loss': 0.0002, 'learning_rate': 5.236951299410352e-06, 'epoch': 3.69}\n",
      "{'loss': 0.0002, 'learning_rate': 5.1495959816553845e-06, 'epoch': 3.71}\n",
      "{'loss': 0.0002, 'learning_rate': 5.062240663900415e-06, 'epoch': 3.73}\n",
      "{'loss': 0.0001, 'learning_rate': 4.974885346145447e-06, 'epoch': 3.76}\n",
      "{'loss': 0.0002, 'learning_rate': 4.887530028390478e-06, 'epoch': 3.78}\n",
      "{'loss': 0.0002, 'learning_rate': 4.80017471063551e-06, 'epoch': 3.8}\n",
      "{'loss': 0.0002, 'learning_rate': 4.7128193928805425e-06, 'epoch': 3.82}\n",
      "{'loss': 0.0001, 'learning_rate': 4.625464075125574e-06, 'epoch': 3.84}\n",
      "{'loss': 0.0002, 'learning_rate': 4.538108757370605e-06, 'epoch': 3.87}\n",
      "{'loss': 0.0001, 'learning_rate': 4.450753439615637e-06, 'epoch': 3.89}\n",
      "{'loss': 0.0002, 'learning_rate': 4.363398121860668e-06, 'epoch': 3.91}\n",
      "{'loss': 0.0001, 'learning_rate': 4.2760428041057005e-06, 'epoch': 3.93}\n",
      "{'loss': 0.0002, 'learning_rate': 4.188687486350732e-06, 'epoch': 3.95}\n",
      "{'loss': 0.0002, 'learning_rate': 4.101332168595764e-06, 'epoch': 3.97}\n",
      "{'loss': 0.0002, 'learning_rate': 4.013976850840795e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d15f8a229348cab16b2f6c9a1c0aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002841673558577895, 'eval_precision': 0.9874070666866926, 'eval_recall': 0.9874070666866926, 'eval_f1': 0.9874070666866926, 'eval_accuracy': 0.9874070666866926, 'eval_runtime': 681.1798, 'eval_samples_per_second': 78.194, 'eval_steps_per_second': 9.774, 'epoch': 4.0}\n",
      "{'loss': 0.0002, 'learning_rate': 3.926621533085827e-06, 'epoch': 4.02}\n",
      "{'loss': 0.0001, 'learning_rate': 3.8392662153308585e-06, 'epoch': 4.04}\n",
      "{'loss': 0.0001, 'learning_rate': 3.75191089757589e-06, 'epoch': 4.06}\n",
      "{'loss': 0.0001, 'learning_rate': 3.664555579820922e-06, 'epoch': 4.08}\n",
      "{'loss': 0.0001, 'learning_rate': 3.5772002620659535e-06, 'epoch': 4.11}\n",
      "{'loss': 0.0001, 'learning_rate': 3.4898449443109848e-06, 'epoch': 4.13}\n",
      "{'loss': 0.0002, 'learning_rate': 3.402489626556017e-06, 'epoch': 4.15}\n",
      "{'loss': 0.0001, 'learning_rate': 3.3151343088010486e-06, 'epoch': 4.17}\n",
      "{'loss': 0.0001, 'learning_rate': 3.22777899104608e-06, 'epoch': 4.19}\n",
      "{'loss': 0.0001, 'learning_rate': 3.140423673291112e-06, 'epoch': 4.21}\n",
      "{'loss': 0.0001, 'learning_rate': 3.0530683555361436e-06, 'epoch': 4.24}\n",
      "{'loss': 0.0001, 'learning_rate': 2.965713037781175e-06, 'epoch': 4.26}\n",
      "{'loss': 0.0001, 'learning_rate': 2.878357720026207e-06, 'epoch': 4.28}\n",
      "{'loss': 0.0001, 'learning_rate': 2.7910024022712382e-06, 'epoch': 4.3}\n",
      "{'loss': 0.0001, 'learning_rate': 2.70364708451627e-06, 'epoch': 4.32}\n",
      "{'loss': 0.0001, 'learning_rate': 2.616291766761302e-06, 'epoch': 4.35}\n",
      "{'loss': 0.0002, 'learning_rate': 2.5289364490063333e-06, 'epoch': 4.37}\n",
      "{'loss': 0.0001, 'learning_rate': 2.441581131251365e-06, 'epoch': 4.39}\n",
      "{'loss': 0.0001, 'learning_rate': 2.354225813496397e-06, 'epoch': 4.41}\n",
      "{'loss': 0.0001, 'learning_rate': 2.2668704957414283e-06, 'epoch': 4.43}\n",
      "{'loss': 0.0001, 'learning_rate': 2.17951517798646e-06, 'epoch': 4.46}\n",
      "{'loss': 0.0001, 'learning_rate': 2.0921598602314917e-06, 'epoch': 4.48}\n",
      "{'loss': 0.0001, 'learning_rate': 2.0048045424765234e-06, 'epoch': 4.5}\n",
      "{'loss': 0.0001, 'learning_rate': 1.917449224721555e-06, 'epoch': 4.52}\n",
      "{'loss': 0.0001, 'learning_rate': 1.8300939069665868e-06, 'epoch': 4.54}\n",
      "{'loss': 0.0001, 'learning_rate': 1.7427385892116182e-06, 'epoch': 4.56}\n",
      "{'loss': 0.0001, 'learning_rate': 1.6553832714566501e-06, 'epoch': 4.59}\n",
      "{'loss': 0.0001, 'learning_rate': 1.5680279537016818e-06, 'epoch': 4.61}\n",
      "{'loss': 0.0001, 'learning_rate': 1.4806726359467133e-06, 'epoch': 4.63}\n",
      "{'loss': 0.0001, 'learning_rate': 1.393317318191745e-06, 'epoch': 4.65}\n",
      "{'loss': 0.0001, 'learning_rate': 1.3059620004367769e-06, 'epoch': 4.67}\n",
      "{'loss': 0.0001, 'learning_rate': 1.2186066826818083e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0001, 'learning_rate': 1.13125136492684e-06, 'epoch': 4.72}\n",
      "{'loss': 0.0001, 'learning_rate': 1.0438960471718717e-06, 'epoch': 4.74}\n",
      "{'loss': 0.0001, 'learning_rate': 9.565407294169034e-07, 'epoch': 4.76}\n",
      "{'loss': 0.0001, 'learning_rate': 8.69185411661935e-07, 'epoch': 4.78}\n",
      "{'loss': 0.0001, 'learning_rate': 7.818300939069666e-07, 'epoch': 4.8}\n",
      "{'loss': 0.0001, 'learning_rate': 6.944747761519983e-07, 'epoch': 4.83}\n",
      "{'loss': 0.0001, 'learning_rate': 6.0711945839703e-07, 'epoch': 4.85}\n",
      "{'loss': 0.0001, 'learning_rate': 5.197641406420616e-07, 'epoch': 4.87}\n",
      "{'loss': 0.0001, 'learning_rate': 4.324088228870933e-07, 'epoch': 4.89}\n",
      "{'loss': 0.0001, 'learning_rate': 3.4505350513212496e-07, 'epoch': 4.91}\n",
      "{'loss': 0.0001, 'learning_rate': 2.5769818737715664e-07, 'epoch': 4.94}\n",
      "{'loss': 0.0001, 'learning_rate': 1.7034286962218827e-07, 'epoch': 4.96}\n",
      "{'loss': 0.0001, 'learning_rate': 8.298755186721993e-08, 'epoch': 4.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064761777f4242348eb081bc9914b3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00030054408125579357, 'eval_precision': 0.9884865950735957, 'eval_recall': 0.9884865950735957, 'eval_f1': 0.9884865950735957, 'eval_accuracy': 0.9884865950735957, 'eval_runtime': 678.2311, 'eval_samples_per_second': 78.534, 'eval_steps_per_second': 9.817, 'epoch': 5.0}\n",
      "{'train_runtime': 39488.3426, 'train_samples_per_second': 23.192, 'train_steps_per_second': 2.899, 'train_loss': 0.0003305545512389137, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114475, training_loss=0.0003305545512389137, metrics={'train_runtime': 39488.3426, 'train_samples_per_second': 23.192, 'train_steps_per_second': 2.899, 'train_loss': 0.0003305545512389137, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>██▇▁▅</td></tr><tr><td>eval/f1</td><td>██▇▁▅</td></tr><tr><td>eval/loss</td><td>▇▁▂▅█</td></tr><tr><td>eval/precision</td><td>██▇▁▅</td></tr><tr><td>eval/recall</td><td>██▇▁▅</td></tr><tr><td>eval/runtime</td><td>█▃▃▇▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▆▂█</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▆▂█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▄▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.98849</td></tr><tr><td>eval/f1</td><td>0.98849</td></tr><tr><td>eval/loss</td><td>0.0003</td></tr><tr><td>eval/precision</td><td>0.98849</td></tr><tr><td>eval/recall</td><td>0.98849</td></tr><tr><td>eval/runtime</td><td>678.2311</td></tr><tr><td>eval/samples_per_second</td><td>78.534</td></tr><tr><td>eval/steps_per_second</td><td>9.817</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>114475</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0001</td></tr><tr><td>train/total_flos</td><td>2.393042236452864e+17</td></tr><tr><td>train/train_loss</td><td>0.00033</td></tr><tr><td>train/train_runtime</td><td>39488.3426</td></tr><tr><td>train/train_samples_per_second</td><td>23.192</td></tr><tr><td>train/train_steps_per_second</td><td>2.899</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert_w_ner_epoch_5_loss2</strong> at: <a href='https://wandb.ai/tian1995/BERT/runs/5dntu29g' target=\"_blank\">https://wandb.ai/tian1995/BERT/runs/5dntu29g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230624_173553-5dntu29g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "trainer.save_model(\"bert_w_ner/models/bert_w_ner_epoch_5_loss2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from data_preprocessing import make_bert_re_data\n",
    "from data_preprocessing import bert_w_ner_preprocess_function\n",
    "additional_tokens, labels, id2label, label2id = get_labels(mode='bert_w_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and tokenizer\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert_w_ner/models/bert_w_ner_epoch_5_loss2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert_w_ner/bert_w_ner_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6b7698431143bd8364be70d5a564e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load test data and preprocess\n",
    "test_file_path = 'data/BioRED/processed/test.tsv'\n",
    "test_data = make_bert_re_data(file_path=test_file_path , lower=True, output_none=True)\n",
    "\n",
    "test_dataset_raw = Dataset.from_dict(test_data)\n",
    "# test_dataset = test_dataset_raw.map(NER_preprocess_function, batched=False)\n",
    "# with bert only:\n",
    "test_dataset = test_dataset_raw.map(lambda example: bert_w_ner_preprocess_function(example, tokenizer), batched=True, remove_columns=[\"input_texts\", \"input_relations\", \"outputs\", \"pmids\"])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2, 30526,     3,    43,  4008, 14870,  9422,  3979, 26383,  1966,\n",
       "            43,  6558,  2386,  1927,  2815,  8877,  4741,  1956, 13833,  4727,\n",
       "          1927, 17397,    19, 26192,    18,  4470,    30,  9901,  2815,  8877,\n",
       "          4741,    12, 22272,  1973,    13,  1956,  1922, 19738,  4727,  1927,\n",
       "          1920, 11529, 12654,  1977,  2458,  1956,    43,  3930,  6573,    18,\n",
       "          1922,  2052,  2161,  2038,  3845,    43, 12203,  2774,  1956,  5944,\n",
       "         26192,    16,    22,    30,    21, 30522, 28355,  3367, 30523,  1930,\n",
       "          6493, 17397, 12722,  2256,  5043,    18,  2860,    30, 13736,  2333,\n",
       "          1930,  2678,  4754,  1985,  3609,  1922,    43, 12203,    18,  1920,\n",
       "            22,    30,    21, 30522, 28355,  3367, 30523,  4305,  1942,    21,\n",
       "            30,    21, 12382,  2444,  2256,  7307, 19567,  7386,  2014,    43,\n",
       "          2149,  3253,  1927, 10114,  2383,  4425,    16,  2154,  2222,  4642,\n",
       "          1920,  6493, 17397,    18,  2274,    30,    43,  4008,    16,  6242,\n",
       "         22272,  1973,    17,    23,  3979,  1982,  2899,  1922,  1920,  9117,\n",
       "          5367,    26,  1927,  3418,  4370,  1927,  1920,  4049,    12,    64,\n",
       "            13,    21,    18,    25,  4435,  5227,  4999,    16,  1956,    43,\n",
       "            49,    17,    17,    34,    43,  6636,  2019,  8905, 15797,  1010,\n",
       "            16,  2154,  7003,    43, 21884,    12, 29799,    13,  1942,    43,\n",
       "         12417,    12,  9353,    13,    18,  1920, 21700,  1982,  9589,  2308,\n",
       "          1920,  3979,  1982,  7685,  1922,  1920,  5853,  1930,  1920, 14611,\n",
       "            18,  2294,  1927,  2052,  3606,  4999,  1922, 17309, 23437,  6492,\n",
       "          2094,  2007,  3200,    17,  6554, 10017,  3404,    43,  7733,  5575,\n",
       "          1986, 11140,    17,  4532,  2308, 19567,    17,  4910,  3214,  1988,\n",
       "          1982,  2458,  1956,    43,  2843,  5950,  1927,  1920,  8121,    17,\n",
       "          3360,  8005,  5447,    16, 18967,  1915,  2936,  5447,  1930,  8411,\n",
       "          5241,  2037,  8005,    18,  2038,  2222,  2435,    43,  2551, 13189,\n",
       "          5174,  1958,  1920, 12260, 30524, 11217, 10308,  1006,    55, 30525,\n",
       "          3606,    18,  2308,    16,  1920,  2303, 12260, 27926, 10308,  5896,\n",
       "          3606,  2430,  2239,  7733,  3214,  1930,  1982,  3974,  2458,  1956,\n",
       "            43,  2843,  5950,  1927,  8005,    18,  4355,    30,  2144,  3056,\n",
       "          3220,  1988,  1920,  4049,    12,    64,    13,    21,    18,    25,\n",
       "            19, 11217, 10308,  1010,    55,  4999,  5273,  1930,  3216, 12260,\n",
       "          4674,  4672,  1942,    43,  7733, 16998,  3214,  2810,  1942,  5520,\n",
       "          8005,  7664,  1930,  5908,  9901, 22272,  1973,  1956, 13833,  4727,\n",
       "          1927, 16934,  1988, 10711,  1942, 19567,  1930, 10114,  2383,  4425,\n",
       "            18,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'labels': tensor([1, 5, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics_infer(logits, labels):\n",
    "    predictions = [np.argmax(logit, axis=-1)[0] for logit in logits]\n",
    "\n",
    "    # only consider non-padding tokens [:4]\n",
    "    true_labels = [[id2label[l.item()] for l in label[[1, 3]]] for label in labels]\n",
    "    true_predictions = [[id2label[l.item()] for l in label[[1, 3]]] for label in predictions]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }, all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60720/60720 [15:02<00:00, 67.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_line in tqdm(test_dataset['input_ids']):\n",
    "    # for n in range(1):\n",
    "        torch.cuda.empty_cache()\n",
    "        out = model(input_ids=input_line.unsqueeze(0).to(\"cuda\"))\n",
    "        # print(f\"{n+1} / {len(test_dataset)}\")\n",
    "        output.append(out[0].to(\"cpu\"))\n",
    "        # output.append(torch.argmax(out[0], dim=-1).squeeze(0))\n",
    "        # output[-1].to(\"cpu\")\n",
    "    # print([tag_to_NER_id[i.item()] for i in output[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output logits to a file\n",
    "# torch.save(output, 'bert_w_ner/results/bert_w_ner_epoch_5_loss2_raw.pt')\n",
    "\n",
    "output = torch.load('bert_w_ner/results/bert_w_ner_epoch_5_loss2_raw.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [pad] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results, all_metric = compute_metrics_infer(output, test_dataset['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.9739383343409421,\n",
       " 'recall': 0.9632047573647043,\n",
       " 'f1': 0.9685418088459913,\n",
       " 'accuracy': 0.9798913043478261}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ner1]': {'precision': 0.483695652173913,\n",
       "  'recall': 0.1530524505588994,\n",
       "  'f1': 0.23252775963422598,\n",
       "  'number': 1163},\n",
       " 'ner2]': {'precision': 0.5015105740181269,\n",
       "  'recall': 0.14273430782459157,\n",
       "  'f1': 0.22222222222222224,\n",
       "  'number': 1163},\n",
       " 'none]': {'precision': 0.9813376608322708,\n",
       "  'recall': 0.9950467619255503,\n",
       "  'f1': 0.9881446650992947,\n",
       "  'number': 59557},\n",
       " 'pad]': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0},\n",
       " 'overall_precision': 0.9739383343409421,\n",
       " 'overall_recall': 0.9632047573647043,\n",
       " 'overall_f1': 0.9685418088459913,\n",
       " 'overall_accuracy': 0.9798913043478261}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "import pickle\n",
    "\n",
    "# save the dictionary to a file\n",
    "with open('bert_w_ner/results/bert_w_ner_epoch_5_loss2.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dictionary from a file\n",
    "with open('bert_w_ner/results/bert_w_ner_epoch_5_loss2.pickle', 'rb') as handle:\n",
    "    results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_infer_alter(logits, labels):\n",
    "    predictions = [np.argmax(logit, axis=-1)[0] for logit in logits]\n",
    "\n",
    "    # only consider non-padding tokens [:4]\n",
    "    all_src = 0\n",
    "    all_tgt = 0\n",
    "    correct_src = 0\n",
    "    pred_src = 0\n",
    "    pred_tgt = 0\n",
    "    correct_tgt = 0\n",
    "    all_none = 0\n",
    "    correct_none = 0\n",
    "    pred_none = 0\n",
    "    true_labels = [[id2label[l.item()] for l in label[[1, 3]]] for label in labels]\n",
    "    true_predictions = [[id2label[l.item()] for l in label[[1, 3]]] for label in predictions]\n",
    "    for prediction, label in zip(true_predictions, true_labels):\n",
    "        if label[0] == '[none]':\n",
    "            all_none += 1\n",
    "            if prediction[0] == '[none]' and prediction[1] == '[none]':\n",
    "                correct_none += 1\n",
    "        else:\n",
    "            all_src += 1\n",
    "            all_tgt += 1\n",
    "            if prediction[0] == label[0]:\n",
    "                correct_src += 1\n",
    "            if prediction[1] == label[1]:\n",
    "                correct_tgt += 1\n",
    "        if prediction[0] == '[none]':\n",
    "            pred_none += 1\n",
    "        else:\n",
    "            if prediction[0] != '[none]':\n",
    "                pred_src += 1\n",
    "            if prediction[1] != '[none]':\n",
    "                pred_tgt += 1\n",
    "\n",
    "    return {\n",
    "        \"src precision\": correct_src / pred_src,\n",
    "        \"src recall\": correct_src / all_src,\n",
    "        \"src_f1\": 2 * correct_src / (pred_src + all_src),\n",
    "        \"tgt precision\": correct_tgt / pred_tgt,\n",
    "        \"tgt recall\": correct_tgt / all_tgt,\n",
    "        \"tgt_f1\": 2 * correct_tgt / (pred_tgt + all_tgt),\n",
    "        'relation precision': correct_none / pred_none,\n",
    "        'relation recall': correct_none / all_none,\n",
    "        'relation_f1': 2 * correct_none / (pred_none + all_none)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src precision': 0.4256410256410256,\n",
       " 'src recall': 0.14273430782459157,\n",
       " 'src_f1': 0.21377978106889892,\n",
       " 'tgt precision': 0.5377643504531722,\n",
       " 'tgt recall': 0.1530524505588994,\n",
       " 'tgt_f1': 0.23828647925033467,\n",
       " 'relation precision': 0.9822973644952759,\n",
       " 'relation recall': 0.9950467619255503,\n",
       " 'relation_f1': 0.9886309608214402}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_infer_alter(output, test_dataset['labels'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioRED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
