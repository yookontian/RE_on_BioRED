{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model: \n",
    "\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "The input be like:\n",
    "\n",
    "    [cls] [relation] [sep] \"text w ner1 and ner2 tags\"\n",
    "\n",
    "(the ner1 and ner2 tags depends on the first occurence orders)\n",
    "\n",
    "The output be like:\n",
    "\n",
    "    [src] [none] [tgt] [none]\n",
    "\n",
    "or\n",
    "\n",
    "    [src] [ner2] [tgt] [ner1]\n",
    "\n",
    "or\n",
    "\n",
    "    [src] [ner1] [tgt] [ner2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_special_tokens': ['[ner1]', '[/ner1]', '[ner2]', '[/ner2]', '[Association]', '[Bind]', '[Comparison]', '[Conversion]', '[Cotreatment]', '[Drug_Interaction]', '[Negative_Correlation]', '[Positive_Correlation]']} \n",
      " ['[pad]', '[src]', '[ner1]', '[ner2]', '[tgt]', '[none]']\n"
     ]
    }
   ],
   "source": [
    "# load labels for bert_w_ner\n",
    "additional_tokens, labels, id2label, label2id = get_labels(mode='bert_w_ner')\n",
    "print(additional_tokens, \"\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [UNK]\n",
      "3: [SEP]\n",
      "0: [PAD]\n",
      "2: [CLS]\n",
      "4: [MASK]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "for id in [1, 3, 0, 2, 4]:\n",
    "    print(f\"{id}: {tokenizer.decode(id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 12 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert_w_ner/bert_w_ner_tokenizer/tokenizer_config.json',\n",
       " 'bert_w_ner/bert_w_ner_tokenizer/special_tokens_map.json',\n",
       " 'bert_w_ner/bert_w_ner_tokenizer/vocab.txt',\n",
       " 'bert_w_ner/bert_w_ner_tokenizer/added_tokens.json',\n",
       " 'bert_w_ner/bert_w_ner_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding new tokens to the tokenizer\n",
    "# since I haven't load the model so I will resize the embedding of the model later]\n",
    "num_added_toks = tokenizer.add_special_tokens(additional_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_pretrained(\"bert_w_ner/bert_w_ner_tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import make_bert_re_data\n",
    "from data_preprocessing import bert_w_ner_preprocess_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and valid file paths\n",
    "train_file_path = 'data/BioRED/processed/train.tsv'\n",
    "valid_file_path = 'data/BioRED/processed/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bert_re data\n",
    "train_data_raw = make_bert_re_data(file_path=train_file_path, lower=True, output_none=True)\n",
    "valid_data_raw = make_bert_re_data(file_path=valid_file_path, lower=True, output_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into Dataset type\n",
    "train_data_raw = Dataset.from_dict(train_data_raw)\n",
    "valid_data_raw = Dataset.from_dict(valid_data_raw)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_data_raw,\n",
    "    \"valid\": valid_data_raw\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c217c6028774c9380d1eaf8b1c9aef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/183160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ddcb2495cf44ddbff776fb969a416f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=[\"inputs\", \"outputs\", \"pmids\"])\n",
    "tokenized_datasets = dataset.map(lambda example: bert_w_ner_preprocess_function(example, tokenizer), batched=True, remove_columns=[\"input_texts\", \"input_relations\", \"outputs\", \"pmids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 183160\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 53264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tensor\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the number of trainable parameters in the model.\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[pad]', '[src]', '[ner1]', '[ner2]', '[tgt]', '[none]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint,\n",
    "                                                        num_labels=len(labels),\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id,\n",
    "                                                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding special tokens to the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30534, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108905478 || all params: 108905478 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,\n",
      "        output_type=TokenClassifierOutput,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,\n",
      "        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.Tensor] = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        token_type_ids: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.Tensor] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        inputs_embeds: Optional[torch.Tensor] = None,\n",
      "        labels: Optional[torch.Tensor] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n",
      "        r\"\"\"\n",
      "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
      "        \"\"\"\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        outputs = self.bert(\n",
      "            input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            token_type_ids=token_type_ids,\n",
      "            position_ids=position_ids,\n",
      "            head_mask=head_mask,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        sequence_output = outputs[0]\n",
      "\n",
      "        sequence_output = self.dropout(sequence_output)\n",
      "        logits = self.classifier(sequence_output)\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            loss_fct = CrossEntropyLoss()\n",
      "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[2:]\n",
      "            return ((loss,) + output) if loss is not None else output\n",
      "\n",
      "        return TokenClassifierOutput(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(model.forward))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # only consider non-padding tokens [:4]\n",
    "    true_labels = [[id2label[l.item()] for l in label[:4]] for label in labels]\n",
    "    true_predictions = [\n",
    "    [id2label[p.item()] for (p, l) in zip(prediction[:4], label[:4])]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customized loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the feature name \"labels\" to \"labels_\" of tokenized_datasets\n",
    "# to not trigger the default loss function the enssembled in Auto model\n",
    "\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"labels\", \"labels_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 183160\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 53264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 5, 4,  ..., 0, 0, 0],\n",
       "        [1, 5, 4,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "tokenized_datasets\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        # only consider first 4 columns of the batched loss and the batched labels\n",
    "        logits = logits[:, :4, :]\n",
    "        labels = labels[:, :4]\n",
    "        loss = loss_fct(logits.reshape(-1, self.model.config.num_labels), labels.reshape(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m309439737\u001b[0m (\u001b[33mtian1995\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tian/Projects/BioRED/wandb/run-20230624_173553-5dntu29g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tian1995/BERT/runs/5dntu29g' target=\"_blank\">bert_w_ner_epoch_5_loss2</a></strong> to <a href='https://wandb.ai/tian1995/BERT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tian1995/BERT' target=\"_blank\">https://wandb.ai/tian1995/BERT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tian1995/BERT/runs/5dntu29g' target=\"_blank\">https://wandb.ai/tian1995/BERT/runs/5dntu29g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tian1995/BERT/runs/5dntu29g?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f519f262b90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"BERT\",\n",
    "    # notes=\"PubmedBERT-FT-NER_w_NERin_10epochs\",\n",
    "    name=\"bert_w_ner_epoch_5_loss2\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",\n",
    "    # per_device_train_batch_size=4,\n",
    "    # per_device_eval_batch_size=4,\n",
    "    auto_find_batch_size=True,\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09c85ced34b4e23ad27208da8395afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0271, 'learning_rate': 1.9912644682245032e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0008, 'learning_rate': 1.9825289364490066e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0006, 'learning_rate': 1.9737934046735096e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0005, 'learning_rate': 1.9650578728980127e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0005, 'learning_rate': 1.956322341122516e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0004, 'learning_rate': 1.947586809347019e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0004, 'learning_rate': 1.938851277571522e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0003, 'learning_rate': 1.9301157457960255e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0004, 'learning_rate': 1.9213802140205286e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0003, 'learning_rate': 1.9126446822450316e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0003, 'learning_rate': 1.903909150469535e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0004, 'learning_rate': 1.895173618694038e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0004, 'learning_rate': 1.8864380869185414e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0004, 'learning_rate': 1.8777025551430444e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0003, 'learning_rate': 1.8689670233675478e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0003, 'learning_rate': 1.860231491592051e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0003, 'learning_rate': 1.851495959816554e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0003, 'learning_rate': 1.8427604280410573e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0003, 'learning_rate': 1.8340248962655603e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0003, 'learning_rate': 1.8252893644900637e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0006, 'learning_rate': 1.8165538327145667e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0004, 'learning_rate': 1.8078183009390698e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0003, 'learning_rate': 1.799082769163573e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7903472373880762e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7816117056125792e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7728761738370826e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7641406420615857e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7554051102860887e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0003, 'learning_rate': 1.746669578510592e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0003, 'learning_rate': 1.737934046735095e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7291985149595982e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7204629831841016e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7117274514086046e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0003, 'learning_rate': 1.7029919196331076e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0003, 'learning_rate': 1.694256387857611e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0002, 'learning_rate': 1.685520856082114e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0003, 'learning_rate': 1.676785324306617e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6680497925311205e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6593142607556235e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6505787289801266e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0002, 'learning_rate': 1.64184319720463e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0003, 'learning_rate': 1.633107665429133e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6243721336536364e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6156366018781394e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0003, 'learning_rate': 1.6069010701026428e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b6e829ca9e418d99ddc5e04d295473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002926877059508115, 'eval_precision': 0.9894863322319015, 'eval_recall': 0.9894863322319015, 'eval_f1': 0.9894863322319015, 'eval_accuracy': 0.9894863322319015, 'eval_runtime': 681.9754, 'eval_samples_per_second': 78.103, 'eval_steps_per_second': 9.763, 'epoch': 1.0}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5981655383271458e-05, 'epoch': 1.0}\n",
      "{'loss': 0.0003, 'learning_rate': 1.589430006551649e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5806944747761522e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5719589430006553e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5632234112251587e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5544878794496617e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5457523476741647e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0003, 'learning_rate': 1.537016815898668e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5282812841231712e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5195457523476742e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0003, 'learning_rate': 1.5108102205721776e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0002, 'learning_rate': 1.5020746887966806e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4933391570211837e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0003, 'learning_rate': 1.484603625245687e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4758680934701901e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4671325616946931e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4583970299191965e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0003, 'learning_rate': 1.4496614981436996e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4409259663682028e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0002, 'learning_rate': 1.432190434592706e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0002, 'learning_rate': 1.423454902817209e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4147193710417122e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0002, 'learning_rate': 1.4059838392662154e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3972483074907187e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3885127757152217e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0002, 'learning_rate': 1.379777243939725e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3710417121642281e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0003, 'learning_rate': 1.3623061803887312e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3535706486132345e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3448351168377376e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3360995850622406e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0002, 'learning_rate': 1.327364053286744e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0002, 'learning_rate': 1.318628521511247e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0002, 'learning_rate': 1.3098929897357503e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0003, 'learning_rate': 1.3011574579602535e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0003, 'learning_rate': 1.2924219261847567e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2836863944092597e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0002, 'learning_rate': 1.274950862633763e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2662153308582661e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0003, 'learning_rate': 1.2574797990827692e-05, 'epoch': 1.86}\n",
      "{'loss': 0.0003, 'learning_rate': 1.2487442673072726e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2400087355317756e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2312732037562786e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0002, 'learning_rate': 1.222537671980782e-05, 'epoch': 1.94}\n",
      "{'loss': 0.0002, 'learning_rate': 1.213802140205285e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0002, 'learning_rate': 1.2050666084297881e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e98bfe86034f1ea7264b77967f1b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002620388404466212, 'eval_precision': 0.989373685791529, 'eval_recall': 0.989373685791529, 'eval_f1': 0.989373685791529, 'eval_accuracy': 0.989373685791529, 'eval_runtime': 679.4867, 'eval_samples_per_second': 78.389, 'eval_steps_per_second': 9.799, 'epoch': 2.0}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1963310766542915e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1875955448787945e-05, 'epoch': 2.03}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1788600131032977e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0003, 'learning_rate': 1.170124481327801e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0003, 'learning_rate': 1.1613889495523042e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1526534177768072e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1439178860013104e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1351823542258136e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1264468224503167e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0002, 'learning_rate': 1.11771129067482e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0002, 'learning_rate': 1.108975758899323e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0002, 'learning_rate': 1.1002402271238261e-05, 'epoch': 2.25}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0915046953483295e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0827691635728325e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0740336317973356e-05, 'epoch': 2.31}\n",
      "{'loss': 0.0002, 'learning_rate': 1.065298100021839e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0002, 'learning_rate': 1.056562568246342e-05, 'epoch': 2.36}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0478270364708452e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0390915046953484e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0303559729198516e-05, 'epoch': 2.42}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0216204411443547e-05, 'epoch': 2.45}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0128849093688579e-05, 'epoch': 2.47}\n",
      "{'loss': 0.0002, 'learning_rate': 1.0041493775933611e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0002, 'learning_rate': 9.954138458178643e-06, 'epoch': 2.51}\n",
      "{'loss': 0.0002, 'learning_rate': 9.866783140423674e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0002, 'learning_rate': 9.779427822668706e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0002, 'learning_rate': 9.692072504913738e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0002, 'learning_rate': 9.604717187158768e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0002, 'learning_rate': 9.5173618694038e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0002, 'learning_rate': 9.430006551648832e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0002, 'learning_rate': 9.342651233893863e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0002, 'learning_rate': 9.255295916138895e-06, 'epoch': 2.69}\n",
      "{'loss': 0.0002, 'learning_rate': 9.167940598383927e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0002, 'learning_rate': 9.080585280628959e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0002, 'learning_rate': 8.993229962873991e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0002, 'learning_rate': 8.905874645119023e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0002, 'learning_rate': 8.818519327364054e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0002, 'learning_rate': 8.731164009609086e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0002, 'learning_rate': 8.643808691854118e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0002, 'learning_rate': 8.556453374099148e-06, 'epoch': 2.86}\n",
      "{'loss': 0.0002, 'learning_rate': 8.46909805634418e-06, 'epoch': 2.88}\n",
      "{'loss': 0.0002, 'learning_rate': 8.381742738589213e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0002, 'learning_rate': 8.294387420834243e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0002, 'learning_rate': 8.207032103079275e-06, 'epoch': 2.95}\n",
      "{'loss': 0.0002, 'learning_rate': 8.119676785324307e-06, 'epoch': 2.97}\n",
      "{'loss': 0.0001, 'learning_rate': 8.032321467569338e-06, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac41a194d9d04485acf54cb3508c1c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002686600782908499, 'eval_precision': 0.9890686016821868, 'eval_recall': 0.9890686016821868, 'eval_f1': 0.9890686016821868, 'eval_accuracy': 0.9890686016821868, 'eval_runtime': 679.4823, 'eval_samples_per_second': 78.389, 'eval_steps_per_second': 9.799, 'epoch': 3.0}\n",
      "{'loss': 0.0002, 'learning_rate': 7.94496614981437e-06, 'epoch': 3.01}\n",
      "{'loss': 0.0002, 'learning_rate': 7.857610832059402e-06, 'epoch': 3.04}\n",
      "{'loss': 0.0002, 'learning_rate': 7.770255514304434e-06, 'epoch': 3.06}\n",
      "{'loss': 0.0002, 'learning_rate': 7.682900196549466e-06, 'epoch': 3.08}\n",
      "{'loss': 0.0002, 'learning_rate': 7.595544878794497e-06, 'epoch': 3.1}\n",
      "{'loss': 0.0002, 'learning_rate': 7.508189561039529e-06, 'epoch': 3.12}\n",
      "{'loss': 0.0002, 'learning_rate': 7.420834243284561e-06, 'epoch': 3.14}\n",
      "{'loss': 0.0002, 'learning_rate': 7.333478925529593e-06, 'epoch': 3.17}\n",
      "{'loss': 0.0002, 'learning_rate': 7.246123607774624e-06, 'epoch': 3.19}\n",
      "{'loss': 0.0002, 'learning_rate': 7.158768290019655e-06, 'epoch': 3.21}\n",
      "{'loss': 0.0002, 'learning_rate': 7.0714129722646874e-06, 'epoch': 3.23}\n",
      "{'loss': 0.0002, 'learning_rate': 6.9840576545097195e-06, 'epoch': 3.25}\n",
      "{'loss': 0.0002, 'learning_rate': 6.89670233675475e-06, 'epoch': 3.28}\n",
      "{'loss': 0.0002, 'learning_rate': 6.809347018999782e-06, 'epoch': 3.3}\n",
      "{'loss': 0.0002, 'learning_rate': 6.721991701244814e-06, 'epoch': 3.32}\n",
      "{'loss': 0.0002, 'learning_rate': 6.6346363834898454e-06, 'epoch': 3.34}\n",
      "{'loss': 0.0001, 'learning_rate': 6.5472810657348775e-06, 'epoch': 3.36}\n",
      "{'loss': 0.0002, 'learning_rate': 6.45992574797991e-06, 'epoch': 3.39}\n",
      "{'loss': 0.0002, 'learning_rate': 6.37257043022494e-06, 'epoch': 3.41}\n",
      "{'loss': 0.0002, 'learning_rate': 6.285215112469972e-06, 'epoch': 3.43}\n",
      "{'loss': 0.0002, 'learning_rate': 6.197859794715004e-06, 'epoch': 3.45}\n",
      "{'loss': 0.0002, 'learning_rate': 6.1105044769600355e-06, 'epoch': 3.47}\n",
      "{'loss': 0.0002, 'learning_rate': 6.023149159205068e-06, 'epoch': 3.49}\n",
      "{'loss': 0.0002, 'learning_rate': 5.935793841450099e-06, 'epoch': 3.52}\n",
      "{'loss': 0.0002, 'learning_rate': 5.84843852369513e-06, 'epoch': 3.54}\n",
      "{'loss': 0.0002, 'learning_rate': 5.761083205940162e-06, 'epoch': 3.56}\n",
      "{'loss': 0.0001, 'learning_rate': 5.673727888185194e-06, 'epoch': 3.58}\n",
      "{'loss': 0.0001, 'learning_rate': 5.586372570430225e-06, 'epoch': 3.6}\n",
      "{'loss': 0.0002, 'learning_rate': 5.499017252675257e-06, 'epoch': 3.63}\n",
      "{'loss': 0.0002, 'learning_rate': 5.411661934920289e-06, 'epoch': 3.65}\n",
      "{'loss': 0.0001, 'learning_rate': 5.32430661716532e-06, 'epoch': 3.67}\n",
      "{'loss': 0.0002, 'learning_rate': 5.236951299410352e-06, 'epoch': 3.69}\n",
      "{'loss': 0.0002, 'learning_rate': 5.1495959816553845e-06, 'epoch': 3.71}\n",
      "{'loss': 0.0002, 'learning_rate': 5.062240663900415e-06, 'epoch': 3.73}\n",
      "{'loss': 0.0001, 'learning_rate': 4.974885346145447e-06, 'epoch': 3.76}\n",
      "{'loss': 0.0002, 'learning_rate': 4.887530028390478e-06, 'epoch': 3.78}\n",
      "{'loss': 0.0002, 'learning_rate': 4.80017471063551e-06, 'epoch': 3.8}\n",
      "{'loss': 0.0002, 'learning_rate': 4.7128193928805425e-06, 'epoch': 3.82}\n",
      "{'loss': 0.0001, 'learning_rate': 4.625464075125574e-06, 'epoch': 3.84}\n",
      "{'loss': 0.0002, 'learning_rate': 4.538108757370605e-06, 'epoch': 3.87}\n",
      "{'loss': 0.0001, 'learning_rate': 4.450753439615637e-06, 'epoch': 3.89}\n",
      "{'loss': 0.0002, 'learning_rate': 4.363398121860668e-06, 'epoch': 3.91}\n",
      "{'loss': 0.0001, 'learning_rate': 4.2760428041057005e-06, 'epoch': 3.93}\n",
      "{'loss': 0.0002, 'learning_rate': 4.188687486350732e-06, 'epoch': 3.95}\n",
      "{'loss': 0.0002, 'learning_rate': 4.101332168595764e-06, 'epoch': 3.97}\n",
      "{'loss': 0.0002, 'learning_rate': 4.013976850840795e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d15f8a229348cab16b2f6c9a1c0aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002841673558577895, 'eval_precision': 0.9874070666866926, 'eval_recall': 0.9874070666866926, 'eval_f1': 0.9874070666866926, 'eval_accuracy': 0.9874070666866926, 'eval_runtime': 681.1798, 'eval_samples_per_second': 78.194, 'eval_steps_per_second': 9.774, 'epoch': 4.0}\n",
      "{'loss': 0.0002, 'learning_rate': 3.926621533085827e-06, 'epoch': 4.02}\n",
      "{'loss': 0.0001, 'learning_rate': 3.8392662153308585e-06, 'epoch': 4.04}\n",
      "{'loss': 0.0001, 'learning_rate': 3.75191089757589e-06, 'epoch': 4.06}\n",
      "{'loss': 0.0001, 'learning_rate': 3.664555579820922e-06, 'epoch': 4.08}\n",
      "{'loss': 0.0001, 'learning_rate': 3.5772002620659535e-06, 'epoch': 4.11}\n",
      "{'loss': 0.0001, 'learning_rate': 3.4898449443109848e-06, 'epoch': 4.13}\n",
      "{'loss': 0.0002, 'learning_rate': 3.402489626556017e-06, 'epoch': 4.15}\n",
      "{'loss': 0.0001, 'learning_rate': 3.3151343088010486e-06, 'epoch': 4.17}\n",
      "{'loss': 0.0001, 'learning_rate': 3.22777899104608e-06, 'epoch': 4.19}\n",
      "{'loss': 0.0001, 'learning_rate': 3.140423673291112e-06, 'epoch': 4.21}\n",
      "{'loss': 0.0001, 'learning_rate': 3.0530683555361436e-06, 'epoch': 4.24}\n",
      "{'loss': 0.0001, 'learning_rate': 2.965713037781175e-06, 'epoch': 4.26}\n",
      "{'loss': 0.0001, 'learning_rate': 2.878357720026207e-06, 'epoch': 4.28}\n",
      "{'loss': 0.0001, 'learning_rate': 2.7910024022712382e-06, 'epoch': 4.3}\n",
      "{'loss': 0.0001, 'learning_rate': 2.70364708451627e-06, 'epoch': 4.32}\n",
      "{'loss': 0.0001, 'learning_rate': 2.616291766761302e-06, 'epoch': 4.35}\n",
      "{'loss': 0.0002, 'learning_rate': 2.5289364490063333e-06, 'epoch': 4.37}\n",
      "{'loss': 0.0001, 'learning_rate': 2.441581131251365e-06, 'epoch': 4.39}\n",
      "{'loss': 0.0001, 'learning_rate': 2.354225813496397e-06, 'epoch': 4.41}\n",
      "{'loss': 0.0001, 'learning_rate': 2.2668704957414283e-06, 'epoch': 4.43}\n",
      "{'loss': 0.0001, 'learning_rate': 2.17951517798646e-06, 'epoch': 4.46}\n",
      "{'loss': 0.0001, 'learning_rate': 2.0921598602314917e-06, 'epoch': 4.48}\n",
      "{'loss': 0.0001, 'learning_rate': 2.0048045424765234e-06, 'epoch': 4.5}\n",
      "{'loss': 0.0001, 'learning_rate': 1.917449224721555e-06, 'epoch': 4.52}\n",
      "{'loss': 0.0001, 'learning_rate': 1.8300939069665868e-06, 'epoch': 4.54}\n",
      "{'loss': 0.0001, 'learning_rate': 1.7427385892116182e-06, 'epoch': 4.56}\n",
      "{'loss': 0.0001, 'learning_rate': 1.6553832714566501e-06, 'epoch': 4.59}\n",
      "{'loss': 0.0001, 'learning_rate': 1.5680279537016818e-06, 'epoch': 4.61}\n",
      "{'loss': 0.0001, 'learning_rate': 1.4806726359467133e-06, 'epoch': 4.63}\n",
      "{'loss': 0.0001, 'learning_rate': 1.393317318191745e-06, 'epoch': 4.65}\n",
      "{'loss': 0.0001, 'learning_rate': 1.3059620004367769e-06, 'epoch': 4.67}\n",
      "{'loss': 0.0001, 'learning_rate': 1.2186066826818083e-06, 'epoch': 4.7}\n",
      "{'loss': 0.0001, 'learning_rate': 1.13125136492684e-06, 'epoch': 4.72}\n",
      "{'loss': 0.0001, 'learning_rate': 1.0438960471718717e-06, 'epoch': 4.74}\n",
      "{'loss': 0.0001, 'learning_rate': 9.565407294169034e-07, 'epoch': 4.76}\n",
      "{'loss': 0.0001, 'learning_rate': 8.69185411661935e-07, 'epoch': 4.78}\n",
      "{'loss': 0.0001, 'learning_rate': 7.818300939069666e-07, 'epoch': 4.8}\n",
      "{'loss': 0.0001, 'learning_rate': 6.944747761519983e-07, 'epoch': 4.83}\n",
      "{'loss': 0.0001, 'learning_rate': 6.0711945839703e-07, 'epoch': 4.85}\n",
      "{'loss': 0.0001, 'learning_rate': 5.197641406420616e-07, 'epoch': 4.87}\n",
      "{'loss': 0.0001, 'learning_rate': 4.324088228870933e-07, 'epoch': 4.89}\n",
      "{'loss': 0.0001, 'learning_rate': 3.4505350513212496e-07, 'epoch': 4.91}\n",
      "{'loss': 0.0001, 'learning_rate': 2.5769818737715664e-07, 'epoch': 4.94}\n",
      "{'loss': 0.0001, 'learning_rate': 1.7034286962218827e-07, 'epoch': 4.96}\n",
      "{'loss': 0.0001, 'learning_rate': 8.298755186721993e-08, 'epoch': 4.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064761777f4242348eb081bc9914b3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [src] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [none] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [tgt] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner2] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/tian/mambaforge/envs/BioRED/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [ner1] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00030054408125579357, 'eval_precision': 0.9884865950735957, 'eval_recall': 0.9884865950735957, 'eval_f1': 0.9884865950735957, 'eval_accuracy': 0.9884865950735957, 'eval_runtime': 678.2311, 'eval_samples_per_second': 78.534, 'eval_steps_per_second': 9.817, 'epoch': 5.0}\n",
      "{'train_runtime': 39488.3426, 'train_samples_per_second': 23.192, 'train_steps_per_second': 2.899, 'train_loss': 0.0003305545512389137, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114475, training_loss=0.0003305545512389137, metrics={'train_runtime': 39488.3426, 'train_samples_per_second': 23.192, 'train_steps_per_second': 2.899, 'train_loss': 0.0003305545512389137, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td></td></tr><tr><td>eval/f1</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/precision</td><td></td></tr><tr><td>eval/recall</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_loss</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.98849</td></tr><tr><td>eval/f1</td><td>0.98849</td></tr><tr><td>eval/loss</td><td>0.0003</td></tr><tr><td>eval/precision</td><td>0.98849</td></tr><tr><td>eval/recall</td><td>0.98849</td></tr><tr><td>eval/runtime</td><td>678.2311</td></tr><tr><td>eval/samples_per_second</td><td>78.534</td></tr><tr><td>eval/steps_per_second</td><td>9.817</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>114475</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0001</td></tr><tr><td>train/total_flos</td><td>2.393042236452864e+17</td></tr><tr><td>train/train_loss</td><td>0.00033</td></tr><tr><td>train/train_runtime</td><td>39488.3426</td></tr><tr><td>train/train_samples_per_second</td><td>23.192</td></tr><tr><td>train/train_steps_per_second</td><td>2.899</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert_w_ner_epoch_5_loss2</strong> at: <a href='https://wandb.ai/tian1995/BERT/runs/5dntu29g' target=\"_blank\">https://wandb.ai/tian1995/BERT/runs/5dntu29g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230624_173553-5dntu29g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "trainer.save_model(\"bert_w_ner/models/bert_w_ner_epoch_5_loss2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch import nn\n",
    "from labels import get_labels\n",
    "from relations import relations\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from data_preprocessing import make_bert_re_data\n",
    "from data_preprocessing import bert_w_ner_preprocess_function\n",
    "additional_tokens, labels, id2label, label2id = get_labels(mode='bert_w_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and tokenizer\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert_w_ner/models/bert_w_ner_epoch_5_loss2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert_w_ner/bert_w_ner_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#load test data and preprocess\n",
    "# test_file_path = 'data/BioRED/processed/test.tsv'\n",
    "# test_data = make_bert_re_data(file_path=test_file_path , lower=True, output_none=True)\n",
    "# # turning all of the items in test_data['pmids'] into string\n",
    "# test_data['pmids'] = [str(pmid) for pmid in test_data['pmids']]\n",
    "# # save test_data\n",
    "# with open('bert_w_ner/data/test_data_dict.json', 'w') as f:\n",
    "#     json.dump(test_data, f)\n",
    "\n",
    "\n",
    "with open('bert_w_ner/data/test_data_dict.json', 'r') as f:\n",
    "    test_data= json.load(f)\n",
    "\n",
    "\n",
    "test_dataset_raw = Dataset.from_dict(test_data)\n",
    "# test_dataset = test_dataset_raw.map(NER_preprocess_function, batched=False)\n",
    "# with bert only:\n",
    "# test_dataset = test_dataset_raw.map(lambda example: bert_w_ner_preprocess_function(example, tokenizer), batched=True, remove_columns=[\"input_texts\", \"input_relations\", \"outputs\", \"pmids\"])\n",
    "\n",
    "# test_dataset.save_to_disk('bert_w_ner/data/test_tokenized_dataset_w_ner')\n",
    "\n",
    "# from datasets import load_from_disk\n",
    "\n",
    "# test_dataset = load_from_disk('GPT_w_ner/data/test_tokenized_dataset_no_ner')\n",
    "# test_dataset.set_format(type='torch', columns=['input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmids': '15485686',\n",
       " 'input_texts': 'a novel scn5a mutation manifests as a malignant form of long qt syndrome with perinatal onset of tachycardia/bradycardia . objective : congenital long qt syndrome ( lqts ) with in utero onset of the rhythm disturbances is associated with a poor prognosis . in this study we investigated a newborn patient with fetal bradycardia , 2:1 [ner1] atrioventricular block [/ner1] and ventricular tachycardia soon after birth . methods : mutational analysis and dna sequencing were conducted in a newborn . the 2:1 [ner1] atrioventricular block [/ner1] improved to 1:1 conduction only after intravenous lidocaine infusion or a high dose of mexiletine , which also controlled the ventricular tachycardia . results : a novel , spontaneous lqts-3 mutation was identified in the transmembrane segment 6 of domain iv of the na(v)1.5 cardiac sodium channel , with a g-->a substitution at codon 1763 , which changed a valine ( gtg ) to a methionine ( atg ) . the proband was heterozygous but the mutation was absent in the parents and the sister . expression of this mutant channel in tsa201 mammalian cells by site-directed mutagenesis revealed a persistent tetrodotoxin-sensitive but lidocaine-resistant current that was associated with a positive shift of the steady-state inactivation curve , steeper activation curve and faster recovery from inactivation . we also found a similar electrophysiological profile for the neighboring [ner2] v1764 m [/ner2] mutant . but , the other neighboring i1762a mutant had no persistent current and was still associated with a positive shift of inactivation . conclusions : these findings suggest that the na(v)1.5/v1763 m channel dysfunction and possible neighboring mutants contribute to a persistent inward current due to altered inactivation kinetics and clinically congenital lqts with perinatal onset of arrhythmias that responded to lidocaine and mexiletine .',\n",
       " 'input_relations': '[Association]',\n",
       " 'outputs': ['[src]', '[none]', '[tgt]', '[none]']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 60720/60720 [15:02<00:00, 67.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_line in tqdm(test_dataset['input_ids']):\n",
    "    # for n in range(1):\n",
    "        torch.cuda.empty_cache()\n",
    "        out = model(input_ids=input_line.unsqueeze(0).to(\"cuda\"))\n",
    "        # print(f\"{n+1} / {len(test_dataset)}\")\n",
    "        output.append(out[0].to(\"cpu\"))\n",
    "        # output.append(torch.argmax(out[0], dim=-1).squeeze(0))\n",
    "        # output[-1].to(\"cpu\")\n",
    "    # print([tag_to_NER_id[i.item()] for i in output[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output logits to a file\n",
    "# torch.save(output, 'bert_w_ner/results/bert_w_ner_epoch_5_loss2_raw.pt')\n",
    "\n",
    "output = torch.load('bert_w_ner/results/bert_w_ner_epoch_5_loss2_raw.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the results\n",
    "# import pickle\n",
    "\n",
    "# # save the dictionary to a file\n",
    "# with open('bert_w_ner/results/bert_w_ner_epoch_5_loss2.pickle', 'wb') as handle:\n",
    "#     pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the dictionary from a file\n",
    "with open('bert_w_ner/results/bert_w_ner_epoch_5_loss2.pickle', 'rb') as handle:\n",
    "    results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = [np.argmax(logit, axis=-1)[0] for logit in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "for item in predictions:\n",
    "    preds.append([id2label[i.item()] for i in item[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tp = 0\n",
    "src_fp = 0\n",
    "src_fn = 0\n",
    "\n",
    "tgt_tp = 0\n",
    "tgt_fp = 0\n",
    "tgt_fn = 0\n",
    "\n",
    "tuple_tp = 0\n",
    "tuple_fp = 0\n",
    "tuple_fn = 0\n",
    "\n",
    "for pred, label in zip(preds, test_dataset_raw['outputs']):\n",
    "    src = False\n",
    "    relation = False\n",
    "    if pred[1] == label[1]:\n",
    "        src_tp += 1\n",
    "        src = True\n",
    "    else:\n",
    "        src_fn += 1\n",
    "        src_fp += 1\n",
    "    if pred[2] == label[2]:\n",
    "        tgt_tp += 1\n",
    "        tgt = True\n",
    "        if src:\n",
    "            tuple_tp += 1\n",
    "        else:\n",
    "            tuple_fn += 1\n",
    "            tuple_fp += 1\n",
    "    else:\n",
    "        tgt_fn += 1\n",
    "        tgt_fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[pad]', 1: '[src]', 2: '[ner1]', 3: '[ner2]', 4: '[tgt]', 5: '[none]'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_precision: 0.9798913043478261, src_recall: 0.9798913043478261, src_f1: 0.9798913043478261\n",
      "tgt_precision: 1.0, tgt_recall: 0.9499036608863198, tgt_f1: 0.974308300395257\n",
      "tuple_precision: 0.9816599053414469, tuple_recall: 0.9816599053414469, tuple_f1: 0.9816599053414469\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for src\n",
    "src_precision = src_tp / (src_tp + src_fp)\n",
    "src_recall = src_tp / (src_tp + src_fn)\n",
    "src_f1 = 2 * src_precision * src_recall / (src_precision + src_recall)\n",
    "\n",
    "# for tgt\n",
    "tgt_precision = tgt_tp / (tgt_tp + tgt_fp)\n",
    "tgt_recall = tgt_tp / (tgt_tp + tgt_fn)\n",
    "tgt_f1 = 2 * tgt_precision * tgt_recall / (tgt_precision + tgt_recall)\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall)\n",
    "\n",
    "print(f\"src_precision: {src_precision}, src_recall: {src_recall}, src_f1: {src_f1}\")\n",
    "print(f\"tgt_precision: {tgt_precision}, tgt_recall: {tgt_recall}, tgt_f1: {tgt_f1}\")\n",
    "print(f\"tuple_precision: {tuple_precision}, tuple_recall: {tuple_recall}, tuple_f1: {tuple_f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference with balanced test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1363\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# {'None': 59557,\n",
    "#  'Association': 635,\n",
    "#  'Bind': 9,\n",
    "#  'Comparison': 6,\n",
    "#  'Conversion': 1,\n",
    "#  'Cotreatment': 14,\n",
    "#  'Drug_Interaction': 2,\n",
    "#  'Negative_Correlation': 171,\n",
    "#  'Positive_Correlation': 325}\n",
    "#  \"\"\"\n",
    "\n",
    "# get the index of the None label\n",
    "\n",
    "none_index = [i for i, example in enumerate(test_dataset_raw) if example['outputs'][1] == '[none]']\n",
    "\n",
    "# randomly select 200 examples from the none_index\n",
    "none_index = random.sample(none_index, 59557 - 200)\n",
    "keep_indices = [i for i in range(len(test_dataset_raw)) if i not in none_index]\n",
    "print(len(keep_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = [np.argmax(logit, axis=-1)[0] for logit in output]\n",
    "\n",
    "preds = []\n",
    "\n",
    "for item in predictions:\n",
    "    preds.append([id2label[i.item()] for i in item[:4]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1363\n",
      "pred: ['[src]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "100 / 1363\n",
      "pred: ['[src]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "200 / 1363\n",
      "pred: ['[src]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "300 / 1363\n",
      "pred: ['[src]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "400 / 1363\n",
      "pred: ['[pad]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "500 / 1363\n",
      "pred: ['[pad]', '[ner2]', '[tgt]', '[ner1]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "600 / 1363\n",
      "pred: ['[pad]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "700 / 1363\n",
      "pred: ['[src]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "800 / 1363\n",
      "pred: ['[src]', '[ner2]', '[tgt]', '[ner1]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "900 / 1363\n",
      "pred: ['[none]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "1000 / 1363\n",
      "pred: ['[src]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "1100 / 1363\n",
      "pred: ['[pad]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n",
      "1200 / 1363\n",
      "pred: ['[pad]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[none]', '[tgt]', '[none]']\n",
      "1300 / 1363\n",
      "pred: ['[src]', '[none]', '[tgt]', '[none]'], \n",
      "label: ['[src]', '[ner2]', '[tgt]', '[ner1]']\n"
     ]
    }
   ],
   "source": [
    "src_tp = 0\n",
    "src_fp = 0\n",
    "src_fn = 0\n",
    "\n",
    "tgt_tp = 0\n",
    "tgt_fp = 0\n",
    "tgt_fn = 0\n",
    "\n",
    "tuple_tp = 0\n",
    "tuple_fp = 0\n",
    "tuple_fn = 0\n",
    "\n",
    "step = 0\n",
    "for i, _ in enumerate(preds):\n",
    "    if i not in keep_indices:\n",
    "        continue\n",
    "\n",
    "    pred = preds[i]\n",
    "    label = test_dataset_raw['outputs'][i]\n",
    "    if step % 100 == 0:\n",
    "        print(f\"{step} / {len(keep_indices)}\")\n",
    "        print(f\"pred: {pred}, \\nlabel: {label}\")\n",
    "    step += 1\n",
    "    src = False\n",
    "    relation = False\n",
    "    if pred[1] == label[1]:\n",
    "        src_tp += 1\n",
    "        src = True\n",
    "    else:\n",
    "        src_fn += 1\n",
    "        src_fn += 1\n",
    "    if pred[2] == label[2]:\n",
    "        tgt_tp += 1\n",
    "        tgt = True\n",
    "        if src:\n",
    "            tuple_tp += 1\n",
    "        else:\n",
    "            tuple_fn += 1\n",
    "            tuple_fp += 1\n",
    "    else:\n",
    "        tgt_fn += 1\n",
    "        tgt_fn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_precision: 0.9999999999726027, \n",
      "src_recall: 0.15459551037630415, \n",
      "src_f1: 0.267791633775526\n",
      "tgt_precision: 0.9999999999921384, \n",
      "tgt_recall: 0.8748280605166794, \n",
      "tgt_f1: 0.9332355049200628\n",
      "tuple_precision: 0.28380503144430974, \n",
      "tuple_recall: 0.28380503144430974, \n",
      "tuple_f1: 0.2838050264443098\n"
     ]
    }
   ],
   "source": [
    "# calculate the precision, recall and f1 score\n",
    "\n",
    "# for src\n",
    "src_precision = src_tp / (src_tp + src_fp + 1e-8)\n",
    "src_recall = src_tp / (src_tp + src_fn + 1e-8)\n",
    "src_f1 = 2 * src_precision * src_recall / (src_precision + src_recall + 1e-8)\n",
    "\n",
    "# for tgt\n",
    "tgt_precision = tgt_tp / (tgt_tp + tgt_fp + 1e-8)\n",
    "tgt_recall = tgt_tp / (tgt_tp + tgt_fn + 1e-8)\n",
    "tgt_f1 = 2 * tgt_precision * tgt_recall / (tgt_precision + tgt_recall + 1e-8)\n",
    "\n",
    "# for tuple\n",
    "tuple_precision = tuple_tp / (tuple_tp + tuple_fp + 1e-8)\n",
    "tuple_recall = tuple_tp / (tuple_tp + tuple_fn + 1e-8)\n",
    "tuple_f1 = 2 * tuple_precision * tuple_recall / (tuple_precision + tuple_recall + 1e-8)\n",
    "\n",
    "print(f\"src_precision: {src_precision}, \\nsrc_recall: {src_recall}, \\nsrc_f1: {src_f1}\")\n",
    "print(f\"tgt_precision: {tgt_precision}, \\ntgt_recall: {tgt_recall}, \\ntgt_f1: {tgt_f1}\")\n",
    "print(f\"tuple_precision: {tuple_precision}, \\ntuple_recall: {tuple_recall}, \\ntuple_f1: {tuple_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioRED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
